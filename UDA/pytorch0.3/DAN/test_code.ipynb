{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d2259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import math\n",
    "import data_loader\n",
    "import ResNet as models\n",
    "from config.settings_test import *\n",
    "from utils.data_preprocess import *\n",
    "import dataset_loader as dl\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from utils.data_load import load_data_patches, generate_data_patches\n",
    "from DatasetsGeneratorFromFiles import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c5d1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = lr[0] / math.pow((1 + 10 * (epoch - 1) / epochs), 0.75)\n",
    "    optimizer.param_groups[1]['lr'] = lr[1] / math.pow((1 + 10 * (epoch - 1) / epochs), 0.75)\n",
    "\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    correct = 0\n",
    "\n",
    "    #iter_source_train = iter(source_train_loader)\n",
    "    #num_iter_train = len_source_train_loader\n",
    "    for i, (data_source_train, label_source_train) in enumerate(train_generator.__getitem__()):\n",
    "        data_source_train, label_source_train = torch.from_numpy(data_source_train), torch.from_numpy(label_source_train)\n",
    "    #for i in range(1, num_iter_train):\n",
    "        #data_source_train, label_source_train = iter_source_train.next()\n",
    "        if cuda:\n",
    "            data_source_train, label_source_train = data_source_train.cuda(), label_source_train.cuda()\n",
    "        data_source_train, label_source_train = Variable(data_source_train), Variable(label_source_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        label_source_train_pred, _ = model(data_source_train)\n",
    "        # loss = F.cross_entropy(F.log_softmax(label_source_train_pred, dim=1), label_source_train.type(torch.long))\n",
    "        loss = F.cross_entropy(label_source_train_pred, label_source_train.type(torch.long), reduction='mean')\n",
    "        with torch.no_grad():\n",
    "            train_loss += loss\n",
    "            pred = label_source_train_pred.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(label_source_train.view_as(pred)).cpu().sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data_source_train), len_source_train_dataset,\n",
    "                100. * i / len_source_train_loader, loss.item()))\n",
    "\n",
    "    correct = correct.item()\n",
    "    correct_rate = correct / len_source_train_dataset\n",
    "    train_loss = train_loss.item() / len_source_train_loader\n",
    "    return correct_rate, train_loss\n",
    "\n",
    "\n",
    "def validate(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for i, (data_source_valid, label_source_valid) in enumerate(valid_generator.__getitem__()):\n",
    "            data_source_valid, label_source_valid = torch.from_numpy(data_source_valid), torch.from_numpy(label_source_valid)\n",
    "        #iter_source_valid = iter(source_valid_loader)\n",
    "        #num_iter_valid = len_source_valid_loader\n",
    "        #for i in range(1, num_iter_valid):\n",
    "            #data_source_valid, label_source_valid = iter_source_valid.next()\n",
    "            if cuda:\n",
    "                data_source_valid, label_source_valid = data_source_valid.cuda(), label_source_valid.cuda()\n",
    "            data_source_valid, label_source_valid = Variable(data_source_valid), Variable(label_source_valid)\n",
    "            s_output, _ = model(data_source_valid)\n",
    "            test_loss += F.cross_entropy(s_output, label_source_valid.type(torch.long), reduction='mean') # sum up batch loss\n",
    "            pred = s_output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(label_source_valid.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss = test_loss.item() / len_source_valid_loader\n",
    "        correct = correct.item()\n",
    "        correct_rate = correct / len_source_valid_dataset\n",
    "        print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            source_name, test_loss, correct, len_source_valid_dataset, 100. * correct_rate))\n",
    "\n",
    "        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\\n'.format(\n",
    "              source_name, '', correct, 100. * correct_rate))\n",
    "        return correct_rate, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da819e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# torch.cuda.set_device(1)\n",
    "writer = SummaryWriter('runs')\n",
    "\n",
    "# Training settings\n",
    "st = Settings()\n",
    "options = st.get_options()\n",
    "\n",
    "second_train = options['second_train']\n",
    "pretrained_model = None\n",
    "train_count = options['train_count']\n",
    "if second_train:\n",
    "    pretrained_model = models.DANNet(num_classes=2)\n",
    "    pretrained_model_path = os.path.join(options['weight_paths'], options['experiment'], '1', options['pre_trained_model'])\n",
    "    pretrained_model = torch.load(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f6ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = options['batch_size']\n",
    "epochs = options['max_epochs']\n",
    "lr = [0.001, 0.01]\n",
    "momentum = 0.9\n",
    "no_cuda =False\n",
    "seed = options['seed']\n",
    "log_interval = 10\n",
    "l2_decay = 5e-4\n",
    "source_path = options['train_folder']\n",
    "source_name = 'ISBI'\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# resize images in path\n",
    "#resize_images(options)\n",
    "\n",
    "# generate csv file\n",
    "#df = generate_csv(options)\n",
    "\n",
    "# split data to train, validate folds\n",
    "#split_folds(options['train_csv_path'], options['seed'], options['k_fold'])\n",
    "\n",
    "# list scan\n",
    "fold = 0\n",
    "# fold train data\n",
    "df = pd.read_csv(options['train_csv_path'])\n",
    "# select training scans\n",
    "train_files = df.loc[df['fold'] != fold, ['patient_id', 'study']].values\n",
    "valid_files = df.loc[df['fold'] == fold, ['patient_id', 'study']].values\n",
    "train_scan_list = [f[0]+f[1] for f in train_files]\n",
    "valid_scan_list = [f[0]+f[1] for f in valid_files]\n",
    "\n",
    "train_scan_list.sort()\n",
    "valid_scan_list.sort()\n",
    "\n",
    "train_x_data = {f: {m: os.path.join(options['train_folder'], f, options['tmp_folder'], n)\n",
    "                    for m, n in zip(options['modalities'], options['preprocess_x_names'])}\n",
    "                for f in train_scan_list}\n",
    "train_y_data = {f: os.path.join(options['train_folder'], f, options['tmp_folder'],\n",
    "                                options['preprocess_y_names'][0])\n",
    "                for f in train_scan_list}\n",
    "\n",
    "valid_x_data = {f: {m: os.path.join(options['train_folder'], f, options['tmp_folder'], n)\n",
    "                    for m, n in zip(options['modalities'], options['preprocess_x_names'])}\n",
    "                for f in valid_scan_list}\n",
    "valid_y_data = {f: os.path.join(options['train_folder'], f, options['tmp_folder'],\n",
    "                                options['preprocess_y_names'][0])\n",
    "                for f in valid_scan_list}\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437a044c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26702b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> DEBUG  training01_01 Voxels to classify: 1724143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:492: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    }
   ],
   "source": [
    "if second_train:\n",
    "    generate_data_patches(train_x_data, train_y_data, options, dataset_name='ISBI', model=pretrained_model)\n",
    "    #pass\n",
    "else:\n",
    "    #generate_data_patches(train_x_data, train_y_data, options, dataset_name='ISBI')\n",
    "    pass\n",
    "train_files, train_files_ref, train_patches = load_data_patches(options['h5_path'], options['train_csv_path'], phase='train', fold=fold, options=options)\n",
    "train_generator = DatasetGenerator(data=train_files, options=options, patches=train_patches)\n",
    "\n",
    "if second_train:\n",
    "    generate_data_patches(valid_x_data, valid_y_data, options, dataset_name='ISBI', model=pretrained_model)\n",
    "    #pass\n",
    "else:\n",
    "    #generate_data_patches(valid_x_data, valid_y_data, options, dataset_name='ISBI')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0759338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path= options['h5_path']\n",
    "train_csv_path = options['train_csv_path']\n",
    "f5_path_column_name = 'f5_path' + options['train_count']\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "x_dict = train_x_data\n",
    "y_dict = train_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86d183b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training01_01'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = list(x_dict.keys())[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b530b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> DEBUG  training01_01 Voxels to classify: 1724143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:492: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    }
   ],
   "source": [
    "model= pretrained_model\n",
    "#for idx in x_dict:\n",
    "train_x_data = {idx: x_dict[idx]}\n",
    "if y_dict is not None:\n",
    "    train_y_data = {idx: y_dict[idx]}\n",
    "    X, Y, _ = load_training_data(train_x_data, train_y_data, options, model=model)\n",
    "    print(X.shape, Y.shape)\n",
    "else:\n",
    "    X = load_target_voxels(train_x_data, options)\n",
    "    Y = None\n",
    "    train_y_data = None\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Path(h5_path).mkdir(parents=True, exist_ok=True)\n",
    "f5_path = os.path.join(h5_path, 'file_'+idx+'.hdf5')\n",
    "if dataset_name == 'ISBI':\n",
    "    index = train_data.loc[train_data.patient_id+train_data.study == idx].index[0]\n",
    "else:\n",
    "    index = train_data.loc[train_data.center_id+'_'+train_data.patient == idx].index[0]\n",
    "\n",
    "train_data.loc[index, f5_path_column_name] = f5_path\n",
    "\n",
    "#for i in raw_data:\n",
    "with h5py.File(f5_path, 'w') as f:\n",
    "    print(X.shape, 'patches', X.shape[0], 'modalities', X.shape[-1])\n",
    "    f.create_dataset(\"id\", data=idx)\n",
    "    f.create_dataset(\"patches\", data=X.shape[0])\n",
    "    f.create_dataset(\"modalities\", data=X.shape[-1])\n",
    "    f.create_dataset(str('X'), data=X)\n",
    "    if Y is not None:\n",
    "        f.create_dataset(str('Y'), data=Y)\n",
    "train_data.to_csv(train_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbd518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/DataSets/ISBI1/h5df_files1/file_training01_02.hdf5\n",
      "DANNet(\n",
      "  (sharedNet): ResNet(\n",
      "    (conv1): Conv3d(2, 64, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool3d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (cls_fc1): Linear(in_features=16384, out_features=2048, bias=True)\n",
      "  (cls_fc): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "valid_files, valid_files_ref, valid_patches = load_data_patches(options['h5_path'], options['train_csv_path'], phase='valid', fold=fold, options=options)\n",
    "valid_generator = DatasetGenerator(data=valid_files, options=options, patches=valid_patches)\n",
    "#source_train_loader = dl.load_training(options, train_x_data, train_y_data, model=pretrained_model)\n",
    "#source_valid_loader = dl.load_training(options, valid_x_data, valid_y_data, model=pretrained_model)\n",
    "\n",
    "#source_test_loader = data_loader.load_testing('', source_path, batch_size, kwargs)\n",
    "\n",
    "len_source_train_dataset = train_generator.__len__() * options['batch_size']\n",
    "len_source_valid_dataset = valid_generator.__len__() * options['batch_size']\n",
    "len_source_train_loader = train_generator.__len__()\n",
    "len_source_valid_loader = valid_generator.__len__()\n",
    "\n",
    "model = models.DANNet(num_classes=2)\n",
    "if options['load_initial_weights']:\n",
    "    model = torch.load(options['initial_weights_file'])\n",
    "elif options['save_initial_weights']:\n",
    "    Path(options['initial_weights_path']).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model, options['initial_weights_file'])\n",
    "#writer.add_graph(model, torch.rand(size=(128,2,16,16,16)))\n",
    "#writer.flush()\n",
    "#writer.close()\n",
    "#sys.exit()\n",
    "correct = 0\n",
    "print(model)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': model.sharedNet.parameters()},\n",
    "    {'params': model.cls_fc.parameters(), 'lr': lr[1]},\n",
    "    ], lr=lr[0], momentum=momentum, weight_decay=l2_decay)\n",
    "path= os.path.join(options['weight_paths'], options['experiment'], train_count)\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "history_df = pd.DataFrame(columns=['lr', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "patience = options['patience']\n",
    "patience_value = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f19372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/32384 (0%)]\tLoss: 0.838205\n",
      "Train Epoch: 1 [1280/32384 (4%)]\tLoss: 2.139352\n",
      "Train Epoch: 1 [2560/32384 (8%)]\tLoss: 4.989221\n",
      "Train Epoch: 1 [3840/32384 (12%)]\tLoss: 3.468369\n",
      "Train Epoch: 1 [5120/32384 (16%)]\tLoss: 2.202323\n",
      "Train Epoch: 1 [6400/32384 (20%)]\tLoss: 0.928869\n",
      "Train Epoch: 1 [7680/32384 (24%)]\tLoss: 1.040794\n",
      "Train Epoch: 1 [8960/32384 (28%)]\tLoss: 0.721055\n",
      "Train Epoch: 1 [10240/32384 (32%)]\tLoss: 0.736845\n",
      "Train Epoch: 1 [11520/32384 (36%)]\tLoss: 0.963362\n",
      "Train Epoch: 1 [12800/32384 (40%)]\tLoss: 0.619272\n",
      "Train Epoch: 1 [14080/32384 (43%)]\tLoss: 0.685717\n",
      "Train Epoch: 1 [15360/32384 (47%)]\tLoss: 0.675628\n",
      "Train Epoch: 1 [16640/32384 (51%)]\tLoss: 0.770761\n",
      "Train Epoch: 1 [17920/32384 (55%)]\tLoss: 0.630115\n",
      "Train Epoch: 1 [19200/32384 (59%)]\tLoss: 0.915417\n",
      "Train Epoch: 1 [20480/32384 (63%)]\tLoss: 0.908883\n",
      "Train Epoch: 1 [21760/32384 (67%)]\tLoss: 0.647430\n",
      "Train Epoch: 1 [23040/32384 (71%)]\tLoss: 0.731678\n",
      "Train Epoch: 1 [24320/32384 (75%)]\tLoss: 0.587523\n",
      "Train Epoch: 1 [25600/32384 (79%)]\tLoss: 0.613241\n",
      "Train Epoch: 1 [26880/32384 (83%)]\tLoss: 0.591506\n",
      "Train Epoch: 1 [28160/32384 (87%)]\tLoss: 0.838563\n",
      "Train Epoch: 1 [29440/32384 (91%)]\tLoss: 0.591714\n",
      "Train Epoch: 1 [30720/32384 (95%)]\tLoss: 0.630899\n",
      "Train Epoch: 1 [32000/32384 (99%)]\tLoss: 0.681051\n",
      "\n",
      "ISBI set: Average loss: 0.7941, Accuracy: 15031/30080 (49.97%)\n",
      "\n",
      "source: ISBI to target:  max correct: 15031 max accuracy 49.97%\n",
      "\n",
      "patience:  0\n",
      "Train Epoch: 2 [0/32384 (0%)]\tLoss: 0.585719\n",
      "Train Epoch: 2 [1280/32384 (4%)]\tLoss: 0.630411\n",
      "Train Epoch: 2 [2560/32384 (8%)]\tLoss: 0.563721\n",
      "Train Epoch: 2 [3840/32384 (12%)]\tLoss: 0.620662\n",
      "Train Epoch: 2 [5120/32384 (16%)]\tLoss: 0.816251\n",
      "Train Epoch: 2 [6400/32384 (20%)]\tLoss: 0.622344\n",
      "Train Epoch: 2 [7680/32384 (24%)]\tLoss: 0.790023\n",
      "Train Epoch: 2 [8960/32384 (28%)]\tLoss: 0.595027\n",
      "Train Epoch: 2 [10240/32384 (32%)]\tLoss: 0.612451\n",
      "Train Epoch: 2 [11520/32384 (36%)]\tLoss: 0.652432\n",
      "Train Epoch: 2 [12800/32384 (40%)]\tLoss: 0.662469\n",
      "Train Epoch: 2 [14080/32384 (43%)]\tLoss: 0.640948\n",
      "Train Epoch: 2 [15360/32384 (47%)]\tLoss: 0.667235\n",
      "Train Epoch: 2 [16640/32384 (51%)]\tLoss: 0.727537\n",
      "Train Epoch: 2 [17920/32384 (55%)]\tLoss: 0.624882\n",
      "Train Epoch: 2 [19200/32384 (59%)]\tLoss: 0.672961\n",
      "Train Epoch: 2 [20480/32384 (63%)]\tLoss: 0.642010\n",
      "Train Epoch: 2 [21760/32384 (67%)]\tLoss: 0.604838\n",
      "Train Epoch: 2 [23040/32384 (71%)]\tLoss: 0.772773\n",
      "Train Epoch: 2 [24320/32384 (75%)]\tLoss: 0.658456\n",
      "Train Epoch: 2 [25600/32384 (79%)]\tLoss: 0.576986\n",
      "Train Epoch: 2 [26880/32384 (83%)]\tLoss: 0.615744\n",
      "Train Epoch: 2 [28160/32384 (87%)]\tLoss: 0.589925\n",
      "Train Epoch: 2 [29440/32384 (91%)]\tLoss: 0.605273\n",
      "Train Epoch: 2 [30720/32384 (95%)]\tLoss: 0.618453\n",
      "Train Epoch: 2 [32000/32384 (99%)]\tLoss: 0.804118\n",
      "\n",
      "ISBI set: Average loss: 0.7350, Accuracy: 15031/30080 (49.97%)\n",
      "\n",
      "source: ISBI to target:  max correct: 15031 max accuracy 49.97%\n",
      "\n",
      "patience:  1\n"
     ]
    }
   ],
   "source": [
    "epochs=2\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_correct, train_loss = train(epoch, model, optimizer)\n",
    "    #torch.cuda.synchronize()\n",
    "    t_correct, test_loss = validate(model)\n",
    "\n",
    "    FILE = os.path.join(path, str(epoch)+'_model.pth')\n",
    "    torch.save(model, FILE)\n",
    "    if t_correct > correct:\n",
    "        correct = t_correct\n",
    "        patience_value = 0\n",
    "    else:\n",
    "        patience_value += 1\n",
    "    print('patience: ', patience_value)\n",
    "    df = pd.DataFrame([[lr[0], train_loss, train_correct, test_loss,  t_correct]], columns=['lr', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "    history_df = history_df.append(df)\n",
    "\n",
    "    history_df.reset_index(inplace=True)\n",
    "    history_df.drop(columns=['index'], inplace=True)\n",
    "    history_df.to_csv(options['history_csv_path'], index=False)\n",
    "    if patience_value >= patience:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38129426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
