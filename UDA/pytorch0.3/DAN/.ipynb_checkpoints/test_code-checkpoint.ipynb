{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5dbfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DANNet(\n",
       "  (sharedNet): ResNet(\n",
       "    (conv1): Conv3d(2, 64, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool3d(kernel_size=1, stride=1, padding=0)\n",
       "  )\n",
       "  (cls_fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model =torch.load('weights/resnet_DAN_full_miccai_train_dropout_patch16_fc3/1/1_model.pth')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21c2813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf87e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d2259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import math\n",
    "import data_loader\n",
    "import ResNet as models\n",
    "from config.settings_test import *\n",
    "from utils.data_preprocess import *\n",
    "import dataset_loader as dl\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from utils.data_load import load_data_patches, generate_data_patches\n",
    "from DatasetsGeneratorFromFiles import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebda2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = lr[0] / math.pow((1 + 10 * (epoch - 1) / epochs), 0.75)\n",
    "    optimizer.param_groups[1]['lr'] = lr[1] / math.pow((1 + 10 * (epoch - 1) / epochs), 0.75)\n",
    "\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    correct = 0\n",
    "\n",
    "    #iter_source_train = iter(source_train_loader)\n",
    "    #num_iter_train = len_source_train_loader\n",
    "    for i, (data_source_train, label_source_train) in enumerate(train_generator.__getitem__()):\n",
    "        data_source_train, label_source_train = torch.from_numpy(data_source_train), torch.from_numpy(label_source_train)\n",
    "    #for i in range(1, num_iter_train):\n",
    "        #data_source_train, label_source_train = iter_source_train.next()\n",
    "        if cuda:\n",
    "            data_source_train, label_source_train = data_source_train.cuda(), label_source_train.cuda()\n",
    "        data_source_train, label_source_train = Variable(data_source_train), Variable(label_source_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        label_source_train_pred, _ = model(data_source_train)\n",
    "        # loss = F.cross_entropy(F.log_softmax(label_source_train_pred, dim=1), label_source_train.type(torch.long))\n",
    "        loss = F.cross_entropy(label_source_train_pred, label_source_train.type(torch.long), reduction='mean')\n",
    "        with torch.no_grad():\n",
    "            train_loss += loss\n",
    "            pred = label_source_train_pred.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(label_source_train.view_as(pred)).cpu().sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data_source_train), len_source_train_dataset,\n",
    "                100. * i / len_source_train_loader, loss.item()))\n",
    "\n",
    "    correct = correct.item()\n",
    "    correct_rate = correct / len_source_train_dataset\n",
    "    train_loss = train_loss.item() / len_source_train_loader\n",
    "    return correct_rate, train_loss\n",
    "\n",
    "\n",
    "def validate(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for i, (data_source_valid, label_source_valid) in enumerate(valid_generator.__getitem__()):\n",
    "            data_source_valid, label_source_valid = torch.from_numpy(data_source_valid), torch.from_numpy(label_source_valid)\n",
    "        #iter_source_valid = iter(source_valid_loader)\n",
    "        #num_iter_valid = len_source_valid_loader\n",
    "        #for i in range(1, num_iter_valid):\n",
    "            #data_source_valid, label_source_valid = iter_source_valid.next()\n",
    "            if cuda:\n",
    "                data_source_valid, label_source_valid = data_source_valid.cuda(), label_source_valid.cuda()\n",
    "            data_source_valid, label_source_valid = Variable(data_source_valid), Variable(label_source_valid)\n",
    "            s_output, _ = model(data_source_valid)\n",
    "            test_loss += F.cross_entropy(s_output, label_source_valid.type(torch.long), reduction='mean') # sum up batch loss\n",
    "            pred = s_output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(label_source_valid.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss = test_loss.item() / len_source_valid_loader\n",
    "        correct = correct.item()\n",
    "        correct_rate = correct / len_source_valid_dataset\n",
    "        print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            source_name, test_loss, correct, len_source_valid_dataset, 100. * correct_rate))\n",
    "\n",
    "        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\\n'.format(\n",
    "              source_name, '', correct, 100. * correct_rate))\n",
    "        return correct_rate, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9fc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# torch.cuda.set_device(1)\n",
    "#writer = SummaryWriter('runs')\n",
    "\n",
    "# Training settings\n",
    "st = Settings()\n",
    "options = st.get_options()\n",
    "\n",
    "second_train = options['second_train']\n",
    "pretrained_model = None\n",
    "train_count = options['train_count']\n",
    "if second_train:\n",
    "    pretrained_model = models.DANNet(num_classes=2)\n",
    "    pretrained_model_path = os.path.join(options['weight_paths'], options['experiment'], '1', options['pre_trained_model'])\n",
    "    pretrained_model = torch.load(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e253117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = options['batch_size']\n",
    "epochs = options['max_epochs']\n",
    "lr = [0.001, 0.01]\n",
    "momentum = 0.9\n",
    "no_cuda =False\n",
    "seed = options['seed']\n",
    "log_interval = 10\n",
    "l2_decay = 5e-4\n",
    "source_path = options['train_folder']\n",
    "source_name = 'ISBI'\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# resize images in path\n",
    "#resize_images(options)\n",
    "\n",
    "# generate csv file\n",
    "#df = generate_csv(options)\n",
    "\n",
    "# split data to train, validate folds\n",
    "#split_folds(options['train_csv_path'], options['seed'], options['k_fold'])\n",
    "\n",
    "# list scan\n",
    "fold = 0\n",
    "# fold train data\n",
    "df = pd.read_csv(options['train_csv_path'])\n",
    "# select training scans\n",
    "train_files = df.loc[df['fold'] != fold, ['patient_id', 'study']].values\n",
    "valid_files = df.loc[df['fold'] == fold, ['patient_id', 'study']].values\n",
    "train_scan_list = [f[0]+f[1] for f in train_files]\n",
    "valid_scan_list = [f[0]+f[1] for f in valid_files]\n",
    "\n",
    "train_scan_list.sort()\n",
    "valid_scan_list.sort()\n",
    "\n",
    "train_x_data = {f: {m: os.path.join(options['train_folder'], f, options['tmp_folder'], n)\n",
    "                    for m, n in zip(options['modalities'], options['preprocess_x_names'])}\n",
    "                for f in train_scan_list}\n",
    "train_y_data = {f: os.path.join(options['train_folder'], f, options['tmp_folder'],\n",
    "                                options['preprocess_y_names'][0])\n",
    "                for f in train_scan_list}\n",
    "\n",
    "valid_x_data = {f: {m: os.path.join(options['train_folder'], f, options['tmp_folder'], n)\n",
    "                    for m, n in zip(options['modalities'], options['preprocess_x_names'])}\n",
    "                for f in valid_scan_list}\n",
    "valid_y_data = {f: os.path.join(options['train_folder'], f, options['tmp_folder'],\n",
    "                                options['preprocess_y_names'][0])\n",
    "                for f in valid_scan_list}\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083f07bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad17cc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> DEBUG  training01_01 Voxels to classify: 1724143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:494: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1028740\n",
      "> DEBUG: testing current_batch: (1028740, 2, 16, 16, 16)\n",
      "...done!\n",
      "> DEBUG  training01_01 lesion volume below  0.5 ml\n",
      "shape (16776, 16, 16, 16)\n",
      "(16776, 2, 16, 16, 16) (16776,)\n",
      "(16776, 2, 16, 16, 16) patches 16776 modalities 16\n",
      "/home/mostafa/Marwa/DataSets/ISBI1/h5df_files2/file_training01_01.hdf5\n",
      "> DEBUG  training01_02 Voxels to classify: 1553044\n",
      "2 375191\n",
      "> DEBUG: testing current_batch: (375191, 2, 16, 16, 16)\n",
      "...done!\n",
      "> DEBUG  training01_02 lesion volume below  0.5 ml\n",
      "shape (16560, 16, 16, 16)\n",
      "(16560, 2, 16, 16, 16) (16560,)\n",
      "(16560, 2, 16, 16, 16) patches 16560 modalities 16\n",
      "/home/mostafa/Marwa/DataSets/ISBI1/h5df_files2/file_training01_02.hdf5\n"
     ]
    }
   ],
   "source": [
    "if second_train:\n",
    "    generate_data_patches(train_x_data, train_y_data, options, dataset_name='ISBI', model=pretrained_model)\n",
    "    #pass\n",
    "else:\n",
    "    generate_data_patches(train_x_data, train_y_data, options, dataset_name='ISBI')\n",
    "    #pass\n",
    "train_files, train_files_ref, train_patches = load_data_patches(options['h5_path'], options['train_csv_path'], phase='train', fold=fold, options=options)\n",
    "train_generator = DatasetGenerator(data=train_files, options=options, patches=train_patches)\n",
    "\n",
    "if second_train:\n",
    "    generate_data_patches(valid_x_data, valid_y_data, options, dataset_name='ISBI', model=pretrained_model)\n",
    "    #pass\n",
    "else:\n",
    "    generate_data_patches(valid_x_data, valid_y_data, options, dataset_name='ISBI')\n",
    "    #pass\n",
    "    \n",
    "valid_files, valid_files_ref, valid_patches = load_data_patches(options['h5_path'], options['train_csv_path'], phase='valid', fold=fold, options=options)\n",
    "valid_generator = DatasetGenerator(data=valid_files, options=options, patches=valid_patches)\n",
    "#source_train_loader = dl.load_training(options, train_x_data, train_y_data, model=pretrained_model)\n",
    "#source_valid_loader = dl.load_training(options, valid_x_data, valid_y_data, model=pretrained_model)\n",
    "\n",
    "#source_test_loader = data_loader.load_testing('', source_path, batch_size, kwargs)\n",
    "\n",
    "len_source_train_dataset = train_generator.__len__() * options['batch_size']\n",
    "len_source_valid_dataset = valid_generator.__len__() * options['batch_size']\n",
    "len_source_train_loader = train_generator.__len__()\n",
    "len_source_valid_loader = valid_generator.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3b0c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANNet(\n",
      "  (sharedNet): ResNet(\n",
      "    (conv1): Conv3d(2, 64, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool3d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (cls_fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (cls_fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = models.DANNet(num_classes=2)\n",
    "if options['load_initial_weights']:\n",
    "    model = torch.load(options['initial_weights_file'])\n",
    "    print(options['initial_weights_file'])\n",
    "elif options['save_initial_weights']:\n",
    "    Path(options['initial_weights_path']).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model, options['initial_weights_file'])\n",
    "#writer.add_graph(model, torch.rand(size=(128,2,16,16,16)))\n",
    "#writer.flush()\n",
    "#writer.close()\n",
    "#sys.exit()\n",
    "correct = 0\n",
    "print(model)\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a0752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mostafa/Marwa/DataSets/ISBI1/train/resnet_DAN_full_isbi_train_dropout_test_cascaded_2_history_data.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options['history_csv_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf6426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b706e71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/16896 (0%)]\tLoss: 0.610307\n",
      "Train Epoch: 1 [1280/16896 (8%)]\tLoss: 0.213794\n",
      "Train Epoch: 1 [2560/16896 (15%)]\tLoss: 0.101769\n",
      "Train Epoch: 1 [3840/16896 (23%)]\tLoss: 0.168999\n",
      "Train Epoch: 1 [5120/16896 (30%)]\tLoss: 0.095871\n",
      "Train Epoch: 1 [6400/16896 (38%)]\tLoss: 0.101786\n",
      "Train Epoch: 1 [7680/16896 (45%)]\tLoss: 0.049578\n",
      "Train Epoch: 1 [8960/16896 (53%)]\tLoss: 0.227126\n",
      "Train Epoch: 1 [10240/16896 (61%)]\tLoss: 0.338351\n",
      "Train Epoch: 1 [11520/16896 (68%)]\tLoss: 0.313772\n",
      "Train Epoch: 1 [12800/16896 (76%)]\tLoss: 0.121754\n",
      "Train Epoch: 1 [14080/16896 (83%)]\tLoss: 0.176736\n",
      "Train Epoch: 1 [15360/16896 (91%)]\tLoss: 0.135410\n",
      "Train Epoch: 1 [16640/16896 (98%)]\tLoss: 0.160511\n",
      "\n",
      "ISBI set: Average loss: 0.4933, Accuracy: 15047/16640 (90.43%)\n",
      "\n",
      "source: ISBI to target:  max correct: 15047 max accuracy 90.43%\n",
      "\n",
      "patience:  0\n",
      "Train Epoch: 2 [0/16896 (0%)]\tLoss: 0.150192\n",
      "Train Epoch: 2 [1280/16896 (8%)]\tLoss: 0.272697\n",
      "Train Epoch: 2 [2560/16896 (15%)]\tLoss: 0.203181\n",
      "Train Epoch: 2 [3840/16896 (23%)]\tLoss: 0.285077\n",
      "Train Epoch: 2 [5120/16896 (30%)]\tLoss: 0.145758\n",
      "Train Epoch: 2 [6400/16896 (38%)]\tLoss: 0.078496\n",
      "Train Epoch: 2 [7680/16896 (45%)]\tLoss: 0.090434\n",
      "Train Epoch: 2 [8960/16896 (53%)]\tLoss: 0.120628\n",
      "Train Epoch: 2 [10240/16896 (61%)]\tLoss: 0.189834\n",
      "Train Epoch: 2 [11520/16896 (68%)]\tLoss: 0.246019\n",
      "Train Epoch: 2 [12800/16896 (76%)]\tLoss: 0.153422\n",
      "Train Epoch: 2 [14080/16896 (83%)]\tLoss: 0.082645\n",
      "Train Epoch: 2 [15360/16896 (91%)]\tLoss: 0.139271\n",
      "Train Epoch: 2 [16640/16896 (98%)]\tLoss: 0.114821\n",
      "\n",
      "ISBI set: Average loss: 0.5294, Accuracy: 15047/16640 (90.43%)\n",
      "\n",
      "source: ISBI to target:  max correct: 15047 max accuracy 90.43%\n",
      "\n",
      "patience:  1\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD([\n",
    "    {'params': model.sharedNet.parameters()},\n",
    "    {'params': model.cls_fc.parameters(), 'lr': lr[1]},\n",
    "    ], lr=lr[0], momentum=momentum, weight_decay=l2_decay)\n",
    "path= os.path.join(options['weight_paths'], options['experiment'], train_count)\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "history_df = pd.DataFrame(columns=['lr', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "if os.path.isfile(options['history_csv_path']):\n",
    "    history_df = pd.read_csv(options['history_csv_path'])\n",
    "patience = options['patience']\n",
    "patience_value = 0\n",
    "epochs=2\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_correct, train_loss = train(epoch, model, optimizer)\n",
    "    #torch.cuda.synchronize()\n",
    "    t_correct, test_loss = validate(model)\n",
    "\n",
    "    FILE = os.path.join(path, str(epoch)+'_model.pth')\n",
    "    torch.save(model, FILE)\n",
    "    if t_correct > correct:\n",
    "        correct = t_correct\n",
    "        patience_value = 0\n",
    "    else:\n",
    "        patience_value += 1\n",
    "    print('patience: ', patience_value)\n",
    "    df = pd.DataFrame([[lr[0], train_loss, train_correct, test_loss,  t_correct]], columns=['lr', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "    history_df = history_df.append(df)\n",
    "\n",
    "    history_df.reset_index(inplace=True)\n",
    "    history_df.drop(columns=['index'], inplace=True)\n",
    "    history_df.to_csv(options['history_csv_path'], index=False)\n",
    "    if patience_value >= patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d742fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa0371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1025c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path= options['h5_path']\n",
    "train_csv_path = options['train_csv_path']\n",
    "f5_path_column_name = 'f5_path' + options['train_count']\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "    \n",
    "x_dict = train_x_data\n",
    "y_dict = train_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265cf4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training01_01'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = list(x_dict.keys())[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad22a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5db77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_data = {idx: y_dict[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7eed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = list(train_x_data.keys())\n",
    "modalities = train_x_data[scans[0]].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2dd9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['training01_01'], dict_keys(['FLAIR', 'T1']))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scans, modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377cdd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DANNet(\n",
       "  (sharedNet): ResNet(\n",
       "    (conv1): Conv3d(2, 64, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool3d(kernel_size=1, stride=1, padding=0)\n",
       "  )\n",
       "  (cls_fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (cls_fc): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= pretrained_model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2869e416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['training01_01']), ['training01_01'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scans = list(train_x_data.keys())\n",
    "train_x_data.keys(),scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e0ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scan\n",
    "candidate_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80826dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=0\n",
    "test_x_data = dict(list(train_x_data.items())[s:s+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e83976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Environments/deep_transfer_learning_env/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> DEBUG  training01_01 Voxels to classify: 1724143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Environments/deep_transfer_learning_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "scans = list(test_x_data.keys())\n",
    "flair_scans = [test_x_data[s]['FLAIR'] for s in scans]\n",
    "flair_image = load_nii(flair_scans[0])\n",
    "seg_image = np.zeros_like(flair_image.get_data().astype('float32'))\n",
    "\n",
    "if candidate_mask is not None:\n",
    "    all_voxels = np.sum(candidate_mask)\n",
    "else:\n",
    "    all_voxels = np.sum(flair_image.get_data() > 0)\n",
    "\n",
    "if options['debug'] is True:\n",
    "        print (\"> DEBUG \", scans[0], \"Voxels to classify:\", all_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f798df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_test_patches\n",
    "voxel_candidates = candidate_mask\n",
    "patch_size =options['patch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa7180e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94fa25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Environments/deep_transfer_learning_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "scans = list(test_x_data.keys())\n",
    "modalities = list(test_x_data[scans[0]].keys())\n",
    "\n",
    "# load all image modalities and normalize intensities\n",
    "images = []\n",
    "\n",
    "for m in modalities:\n",
    "    raw_images = [load_nii(test_x_data[s][m]).get_data() for s in scans]\n",
    "    images.append([normalize_data(im) for im in raw_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c33897dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if voxel_candidates is None:\n",
    "    flair_scans = [test_x_data[s]['FLAIR'] for s in scans]\n",
    "    selected_voxels = [get_mask_voxels(mask)\n",
    "                       for mask in select_training_voxels(flair_scans,\n",
    "                                                          0.6)][0]\n",
    "else:\n",
    "    selected_voxels = get_mask_voxels(voxel_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ae61640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914984"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96e9125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914984 (16, 16, 16)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "append() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8697/2380414789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_modality\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_voxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: append() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for image_modality in images:\n",
    "    d=get_patches(image_modality[0], selected_voxels, patch_size)\n",
    "    print(len(d),d[0].shape)\n",
    "    X.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b8019f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ef6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.stack(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b8b8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1028740\n",
      "(1028740, 32, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "# compute lesion segmentation in batches of size options['batch_size']\n",
    "batch, centers = load_test_patches(test_x_data,\n",
    "                                   options['patch_size'],\n",
    "                                   options['batch_size'],\n",
    "                                   candidate_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbc97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06c2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de1042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e71c237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options['patch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3207808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257185.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56787564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "dd=[]\n",
    "for i in range(8):\n",
    "    print(i)\n",
    "    start=i * len(X[0])//4\n",
    "    end=start + len(X[0])//4\n",
    "    if i ==7:\n",
    "        end=len(X[0])\n",
    "    dd.append( np.concatenate(X[:][start:end], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=[]\n",
    "for i in range(4):\n",
    "    start=i * len(X[0])//4\n",
    "    end=start + len(X[0])//4\n",
    "    if i ==3:\n",
    "        end=len(X[0])\n",
    "    if i==0:\n",
    "        dd.append( np.concatenate(X[:][start:end], axis=1))\n",
    "    else:\n",
    "        xx=np.concatenate(X[:][start:end], axis=1)\n",
    "        Xs = np.concatenate((Xs, xx), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75428565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.concatenate(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c6dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81827893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:494: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    }
   ],
   "source": [
    "modality_data1 = images[0]\n",
    "modality_data2 = images[1]\n",
    "modality_patches1=get_patches(modality_data1[0], selected_voxels, patch_size)\n",
    "modality_patches2=get_patches(modality_data2[0], selected_voxels, patch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74b0a6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1028740, (32, 32, 32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modality_patches1), modality_patches1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30459d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_len=len(modality_patches1)//4\n",
    "ss1=modality_patches1[:half_len]\n",
    "modality_data_array1 = np.array(ss1)\n",
    "#ss2=modality_patches1[half_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ss1),ss1[0].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t_start = time.time()\n",
    "modality_data_array1 = np.array(ss1)\n",
    "#modality_data_tensor2 = torch.tensor(modality_patches2)\n",
    "t_end =time.time()\n",
    "print(t_end - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t_start = time.time()\n",
    "modality_data_tensor1 = torch.tensor(modality_patches1)\n",
    "#modality_data_tensor2 = torch.tensor(modality_patches2)\n",
    "t_end =time.time()\n",
    "print(t_end - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "769f2865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 181, 217, 181])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modality_data_tensor1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d1d3c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 362, 217, 181])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.cat((modality_data1, modality_data2),1)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:494: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patches\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "X = []\n",
    "t_start = time.time()\n",
    "for image_modality in images:\n",
    "    modality_patches=get_patches(image_modality[0], selected_voxels, patch_size)\n",
    "    modality_patches=np.array(modality_patches)\n",
    "    print('patches')\n",
    "    X.append(modality_patches)\n",
    "t_end =time.time()\n",
    "print(t_end - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc7eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:494: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.459554433822632\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4be9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413ff3e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29199/3376011005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'history_csv_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'options' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(options['history_csv_path'])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:494: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    }
   ],
   "source": [
    "X = None\n",
    "for image_modality in images:\n",
    "    modality_patches = get_patches(image_modality[0], selected_voxels, patch_size)\n",
    "    if X is None:\n",
    "        X = modality_patches\n",
    "    else:\n",
    "        np.concatenate((X, modality_patches), axis=1)\n",
    "    X.append()\n",
    "\n",
    "#print(len(X), len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e64f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6799fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = []\n",
    "list_of_tuples = all([isinstance(center, tuple) for center in centers])\n",
    "sizes_match = [len(center) == len(patch_size) for center in centers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c04f3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "if list_of_tuples and sizes_match:\n",
    "    patch_half = tuple([idx//2 for idx in patch_size])\n",
    "    new_centers = [map(add, center, patch_half) for center in centers]\n",
    "    padding = tuple((idx, size-idx)\n",
    "                    for idx, size in zip(patch_half, patch_size))\n",
    "    new_image = np.pad(image, padding, mode='constant', constant_values=0)\n",
    "    slices = [[slice(c_idx-p_idx, c_idx+(s_idx-p_idx))\n",
    "               for (c_idx, p_idx, s_idx) in zip(center,\n",
    "                                                patch_half,\n",
    "                                                patch_size)]\n",
    "              for center in new_centers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79f27f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slices)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfb83c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Environments/deep_transfer_learning_env/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.712451219558716"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_start=time.time()\n",
    "patches = [new_image[idx] for idx in slices]\n",
    "patches=np.array(patches)\n",
    "t_end= time.time()\n",
    "t_end - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f20f129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1028740, 32, 32, 32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c07049cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "/rid: 0 of: 1028740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Environments/deep_transfer_learning_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 1 of: 1028740\n",
      "/rid: 2 of: 1028740\n",
      "/rid: 3 of: 1028740\n",
      "/rid: 4 of: 1028740\n",
      "/rid: 5 of: 1028740\n",
      "/rid: 6 of: 1028740\n",
      "/rid: 7 of: 1028740\n",
      "/rid: 8 of: 1028740\n",
      "/rid: 9 of: 1028740\n",
      "/rid: 10 of: 1028740\n",
      "/rid: 11 of: 1028740\n",
      "/rid: 12 of: 1028740\n",
      "/rid: 13 of: 1028740\n",
      "/rid: 14 of: 1028740\n",
      "/rid: 15 of: 1028740\n",
      "/rid: 16 of: 1028740\n",
      "/rid: 17 of: 1028740\n",
      "/rid: 18 of: 1028740\n",
      "/rid: 19 of: 1028740\n",
      "/rid: 20 of: 1028740\n",
      "/rid: 21 of: 1028740\n",
      "/rid: 22 of: 1028740\n",
      "/rid: 23 of: 1028740\n",
      "/rid: 24 of: 1028740\n",
      "/rid: 25 of: 1028740\n",
      "/rid: 26 of: 1028740\n",
      "/rid: 27 of: 1028740\n",
      "/rid: 28 of: 1028740\n",
      "/rid: 29 of: 1028740\n",
      "/rid: 30 of: 1028740\n",
      "/rid: 31 of: 1028740\n",
      "/rid: 32 of: 1028740\n",
      "/rid: 33 of: 1028740\n",
      "/rid: 34 of: 1028740\n",
      "/rid: 35 of: 1028740\n",
      "/rid: 36 of: 1028740\n",
      "/rid: 37 of: 1028740\n",
      "/rid: 38 of: 1028740\n",
      "/rid: 39 of: 1028740\n",
      "/rid: 40 of: 1028740\n",
      "/rid: 41 of: 1028740\n",
      "/rid: 42 of: 1028740\n",
      "/rid: 43 of: 1028740\n",
      "/rid: 44 of: 1028740\n",
      "/rid: 45 of: 1028740\n",
      "/rid: 46 of: 1028740\n",
      "/rid: 47 of: 1028740\n",
      "/rid: 48 of: 1028740\n",
      "/rid: 49 of: 1028740\n",
      "/rid: 50 of: 1028740\n",
      "/rid: 51 of: 1028740\n",
      "/rid: 52 of: 1028740\n",
      "/rid: 53 of: 1028740\n",
      "/rid: 54 of: 1028740\n",
      "/rid: 55 of: 1028740\n",
      "/rid: 56 of: 1028740\n",
      "/rid: 57 of: 1028740\n",
      "/rid: 58 of: 1028740\n",
      "/rid: 59 of: 1028740\n",
      "/rid: 60 of: 1028740\n",
      "/rid: 61 of: 1028740\n",
      "/rid: 62 of: 1028740\n",
      "/rid: 63 of: 1028740\n",
      "/rid: 64 of: 1028740\n",
      "/rid: 65 of: 1028740\n",
      "/rid: 66 of: 1028740\n",
      "/rid: 67 of: 1028740\n",
      "/rid: 68 of: 1028740\n",
      "/rid: 69 of: 1028740\n",
      "/rid: 70 of: 1028740\n",
      "/rid: 71 of: 1028740\n",
      "/rid: 72 of: 1028740\n",
      "/rid: 73 of: 1028740\n",
      "/rid: 74 of: 1028740\n",
      "/rid: 75 of: 1028740\n",
      "/rid: 76 of: 1028740\n",
      "/rid: 77 of: 1028740\n",
      "/rid: 78 of: 1028740\n",
      "/rid: 79 of: 1028740\n",
      "/rid: 80 of: 1028740\n",
      "/rid: 81 of: 1028740\n",
      "/rid: 82 of: 1028740\n",
      "/rid: 83 of: 1028740\n",
      "/rid: 84 of: 1028740\n",
      "/rid: 85 of: 1028740\n",
      "/rid: 86 of: 1028740\n",
      "/rid: 87 of: 1028740\n",
      "/rid: 88 of: 1028740\n",
      "/rid: 89 of: 1028740\n",
      "/rid: 90 of: 1028740\n",
      "/rid: 91 of: 1028740\n",
      "/rid: 92 of: 1028740\n",
      "/rid: 93 of: 1028740\n",
      "/rid: 94 of: 1028740\n",
      "/rid: 95 of: 1028740\n",
      "/rid: 96 of: 1028740\n",
      "/rid: 97 of: 1028740\n",
      "/rid: 98 of: 1028740\n",
      "/rid: 99 of: 1028740\n",
      "/rid: 100 of: 1028740\n",
      "/rid: 101 of: 1028740\n",
      "/rid: 102 of: 1028740\n",
      "/rid: 103 of: 1028740\n",
      "/rid: 104 of: 1028740\n",
      "/rid: 105 of: 1028740\n",
      "/rid: 106 of: 1028740\n",
      "/rid: 107 of: 1028740\n",
      "/rid: 108 of: 1028740\n",
      "/rid: 109 of: 1028740\n",
      "/rid: 110 of: 1028740\n",
      "/rid: 111 of: 1028740\n",
      "/rid: 112 of: 1028740\n",
      "/rid: 113 of: 1028740\n",
      "/rid: 114 of: 1028740\n",
      "/rid: 115 of: 1028740\n",
      "/rid: 116 of: 1028740\n",
      "/rid: 117 of: 1028740\n",
      "/rid: 118 of: 1028740\n",
      "/rid: 119 of: 1028740\n",
      "/rid: 120 of: 1028740\n",
      "/rid: 121 of: 1028740\n",
      "/rid: 122 of: 1028740\n",
      "/rid: 123 of: 1028740\n",
      "/rid: 124 of: 1028740\n",
      "/rid: 125 of: 1028740\n",
      "/rid: 126 of: 1028740\n",
      "/rid: 127 of: 1028740\n",
      "/rid: 128 of: 1028740\n",
      "/rid: 129 of: 1028740\n",
      "/rid: 130 of: 1028740\n",
      "/rid: 131 of: 1028740\n",
      "/rid: 132 of: 1028740\n",
      "/rid: 133 of: 1028740\n",
      "/rid: 134 of: 1028740\n",
      "/rid: 135 of: 1028740\n",
      "/rid: 136 of: 1028740\n",
      "/rid: 137 of: 1028740\n",
      "/rid: 138 of: 1028740\n",
      "/rid: 139 of: 1028740\n",
      "/rid: 140 of: 1028740\n",
      "/rid: 141 of: 1028740\n",
      "/rid: 142 of: 1028740\n",
      "/rid: 143 of: 1028740\n",
      "/rid: 144 of: 1028740\n",
      "/rid: 145 of: 1028740\n",
      "/rid: 146 of: 1028740\n",
      "/rid: 147 of: 1028740\n",
      "/rid: 148 of: 1028740\n",
      "/rid: 149 of: 1028740\n",
      "/rid: 150 of: 1028740\n",
      "/rid: 151 of: 1028740\n",
      "/rid: 152 of: 1028740\n",
      "/rid: 153 of: 1028740\n",
      "/rid: 154 of: 1028740\n",
      "/rid: 155 of: 1028740\n",
      "/rid: 156 of: 1028740\n",
      "/rid: 157 of: 1028740\n",
      "/rid: 158 of: 1028740\n",
      "/rid: 159 of: 1028740\n",
      "/rid: 160 of: 1028740\n",
      "/rid: 161 of: 1028740\n",
      "/rid: 162 of: 1028740\n",
      "/rid: 163 of: 1028740\n",
      "/rid: 164 of: 1028740\n",
      "/rid: 165 of: 1028740\n",
      "/rid: 166 of: 1028740\n",
      "/rid: 167 of: 1028740\n",
      "/rid: 168 of: 1028740\n",
      "/rid: 169 of: 1028740\n",
      "/rid: 170 of: 1028740\n",
      "/rid: 171 of: 1028740\n",
      "/rid: 172 of: 1028740\n",
      "/rid: 173 of: 1028740\n",
      "/rid: 174 of: 1028740\n",
      "/rid: 175 of: 1028740\n",
      "/rid: 176 of: 1028740\n",
      "/rid: 177 of: 1028740\n",
      "/rid: 178 of: 1028740\n",
      "/rid: 179 of: 1028740\n",
      "/rid: 180 of: 1028740\n",
      "/rid: 181 of: 1028740\n",
      "/rid: 182 of: 1028740\n",
      "/rid: 183 of: 1028740\n",
      "/rid: 184 of: 1028740\n",
      "/rid: 185 of: 1028740\n",
      "/rid: 186 of: 1028740\n",
      "/rid: 187 of: 1028740\n",
      "/rid: 188 of: 1028740\n",
      "/rid: 189 of: 1028740\n",
      "/rid: 190 of: 1028740\n",
      "/rid: 191 of: 1028740\n",
      "/rid: 192 of: 1028740\n",
      "/rid: 193 of: 1028740\n",
      "/rid: 194 of: 1028740\n",
      "/rid: 195 of: 1028740\n",
      "/rid: 196 of: 1028740\n",
      "/rid: 197 of: 1028740\n",
      "/rid: 198 of: 1028740\n",
      "/rid: 199 of: 1028740\n",
      "/rid: 200 of: 1028740\n",
      "/rid: 201 of: 1028740\n",
      "/rid: 202 of: 1028740\n",
      "/rid: 203 of: 1028740\n",
      "/rid: 204 of: 1028740\n",
      "/rid: 205 of: 1028740\n",
      "/rid: 206 of: 1028740\n",
      "/rid: 207 of: 1028740\n",
      "/rid: 208 of: 1028740\n",
      "/rid: 209 of: 1028740\n",
      "/rid: 210 of: 1028740\n",
      "/rid: 211 of: 1028740\n",
      "/rid: 212 of: 1028740\n",
      "/rid: 213 of: 1028740\n",
      "/rid: 214 of: 1028740\n",
      "/rid: 215 of: 1028740\n",
      "/rid: 216 of: 1028740\n",
      "/rid: 217 of: 1028740\n",
      "/rid: 218 of: 1028740\n",
      "/rid: 219 of: 1028740\n",
      "/rid: 220 of: 1028740\n",
      "/rid: 221 of: 1028740\n",
      "/rid: 222 of: 1028740\n",
      "/rid: 223 of: 1028740\n",
      "/rid: 224 of: 1028740\n",
      "/rid: 225 of: 1028740\n",
      "/rid: 226 of: 1028740\n",
      "/rid: 227 of: 1028740\n",
      "/rid: 228 of: 1028740\n",
      "/rid: 229 of: 1028740\n",
      "/rid: 230 of: 1028740\n",
      "/rid: 231 of: 1028740\n",
      "/rid: 232 of: 1028740\n",
      "/rid: 233 of: 1028740\n",
      "/rid: 234 of: 1028740\n",
      "/rid: 235 of: 1028740\n",
      "/rid: 236 of: 1028740\n",
      "/rid: 237 of: 1028740\n",
      "/rid: 238 of: 1028740\n",
      "/rid: 239 of: 1028740\n",
      "/rid: 240 of: 1028740\n",
      "/rid: 241 of: 1028740\n",
      "/rid: 242 of: 1028740\n",
      "/rid: 243 of: 1028740\n",
      "/rid: 244 of: 1028740\n",
      "/rid: 245 of: 1028740\n",
      "/rid: 246 of: 1028740\n",
      "/rid: 247 of: 1028740\n",
      "/rid: 248 of: 1028740\n",
      "/rid: 249 of: 1028740\n",
      "/rid: 250 of: 1028740\n",
      "/rid: 251 of: 1028740\n",
      "/rid: 252 of: 1028740\n",
      "/rid: 253 of: 1028740\n",
      "/rid: 254 of: 1028740\n",
      "/rid: 255 of: 1028740\n",
      "/rid: 256 of: 1028740\n",
      "/rid: 257 of: 1028740\n",
      "/rid: 258 of: 1028740\n",
      "/rid: 259 of: 1028740\n",
      "/rid: 260 of: 1028740\n",
      "/rid: 261 of: 1028740\n",
      "/rid: 262 of: 1028740\n",
      "/rid: 263 of: 1028740\n",
      "/rid: 264 of: 1028740\n",
      "/rid: 265 of: 1028740\n",
      "/rid: 266 of: 1028740\n",
      "/rid: 267 of: 1028740\n",
      "/rid: 268 of: 1028740\n",
      "/rid: 269 of: 1028740\n",
      "/rid: 270 of: 1028740\n",
      "/rid: 271 of: 1028740\n",
      "/rid: 272 of: 1028740\n",
      "/rid: 273 of: 1028740\n",
      "/rid: 274 of: 1028740\n",
      "/rid: 275 of: 1028740\n",
      "/rid: 276 of: 1028740\n",
      "/rid: 277 of: 1028740\n",
      "/rid: 278 of: 1028740\n",
      "/rid: 279 of: 1028740\n",
      "/rid: 280 of: 1028740\n",
      "/rid: 281 of: 1028740\n",
      "/rid: 282 of: 1028740\n",
      "/rid: 283 of: 1028740\n",
      "/rid: 284 of: 1028740\n",
      "/rid: 285 of: 1028740\n",
      "/rid: 286 of: 1028740\n",
      "/rid: 287 of: 1028740\n",
      "/rid: 288 of: 1028740\n",
      "/rid: 289 of: 1028740\n",
      "/rid: 290 of: 1028740\n",
      "/rid: 291 of: 1028740\n",
      "/rid: 292 of: 1028740\n",
      "/rid: 293 of: 1028740\n",
      "/rid: 294 of: 1028740\n",
      "/rid: 295 of: 1028740\n",
      "/rid: 296 of: 1028740\n",
      "/rid: 297 of: 1028740\n",
      "/rid: 298 of: 1028740\n",
      "/rid: 299 of: 1028740\n",
      "/rid: 300 of: 1028740\n",
      "/rid: 301 of: 1028740\n",
      "/rid: 302 of: 1028740\n",
      "/rid: 303 of: 1028740\n",
      "/rid: 304 of: 1028740\n",
      "/rid: 305 of: 1028740\n",
      "/rid: 306 of: 1028740\n",
      "/rid: 307 of: 1028740\n",
      "/rid: 308 of: 1028740\n",
      "/rid: 309 of: 1028740\n",
      "/rid: 310 of: 1028740\n",
      "/rid: 311 of: 1028740\n",
      "/rid: 312 of: 1028740\n",
      "/rid: 313 of: 1028740\n",
      "/rid: 314 of: 1028740\n",
      "/rid: 315 of: 1028740\n",
      "/rid: 316 of: 1028740\n",
      "/rid: 317 of: 1028740\n",
      "/rid: 318 of: 1028740\n",
      "/rid: 319 of: 1028740\n",
      "/rid: 320 of: 1028740\n",
      "/rid: 321 of: 1028740\n",
      "/rid: 322 of: 1028740\n",
      "/rid: 323 of: 1028740\n",
      "/rid: 324 of: 1028740\n",
      "/rid: 325 of: 1028740\n",
      "/rid: 326 of: 1028740\n",
      "/rid: 327 of: 1028740\n",
      "/rid: 328 of: 1028740\n",
      "/rid: 329 of: 1028740\n",
      "/rid: 330 of: 1028740\n",
      "/rid: 331 of: 1028740\n",
      "/rid: 332 of: 1028740\n",
      "/rid: 333 of: 1028740\n",
      "/rid: 334 of: 1028740\n",
      "/rid: 335 of: 1028740\n",
      "/rid: 336 of: 1028740\n",
      "/rid: 337 of: 1028740\n",
      "/rid: 338 of: 1028740\n",
      "/rid: 339 of: 1028740\n",
      "/rid: 340 of: 1028740\n",
      "/rid: 341 of: 1028740\n",
      "/rid: 342 of: 1028740\n",
      "/rid: 343 of: 1028740\n",
      "/rid: 344 of: 1028740\n",
      "/rid: 345 of: 1028740\n",
      "/rid: 346 of: 1028740\n",
      "/rid: 347 of: 1028740\n",
      "/rid: 348 of: 1028740\n",
      "/rid: 349 of: 1028740\n",
      "/rid: 350 of: 1028740\n",
      "/rid: 351 of: 1028740\n",
      "/rid: 352 of: 1028740\n",
      "/rid: 353 of: 1028740\n",
      "/rid: 354 of: 1028740\n",
      "/rid: 355 of: 1028740\n",
      "/rid: 356 of: 1028740\n",
      "/rid: 357 of: 1028740\n",
      "/rid: 358 of: 1028740\n",
      "/rid: 359 of: 1028740\n",
      "/rid: 360 of: 1028740\n",
      "/rid: 361 of: 1028740\n",
      "/rid: 362 of: 1028740\n",
      "/rid: 363 of: 1028740\n",
      "/rid: 364 of: 1028740\n",
      "/rid: 365 of: 1028740\n",
      "/rid: 366 of: 1028740\n",
      "/rid: 367 of: 1028740\n",
      "/rid: 368 of: 1028740\n",
      "/rid: 369 of: 1028740\n",
      "/rid: 370 of: 1028740\n",
      "/rid: 371 of: 1028740\n",
      "/rid: 372 of: 1028740\n",
      "/rid: 373 of: 1028740\n",
      "/rid: 374 of: 1028740\n",
      "/rid: 375 of: 1028740\n",
      "/rid: 376 of: 1028740\n",
      "/rid: 377 of: 1028740\n",
      "/rid: 378 of: 1028740\n",
      "/rid: 379 of: 1028740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 380 of: 1028740\n",
      "/rid: 381 of: 1028740\n",
      "/rid: 382 of: 1028740\n",
      "/rid: 383 of: 1028740\n",
      "/rid: 384 of: 1028740\n",
      "/rid: 385 of: 1028740\n",
      "/rid: 386 of: 1028740\n",
      "/rid: 387 of: 1028740\n",
      "/rid: 388 of: 1028740\n",
      "/rid: 389 of: 1028740\n",
      "/rid: 390 of: 1028740\n",
      "/rid: 391 of: 1028740\n",
      "/rid: 392 of: 1028740\n",
      "/rid: 393 of: 1028740\n",
      "/rid: 394 of: 1028740\n",
      "/rid: 395 of: 1028740\n",
      "/rid: 396 of: 1028740\n",
      "/rid: 397 of: 1028740\n",
      "/rid: 398 of: 1028740\n",
      "/rid: 399 of: 1028740\n",
      "/rid: 400 of: 1028740\n",
      "/rid: 401 of: 1028740\n",
      "/rid: 402 of: 1028740\n",
      "/rid: 403 of: 1028740\n",
      "/rid: 404 of: 1028740\n",
      "/rid: 405 of: 1028740\n",
      "/rid: 406 of: 1028740\n",
      "/rid: 407 of: 1028740\n",
      "/rid: 408 of: 1028740\n",
      "/rid: 409 of: 1028740\n",
      "/rid: 410 of: 1028740\n",
      "/rid: 411 of: 1028740\n",
      "/rid: 412 of: 1028740\n",
      "/rid: 413 of: 1028740\n",
      "/rid: 414 of: 1028740\n",
      "/rid: 415 of: 1028740\n",
      "/rid: 416 of: 1028740\n",
      "/rid: 417 of: 1028740\n",
      "/rid: 418 of: 1028740\n",
      "/rid: 419 of: 1028740\n",
      "/rid: 420 of: 1028740\n",
      "/rid: 421 of: 1028740\n",
      "/rid: 422 of: 1028740\n",
      "/rid: 423 of: 1028740\n",
      "/rid: 424 of: 1028740\n",
      "/rid: 425 of: 1028740\n",
      "/rid: 426 of: 1028740\n",
      "/rid: 427 of: 1028740\n",
      "/rid: 428 of: 1028740\n",
      "/rid: 429 of: 1028740\n",
      "/rid: 430 of: 1028740\n",
      "/rid: 431 of: 1028740\n",
      "/rid: 432 of: 1028740\n",
      "/rid: 433 of: 1028740\n",
      "/rid: 434 of: 1028740\n",
      "/rid: 435 of: 1028740\n",
      "/rid: 436 of: 1028740\n",
      "/rid: 437 of: 1028740\n",
      "/rid: 438 of: 1028740\n",
      "/rid: 439 of: 1028740\n",
      "/rid: 440 of: 1028740\n",
      "/rid: 441 of: 1028740\n",
      "/rid: 442 of: 1028740\n",
      "/rid: 443 of: 1028740\n",
      "/rid: 444 of: 1028740\n",
      "/rid: 445 of: 1028740\n",
      "/rid: 446 of: 1028740\n",
      "/rid: 447 of: 1028740\n",
      "/rid: 448 of: 1028740\n",
      "/rid: 449 of: 1028740\n",
      "/rid: 450 of: 1028740\n",
      "/rid: 451 of: 1028740\n",
      "/rid: 452 of: 1028740\n",
      "/rid: 453 of: 1028740\n",
      "/rid: 454 of: 1028740\n",
      "/rid: 455 of: 1028740\n",
      "/rid: 456 of: 1028740\n",
      "/rid: 457 of: 1028740\n",
      "/rid: 458 of: 1028740\n",
      "/rid: 459 of: 1028740\n",
      "/rid: 460 of: 1028740\n",
      "/rid: 461 of: 1028740\n",
      "/rid: 462 of: 1028740\n",
      "/rid: 463 of: 1028740\n",
      "/rid: 464 of: 1028740\n",
      "/rid: 465 of: 1028740\n",
      "/rid: 466 of: 1028740\n",
      "/rid: 467 of: 1028740\n",
      "/rid: 468 of: 1028740\n",
      "/rid: 469 of: 1028740\n",
      "/rid: 470 of: 1028740\n",
      "/rid: 471 of: 1028740\n",
      "/rid: 472 of: 1028740\n",
      "/rid: 473 of: 1028740\n",
      "/rid: 474 of: 1028740\n",
      "/rid: 475 of: 1028740\n",
      "/rid: 476 of: 1028740\n",
      "/rid: 477 of: 1028740\n",
      "/rid: 478 of: 1028740\n",
      "/rid: 479 of: 1028740\n",
      "/rid: 480 of: 1028740\n",
      "/rid: 481 of: 1028740\n",
      "/rid: 482 of: 1028740\n",
      "/rid: 483 of: 1028740\n",
      "/rid: 484 of: 1028740\n",
      "/rid: 485 of: 1028740\n",
      "/rid: 486 of: 1028740\n",
      "/rid: 487 of: 1028740\n",
      "/rid: 488 of: 1028740\n",
      "/rid: 489 of: 1028740\n",
      "/rid: 490 of: 1028740\n",
      "/rid: 491 of: 1028740\n",
      "/rid: 492 of: 1028740\n",
      "/rid: 493 of: 1028740\n",
      "/rid: 494 of: 1028740\n",
      "/rid: 495 of: 1028740\n",
      "/rid: 496 of: 1028740\n",
      "/rid: 497 of: 1028740\n",
      "/rid: 498 of: 1028740\n",
      "/rid: 499 of: 1028740\n",
      "/rid: 500 of: 1028740\n",
      "/rid: 501 of: 1028740\n",
      "/rid: 502 of: 1028740\n",
      "/rid: 503 of: 1028740\n",
      "/rid: 504 of: 1028740\n",
      "/rid: 505 of: 1028740\n",
      "/rid: 506 of: 1028740\n",
      "/rid: 507 of: 1028740\n",
      "/rid: 508 of: 1028740\n",
      "/rid: 509 of: 1028740\n",
      "/rid: 510 of: 1028740\n",
      "/rid: 511 of: 1028740\n",
      "/rid: 512 of: 1028740\n",
      "/rid: 513 of: 1028740\n",
      "/rid: 514 of: 1028740\n",
      "/rid: 515 of: 1028740\n",
      "/rid: 516 of: 1028740\n",
      "/rid: 517 of: 1028740\n",
      "/rid: 518 of: 1028740\n",
      "/rid: 519 of: 1028740\n",
      "/rid: 520 of: 1028740\n",
      "/rid: 521 of: 1028740\n",
      "/rid: 522 of: 1028740\n",
      "/rid: 523 of: 1028740\n",
      "/rid: 524 of: 1028740\n",
      "/rid: 525 of: 1028740\n",
      "/rid: 526 of: 1028740\n",
      "/rid: 527 of: 1028740\n",
      "/rid: 528 of: 1028740\n",
      "/rid: 529 of: 1028740\n",
      "/rid: 530 of: 1028740\n",
      "/rid: 531 of: 1028740\n",
      "/rid: 532 of: 1028740\n",
      "/rid: 533 of: 1028740\n",
      "/rid: 534 of: 1028740\n",
      "/rid: 535 of: 1028740\n",
      "/rid: 536 of: 1028740\n",
      "/rid: 537 of: 1028740\n",
      "/rid: 538 of: 1028740\n",
      "/rid: 539 of: 1028740\n",
      "/rid: 540 of: 1028740\n",
      "/rid: 541 of: 1028740\n",
      "/rid: 542 of: 1028740\n",
      "/rid: 543 of: 1028740\n",
      "/rid: 544 of: 1028740\n",
      "/rid: 545 of: 1028740\n",
      "/rid: 546 of: 1028740\n",
      "/rid: 547 of: 1028740\n",
      "/rid: 548 of: 1028740\n",
      "/rid: 549 of: 1028740\n",
      "/rid: 550 of: 1028740\n",
      "/rid: 551 of: 1028740\n",
      "/rid: 552 of: 1028740\n",
      "/rid: 553 of: 1028740\n",
      "/rid: 554 of: 1028740\n",
      "/rid: 555 of: 1028740\n",
      "/rid: 556 of: 1028740\n",
      "/rid: 557 of: 1028740\n",
      "/rid: 558 of: 1028740\n",
      "/rid: 559 of: 1028740\n",
      "/rid: 560 of: 1028740\n",
      "/rid: 561 of: 1028740\n",
      "/rid: 562 of: 1028740\n",
      "/rid: 563 of: 1028740\n",
      "/rid: 564 of: 1028740\n",
      "/rid: 565 of: 1028740\n",
      "/rid: 566 of: 1028740\n",
      "/rid: 567 of: 1028740\n",
      "/rid: 568 of: 1028740\n",
      "/rid: 569 of: 1028740\n",
      "/rid: 570 of: 1028740\n",
      "/rid: 571 of: 1028740\n",
      "/rid: 572 of: 1028740\n",
      "/rid: 573 of: 1028740\n",
      "/rid: 574 of: 1028740\n",
      "/rid: 575 of: 1028740\n",
      "/rid: 576 of: 1028740\n",
      "/rid: 577 of: 1028740\n",
      "/rid: 578 of: 1028740\n",
      "/rid: 579 of: 1028740\n",
      "/rid: 580 of: 1028740\n",
      "/rid: 581 of: 1028740\n",
      "/rid: 582 of: 1028740\n",
      "/rid: 583 of: 1028740\n",
      "/rid: 584 of: 1028740\n",
      "/rid: 585 of: 1028740\n",
      "/rid: 586 of: 1028740\n",
      "/rid: 587 of: 1028740\n",
      "/rid: 588 of: 1028740\n",
      "/rid: 589 of: 1028740\n",
      "/rid: 590 of: 1028740\n",
      "/rid: 591 of: 1028740\n",
      "/rid: 592 of: 1028740\n",
      "/rid: 593 of: 1028740\n",
      "/rid: 594 of: 1028740\n",
      "/rid: 595 of: 1028740\n",
      "/rid: 596 of: 1028740\n",
      "/rid: 597 of: 1028740\n",
      "/rid: 598 of: 1028740\n",
      "/rid: 599 of: 1028740\n",
      "/rid: 600 of: 1028740\n",
      "/rid: 601 of: 1028740\n",
      "/rid: 602 of: 1028740\n",
      "/rid: 603 of: 1028740\n",
      "/rid: 604 of: 1028740\n",
      "/rid: 605 of: 1028740\n",
      "/rid: 606 of: 1028740\n",
      "/rid: 607 of: 1028740\n",
      "/rid: 608 of: 1028740\n",
      "/rid: 609 of: 1028740\n",
      "/rid: 610 of: 1028740\n",
      "/rid: 611 of: 1028740\n",
      "/rid: 612 of: 1028740\n",
      "/rid: 613 of: 1028740\n",
      "/rid: 614 of: 1028740\n",
      "/rid: 615 of: 1028740\n",
      "/rid: 616 of: 1028740\n",
      "/rid: 617 of: 1028740\n",
      "/rid: 618 of: 1028740\n",
      "/rid: 619 of: 1028740\n",
      "/rid: 620 of: 1028740\n",
      "/rid: 621 of: 1028740\n",
      "/rid: 622 of: 1028740\n",
      "/rid: 623 of: 1028740\n",
      "/rid: 624 of: 1028740\n",
      "/rid: 625 of: 1028740\n",
      "/rid: 626 of: 1028740\n",
      "/rid: 627 of: 1028740\n",
      "/rid: 628 of: 1028740\n",
      "/rid: 629 of: 1028740\n",
      "/rid: 630 of: 1028740\n",
      "/rid: 631 of: 1028740\n",
      "/rid: 632 of: 1028740\n",
      "/rid: 633 of: 1028740\n",
      "/rid: 634 of: 1028740\n",
      "/rid: 635 of: 1028740\n",
      "/rid: 636 of: 1028740\n",
      "/rid: 637 of: 1028740\n",
      "/rid: 638 of: 1028740\n",
      "/rid: 639 of: 1028740\n",
      "/rid: 640 of: 1028740\n",
      "/rid: 641 of: 1028740\n",
      "/rid: 642 of: 1028740\n",
      "/rid: 643 of: 1028740\n",
      "/rid: 644 of: 1028740\n",
      "/rid: 645 of: 1028740\n",
      "/rid: 646 of: 1028740\n",
      "/rid: 647 of: 1028740\n",
      "/rid: 648 of: 1028740\n",
      "/rid: 649 of: 1028740\n",
      "/rid: 650 of: 1028740\n",
      "/rid: 651 of: 1028740\n",
      "/rid: 652 of: 1028740\n",
      "/rid: 653 of: 1028740\n",
      "/rid: 654 of: 1028740\n",
      "/rid: 655 of: 1028740\n",
      "/rid: 656 of: 1028740\n",
      "/rid: 657 of: 1028740\n",
      "/rid: 658 of: 1028740\n",
      "/rid: 659 of: 1028740\n",
      "/rid: 660 of: 1028740\n",
      "/rid: 661 of: 1028740\n",
      "/rid: 662 of: 1028740\n",
      "/rid: 663 of: 1028740\n",
      "/rid: 664 of: 1028740\n",
      "/rid: 665 of: 1028740\n",
      "/rid: 666 of: 1028740\n",
      "/rid: 667 of: 1028740\n",
      "/rid: 668 of: 1028740\n",
      "/rid: 669 of: 1028740\n",
      "/rid: 670 of: 1028740\n",
      "/rid: 671 of: 1028740\n",
      "/rid: 672 of: 1028740\n",
      "/rid: 673 of: 1028740\n",
      "/rid: 674 of: 1028740\n",
      "/rid: 675 of: 1028740\n",
      "/rid: 676 of: 1028740\n",
      "/rid: 677 of: 1028740\n",
      "/rid: 678 of: 1028740\n",
      "/rid: 679 of: 1028740\n",
      "/rid: 680 of: 1028740\n",
      "/rid: 681 of: 1028740\n",
      "/rid: 682 of: 1028740\n",
      "/rid: 683 of: 1028740\n",
      "/rid: 684 of: 1028740\n",
      "/rid: 685 of: 1028740\n",
      "/rid: 686 of: 1028740\n",
      "/rid: 687 of: 1028740\n",
      "/rid: 688 of: 1028740\n",
      "/rid: 689 of: 1028740\n",
      "/rid: 690 of: 1028740\n",
      "/rid: 691 of: 1028740\n",
      "/rid: 692 of: 1028740\n",
      "/rid: 693 of: 1028740\n",
      "/rid: 694 of: 1028740\n",
      "/rid: 695 of: 1028740\n",
      "/rid: 696 of: 1028740\n",
      "/rid: 697 of: 1028740\n",
      "/rid: 698 of: 1028740\n",
      "/rid: 699 of: 1028740\n",
      "/rid: 700 of: 1028740\n",
      "/rid: 701 of: 1028740\n",
      "/rid: 702 of: 1028740\n",
      "/rid: 703 of: 1028740\n",
      "/rid: 704 of: 1028740\n",
      "/rid: 705 of: 1028740\n",
      "/rid: 706 of: 1028740\n",
      "/rid: 707 of: 1028740\n",
      "/rid: 708 of: 1028740\n",
      "/rid: 709 of: 1028740\n",
      "/rid: 710 of: 1028740\n",
      "/rid: 711 of: 1028740\n",
      "/rid: 712 of: 1028740\n",
      "/rid: 713 of: 1028740\n",
      "/rid: 714 of: 1028740\n",
      "/rid: 715 of: 1028740\n",
      "/rid: 716 of: 1028740\n",
      "/rid: 717 of: 1028740\n",
      "/rid: 718 of: 1028740\n",
      "/rid: 719 of: 1028740\n",
      "/rid: 720 of: 1028740\n",
      "/rid: 721 of: 1028740\n",
      "/rid: 722 of: 1028740\n",
      "/rid: 723 of: 1028740\n",
      "/rid: 724 of: 1028740\n",
      "/rid: 725 of: 1028740\n",
      "/rid: 726 of: 1028740\n",
      "/rid: 727 of: 1028740\n",
      "/rid: 728 of: 1028740\n",
      "/rid: 729 of: 1028740\n",
      "/rid: 730 of: 1028740\n",
      "/rid: 731 of: 1028740\n",
      "/rid: 732 of: 1028740\n",
      "/rid: 733 of: 1028740\n",
      "/rid: 734 of: 1028740\n",
      "/rid: 735 of: 1028740\n",
      "/rid: 736 of: 1028740\n",
      "/rid: 737 of: 1028740\n",
      "/rid: 738 of: 1028740\n",
      "/rid: 739 of: 1028740\n",
      "/rid: 740 of: 1028740\n",
      "/rid: 741 of: 1028740\n",
      "/rid: 742 of: 1028740\n",
      "/rid: 743 of: 1028740\n",
      "/rid: 744 of: 1028740\n",
      "/rid: 745 of: 1028740\n",
      "/rid: 746 of: 1028740\n",
      "/rid: 747 of: 1028740\n",
      "/rid: 748 of: 1028740\n",
      "/rid: 749 of: 1028740\n",
      "/rid: 750 of: 1028740\n",
      "/rid: 751 of: 1028740\n",
      "/rid: 752 of: 1028740\n",
      "/rid: 753 of: 1028740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 754 of: 1028740\n",
      "/rid: 755 of: 1028740\n",
      "/rid: 756 of: 1028740\n",
      "/rid: 757 of: 1028740\n",
      "/rid: 758 of: 1028740\n",
      "/rid: 759 of: 1028740\n",
      "/rid: 760 of: 1028740\n",
      "/rid: 761 of: 1028740\n",
      "/rid: 762 of: 1028740\n",
      "/rid: 763 of: 1028740\n",
      "/rid: 764 of: 1028740\n",
      "/rid: 765 of: 1028740\n",
      "/rid: 766 of: 1028740\n",
      "/rid: 767 of: 1028740\n",
      "/rid: 768 of: 1028740\n",
      "/rid: 769 of: 1028740\n",
      "/rid: 770 of: 1028740\n",
      "/rid: 771 of: 1028740\n",
      "/rid: 772 of: 1028740\n",
      "/rid: 773 of: 1028740\n",
      "/rid: 774 of: 1028740\n",
      "/rid: 775 of: 1028740\n",
      "/rid: 776 of: 1028740\n",
      "/rid: 777 of: 1028740\n",
      "/rid: 778 of: 1028740\n",
      "/rid: 779 of: 1028740\n",
      "/rid: 780 of: 1028740\n",
      "/rid: 781 of: 1028740\n",
      "/rid: 782 of: 1028740\n",
      "/rid: 783 of: 1028740\n",
      "/rid: 784 of: 1028740\n",
      "/rid: 785 of: 1028740\n",
      "/rid: 786 of: 1028740\n",
      "/rid: 787 of: 1028740\n",
      "/rid: 788 of: 1028740\n",
      "/rid: 789 of: 1028740\n",
      "/rid: 790 of: 1028740\n",
      "/rid: 791 of: 1028740\n",
      "/rid: 792 of: 1028740\n",
      "/rid: 793 of: 1028740\n",
      "/rid: 794 of: 1028740\n",
      "/rid: 795 of: 1028740\n",
      "/rid: 796 of: 1028740\n",
      "/rid: 797 of: 1028740\n",
      "/rid: 798 of: 1028740\n",
      "/rid: 799 of: 1028740\n",
      "/rid: 800 of: 1028740\n",
      "/rid: 801 of: 1028740\n",
      "/rid: 802 of: 1028740\n",
      "/rid: 803 of: 1028740\n",
      "/rid: 804 of: 1028740\n",
      "/rid: 805 of: 1028740\n",
      "/rid: 806 of: 1028740\n",
      "/rid: 807 of: 1028740\n",
      "/rid: 808 of: 1028740\n",
      "/rid: 809 of: 1028740\n",
      "/rid: 810 of: 1028740\n",
      "/rid: 811 of: 1028740\n",
      "/rid: 812 of: 1028740\n",
      "/rid: 813 of: 1028740\n",
      "/rid: 814 of: 1028740\n",
      "/rid: 815 of: 1028740\n",
      "/rid: 816 of: 1028740\n",
      "/rid: 817 of: 1028740\n",
      "/rid: 818 of: 1028740\n",
      "/rid: 819 of: 1028740\n",
      "/rid: 820 of: 1028740\n",
      "/rid: 821 of: 1028740\n",
      "/rid: 822 of: 1028740\n",
      "/rid: 823 of: 1028740\n",
      "/rid: 824 of: 1028740\n",
      "/rid: 825 of: 1028740\n",
      "/rid: 826 of: 1028740\n",
      "/rid: 827 of: 1028740\n",
      "/rid: 828 of: 1028740\n",
      "/rid: 829 of: 1028740\n",
      "/rid: 830 of: 1028740\n",
      "/rid: 831 of: 1028740\n",
      "/rid: 832 of: 1028740\n",
      "/rid: 833 of: 1028740\n",
      "/rid: 834 of: 1028740\n",
      "/rid: 835 of: 1028740\n",
      "/rid: 836 of: 1028740\n",
      "/rid: 837 of: 1028740\n",
      "/rid: 838 of: 1028740\n",
      "/rid: 839 of: 1028740\n",
      "/rid: 840 of: 1028740\n",
      "/rid: 841 of: 1028740\n",
      "/rid: 842 of: 1028740\n",
      "/rid: 843 of: 1028740\n",
      "/rid: 844 of: 1028740\n",
      "/rid: 845 of: 1028740\n",
      "/rid: 846 of: 1028740\n",
      "/rid: 847 of: 1028740\n",
      "/rid: 848 of: 1028740\n",
      "/rid: 849 of: 1028740\n",
      "/rid: 850 of: 1028740\n",
      "/rid: 851 of: 1028740\n",
      "/rid: 852 of: 1028740\n",
      "/rid: 853 of: 1028740\n",
      "/rid: 854 of: 1028740\n",
      "/rid: 855 of: 1028740\n",
      "/rid: 856 of: 1028740\n",
      "/rid: 857 of: 1028740\n",
      "/rid: 858 of: 1028740\n",
      "/rid: 859 of: 1028740\n",
      "/rid: 860 of: 1028740\n",
      "/rid: 861 of: 1028740\n",
      "/rid: 862 of: 1028740\n",
      "/rid: 863 of: 1028740\n",
      "/rid: 864 of: 1028740\n",
      "/rid: 865 of: 1028740\n",
      "/rid: 866 of: 1028740\n",
      "/rid: 867 of: 1028740\n",
      "/rid: 868 of: 1028740\n",
      "/rid: 869 of: 1028740\n",
      "/rid: 870 of: 1028740\n",
      "/rid: 871 of: 1028740\n",
      "/rid: 872 of: 1028740\n",
      "/rid: 873 of: 1028740\n",
      "/rid: 874 of: 1028740\n",
      "/rid: 875 of: 1028740\n",
      "/rid: 876 of: 1028740\n",
      "/rid: 877 of: 1028740\n",
      "/rid: 878 of: 1028740\n",
      "/rid: 879 of: 1028740\n",
      "/rid: 880 of: 1028740\n",
      "/rid: 881 of: 1028740\n",
      "/rid: 882 of: 1028740\n",
      "/rid: 883 of: 1028740\n",
      "/rid: 884 of: 1028740\n",
      "/rid: 885 of: 1028740\n",
      "/rid: 886 of: 1028740\n",
      "/rid: 887 of: 1028740\n",
      "/rid: 888 of: 1028740\n",
      "/rid: 889 of: 1028740\n",
      "/rid: 890 of: 1028740\n",
      "/rid: 891 of: 1028740\n",
      "/rid: 892 of: 1028740\n",
      "/rid: 893 of: 1028740\n",
      "/rid: 894 of: 1028740\n",
      "/rid: 895 of: 1028740\n",
      "/rid: 896 of: 1028740\n",
      "/rid: 897 of: 1028740\n",
      "/rid: 898 of: 1028740\n",
      "/rid: 899 of: 1028740\n",
      "/rid: 900 of: 1028740\n",
      "/rid: 901 of: 1028740\n",
      "/rid: 902 of: 1028740\n",
      "/rid: 903 of: 1028740\n",
      "/rid: 904 of: 1028740\n",
      "/rid: 905 of: 1028740\n",
      "/rid: 906 of: 1028740\n",
      "/rid: 907 of: 1028740\n",
      "/rid: 908 of: 1028740\n",
      "/rid: 909 of: 1028740\n",
      "/rid: 910 of: 1028740\n",
      "/rid: 911 of: 1028740\n",
      "/rid: 912 of: 1028740\n",
      "/rid: 913 of: 1028740\n",
      "/rid: 914 of: 1028740\n",
      "/rid: 915 of: 1028740\n",
      "/rid: 916 of: 1028740\n",
      "/rid: 917 of: 1028740\n",
      "/rid: 918 of: 1028740\n",
      "/rid: 919 of: 1028740\n",
      "/rid: 920 of: 1028740\n",
      "/rid: 921 of: 1028740\n",
      "/rid: 922 of: 1028740\n",
      "/rid: 923 of: 1028740\n",
      "/rid: 924 of: 1028740\n",
      "/rid: 925 of: 1028740\n",
      "/rid: 926 of: 1028740\n",
      "/rid: 927 of: 1028740\n",
      "/rid: 928 of: 1028740\n",
      "/rid: 929 of: 1028740\n",
      "/rid: 930 of: 1028740\n",
      "/rid: 931 of: 1028740\n",
      "/rid: 932 of: 1028740\n",
      "/rid: 933 of: 1028740\n",
      "/rid: 934 of: 1028740\n",
      "/rid: 935 of: 1028740\n",
      "/rid: 936 of: 1028740\n",
      "/rid: 937 of: 1028740\n",
      "/rid: 938 of: 1028740\n",
      "/rid: 939 of: 1028740\n",
      "/rid: 940 of: 1028740\n",
      "/rid: 941 of: 1028740\n",
      "/rid: 942 of: 1028740\n",
      "/rid: 943 of: 1028740\n",
      "/rid: 944 of: 1028740\n",
      "/rid: 945 of: 1028740\n",
      "/rid: 946 of: 1028740\n",
      "/rid: 947 of: 1028740\n",
      "/rid: 948 of: 1028740\n",
      "/rid: 949 of: 1028740\n",
      "/rid: 950 of: 1028740\n",
      "/rid: 951 of: 1028740\n",
      "/rid: 952 of: 1028740\n",
      "/rid: 953 of: 1028740\n",
      "/rid: 954 of: 1028740\n",
      "/rid: 955 of: 1028740\n",
      "/rid: 956 of: 1028740\n",
      "/rid: 957 of: 1028740\n",
      "/rid: 958 of: 1028740\n",
      "/rid: 959 of: 1028740\n",
      "/rid: 960 of: 1028740\n",
      "/rid: 961 of: 1028740\n",
      "/rid: 962 of: 1028740\n",
      "/rid: 963 of: 1028740\n",
      "/rid: 964 of: 1028740\n",
      "/rid: 965 of: 1028740\n",
      "/rid: 966 of: 1028740\n",
      "/rid: 967 of: 1028740\n",
      "/rid: 968 of: 1028740\n",
      "/rid: 969 of: 1028740\n",
      "/rid: 970 of: 1028740\n",
      "/rid: 971 of: 1028740\n",
      "/rid: 972 of: 1028740\n",
      "/rid: 973 of: 1028740\n",
      "/rid: 974 of: 1028740\n",
      "/rid: 975 of: 1028740\n",
      "/rid: 976 of: 1028740\n",
      "/rid: 977 of: 1028740\n",
      "/rid: 978 of: 1028740\n",
      "/rid: 979 of: 1028740\n",
      "/rid: 980 of: 1028740\n",
      "/rid: 981 of: 1028740\n",
      "/rid: 982 of: 1028740\n",
      "/rid: 983 of: 1028740\n",
      "/rid: 984 of: 1028740\n",
      "/rid: 985 of: 1028740\n",
      "/rid: 986 of: 1028740\n",
      "/rid: 987 of: 1028740\n",
      "/rid: 988 of: 1028740\n",
      "/rid: 989 of: 1028740\n",
      "/rid: 990 of: 1028740\n",
      "/rid: 991 of: 1028740\n",
      "/rid: 992 of: 1028740\n",
      "/rid: 993 of: 1028740\n",
      "/rid: 994 of: 1028740\n",
      "/rid: 995 of: 1028740\n",
      "/rid: 996 of: 1028740\n",
      "/rid: 997 of: 1028740\n",
      "/rid: 998 of: 1028740\n",
      "/rid: 999 of: 1028740\n",
      "/rid: 1000 of: 1028740\n",
      "/rid: 1001 of: 1028740\n",
      "/rid: 1002 of: 1028740\n",
      "/rid: 1003 of: 1028740\n",
      "/rid: 1004 of: 1028740\n",
      "/rid: 1005 of: 1028740\n",
      "/rid: 1006 of: 1028740\n",
      "/rid: 1007 of: 1028740\n",
      "/rid: 1008 of: 1028740\n",
      "/rid: 1009 of: 1028740\n",
      "/rid: 1010 of: 1028740\n",
      "/rid: 1011 of: 1028740\n",
      "/rid: 1012 of: 1028740\n",
      "/rid: 1013 of: 1028740\n",
      "/rid: 1014 of: 1028740\n",
      "/rid: 1015 of: 1028740\n",
      "/rid: 1016 of: 1028740\n",
      "/rid: 1017 of: 1028740\n",
      "/rid: 1018 of: 1028740\n",
      "/rid: 1019 of: 1028740\n",
      "/rid: 1020 of: 1028740\n",
      "/rid: 1021 of: 1028740\n",
      "/rid: 1022 of: 1028740\n",
      "/rid: 1023 of: 1028740\n",
      "/rid: 1024 of: 1028740\n",
      "/rid: 1025 of: 1028740\n",
      "/rid: 1026 of: 1028740\n",
      "/rid: 1027 of: 1028740\n",
      "/rid: 1028 of: 1028740\n",
      "/rid: 1029 of: 1028740\n",
      "/rid: 1030 of: 1028740\n",
      "/rid: 1031 of: 1028740\n",
      "/rid: 1032 of: 1028740\n",
      "/rid: 1033 of: 1028740\n",
      "/rid: 1034 of: 1028740\n",
      "/rid: 1035 of: 1028740\n",
      "/rid: 1036 of: 1028740\n",
      "/rid: 1037 of: 1028740\n",
      "/rid: 1038 of: 1028740\n",
      "/rid: 1039 of: 1028740\n",
      "/rid: 1040 of: 1028740\n",
      "/rid: 1041 of: 1028740\n",
      "/rid: 1042 of: 1028740\n",
      "/rid: 1043 of: 1028740\n",
      "/rid: 1044 of: 1028740\n",
      "/rid: 1045 of: 1028740\n",
      "/rid: 1046 of: 1028740\n",
      "/rid: 1047 of: 1028740\n",
      "/rid: 1048 of: 1028740\n",
      "/rid: 1049 of: 1028740\n",
      "/rid: 1050 of: 1028740\n",
      "/rid: 1051 of: 1028740\n",
      "/rid: 1052 of: 1028740\n",
      "/rid: 1053 of: 1028740\n",
      "/rid: 1054 of: 1028740\n",
      "/rid: 1055 of: 1028740\n",
      "/rid: 1056 of: 1028740\n",
      "/rid: 1057 of: 1028740\n",
      "/rid: 1058 of: 1028740\n",
      "/rid: 1059 of: 1028740\n",
      "/rid: 1060 of: 1028740\n",
      "/rid: 1061 of: 1028740\n",
      "/rid: 1062 of: 1028740\n",
      "/rid: 1063 of: 1028740\n",
      "/rid: 1064 of: 1028740\n",
      "/rid: 1065 of: 1028740\n",
      "/rid: 1066 of: 1028740\n",
      "/rid: 1067 of: 1028740\n",
      "/rid: 1068 of: 1028740\n",
      "/rid: 1069 of: 1028740\n",
      "/rid: 1070 of: 1028740\n",
      "/rid: 1071 of: 1028740\n",
      "/rid: 1072 of: 1028740\n",
      "/rid: 1073 of: 1028740\n",
      "/rid: 1074 of: 1028740\n",
      "/rid: 1075 of: 1028740\n",
      "/rid: 1076 of: 1028740\n",
      "/rid: 1077 of: 1028740\n",
      "/rid: 1078 of: 1028740\n",
      "/rid: 1079 of: 1028740\n",
      "/rid: 1080 of: 1028740\n",
      "/rid: 1081 of: 1028740\n",
      "/rid: 1082 of: 1028740\n",
      "/rid: 1083 of: 1028740\n",
      "/rid: 1084 of: 1028740\n",
      "/rid: 1085 of: 1028740\n",
      "/rid: 1086 of: 1028740\n",
      "/rid: 1087 of: 1028740\n",
      "/rid: 1088 of: 1028740\n",
      "/rid: 1089 of: 1028740\n",
      "/rid: 1090 of: 1028740\n",
      "/rid: 1091 of: 1028740\n",
      "/rid: 1092 of: 1028740\n",
      "/rid: 1093 of: 1028740\n",
      "/rid: 1094 of: 1028740\n",
      "/rid: 1095 of: 1028740\n",
      "/rid: 1096 of: 1028740\n",
      "/rid: 1097 of: 1028740\n",
      "/rid: 1098 of: 1028740\n",
      "/rid: 1099 of: 1028740\n",
      "/rid: 1100 of: 1028740\n",
      "/rid: 1101 of: 1028740\n",
      "/rid: 1102 of: 1028740\n",
      "/rid: 1103 of: 1028740\n",
      "/rid: 1104 of: 1028740\n",
      "/rid: 1105 of: 1028740\n",
      "/rid: 1106 of: 1028740\n",
      "/rid: 1107 of: 1028740\n",
      "/rid: 1108 of: 1028740\n",
      "/rid: 1109 of: 1028740\n",
      "/rid: 1110 of: 1028740\n",
      "/rid: 1111 of: 1028740\n",
      "/rid: 1112 of: 1028740\n",
      "/rid: 1113 of: 1028740\n",
      "/rid: 1114 of: 1028740\n",
      "/rid: 1115 of: 1028740\n",
      "/rid: 1116 of: 1028740\n",
      "/rid: 1117 of: 1028740\n",
      "/rid: 1118 of: 1028740\n",
      "/rid: 1119 of: 1028740\n",
      "/rid: 1120 of: 1028740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 1121 of: 1028740\n",
      "/rid: 1122 of: 1028740\n",
      "/rid: 1123 of: 1028740\n",
      "/rid: 1124 of: 1028740\n",
      "/rid: 1125 of: 1028740\n",
      "/rid: 1126 of: 1028740\n",
      "/rid: 1127 of: 1028740\n",
      "/rid: 1128 of: 1028740\n",
      "/rid: 1129 of: 1028740\n",
      "/rid: 1130 of: 1028740\n",
      "/rid: 1131 of: 1028740\n",
      "/rid: 1132 of: 1028740\n",
      "/rid: 1133 of: 1028740\n",
      "/rid: 1134 of: 1028740\n",
      "/rid: 1135 of: 1028740\n",
      "/rid: 1136 of: 1028740\n",
      "/rid: 1137 of: 1028740\n",
      "/rid: 1138 of: 1028740\n",
      "/rid: 1139 of: 1028740\n",
      "/rid: 1140 of: 1028740\n",
      "/rid: 1141 of: 1028740\n",
      "/rid: 1142 of: 1028740\n",
      "/rid: 1143 of: 1028740\n",
      "/rid: 1144 of: 1028740\n",
      "/rid: 1145 of: 1028740\n",
      "/rid: 1146 of: 1028740\n",
      "/rid: 1147 of: 1028740\n",
      "/rid: 1148 of: 1028740\n",
      "/rid: 1149 of: 1028740\n",
      "/rid: 1150 of: 1028740\n",
      "/rid: 1151 of: 1028740\n",
      "/rid: 1152 of: 1028740\n",
      "/rid: 1153 of: 1028740\n",
      "/rid: 1154 of: 1028740\n",
      "/rid: 1155 of: 1028740\n",
      "/rid: 1156 of: 1028740\n",
      "/rid: 1157 of: 1028740\n",
      "/rid: 1158 of: 1028740\n",
      "/rid: 1159 of: 1028740\n",
      "/rid: 1160 of: 1028740\n",
      "/rid: 1161 of: 1028740\n",
      "/rid: 1162 of: 1028740\n",
      "/rid: 1163 of: 1028740\n",
      "/rid: 1164 of: 1028740\n",
      "/rid: 1165 of: 1028740\n",
      "/rid: 1166 of: 1028740\n",
      "/rid: 1167 of: 1028740\n",
      "/rid: 1168 of: 1028740\n",
      "/rid: 1169 of: 1028740\n",
      "/rid: 1170 of: 1028740\n",
      "/rid: 1171 of: 1028740\n",
      "/rid: 1172 of: 1028740\n",
      "/rid: 1173 of: 1028740\n",
      "/rid: 1174 of: 1028740\n",
      "/rid: 1175 of: 1028740\n",
      "/rid: 1176 of: 1028740\n",
      "/rid: 1177 of: 1028740\n",
      "/rid: 1178 of: 1028740\n",
      "/rid: 1179 of: 1028740\n",
      "/rid: 1180 of: 1028740\n",
      "/rid: 1181 of: 1028740\n",
      "/rid: 1182 of: 1028740\n",
      "/rid: 1183 of: 1028740\n",
      "/rid: 1184 of: 1028740\n",
      "/rid: 1185 of: 1028740\n",
      "/rid: 1186 of: 1028740\n",
      "/rid: 1187 of: 1028740\n",
      "/rid: 1188 of: 1028740\n",
      "/rid: 1189 of: 1028740\n",
      "/rid: 1190 of: 1028740\n",
      "/rid: 1191 of: 1028740\n",
      "/rid: 1192 of: 1028740\n",
      "/rid: 1193 of: 1028740\n",
      "/rid: 1194 of: 1028740\n",
      "/rid: 1195 of: 1028740\n",
      "/rid: 1196 of: 1028740\n",
      "/rid: 1197 of: 1028740\n",
      "/rid: 1198 of: 1028740\n",
      "/rid: 1199 of: 1028740\n",
      "/rid: 1200 of: 1028740\n",
      "/rid: 1201 of: 1028740\n",
      "/rid: 1202 of: 1028740\n",
      "/rid: 1203 of: 1028740\n",
      "/rid: 1204 of: 1028740\n",
      "/rid: 1205 of: 1028740\n",
      "/rid: 1206 of: 1028740\n",
      "/rid: 1207 of: 1028740\n",
      "/rid: 1208 of: 1028740\n",
      "/rid: 1209 of: 1028740\n",
      "/rid: 1210 of: 1028740\n",
      "/rid: 1211 of: 1028740\n",
      "/rid: 1212 of: 1028740\n",
      "/rid: 1213 of: 1028740\n",
      "/rid: 1214 of: 1028740\n",
      "/rid: 1215 of: 1028740\n",
      "/rid: 1216 of: 1028740\n",
      "/rid: 1217 of: 1028740\n",
      "/rid: 1218 of: 1028740\n",
      "/rid: 1219 of: 1028740\n",
      "/rid: 1220 of: 1028740\n",
      "/rid: 1221 of: 1028740\n",
      "/rid: 1222 of: 1028740\n",
      "/rid: 1223 of: 1028740\n",
      "/rid: 1224 of: 1028740\n",
      "/rid: 1225 of: 1028740\n",
      "/rid: 1226 of: 1028740\n",
      "/rid: 1227 of: 1028740\n",
      "/rid: 1228 of: 1028740\n",
      "/rid: 1229 of: 1028740\n",
      "/rid: 1230 of: 1028740\n",
      "/rid: 1231 of: 1028740\n",
      "/rid: 1232 of: 1028740\n",
      "/rid: 1233 of: 1028740\n",
      "/rid: 1234 of: 1028740\n",
      "/rid: 1235 of: 1028740\n",
      "/rid: 1236 of: 1028740\n",
      "/rid: 1237 of: 1028740\n",
      "/rid: 1238 of: 1028740\n",
      "/rid: 1239 of: 1028740\n",
      "/rid: 1240 of: 1028740\n",
      "/rid: 1241 of: 1028740\n",
      "/rid: 1242 of: 1028740\n",
      "/rid: 1243 of: 1028740\n",
      "/rid: 1244 of: 1028740\n",
      "/rid: 1245 of: 1028740\n",
      "/rid: 1246 of: 1028740\n",
      "/rid: 1247 of: 1028740\n",
      "/rid: 1248 of: 1028740\n",
      "/rid: 1249 of: 1028740\n",
      "/rid: 1250 of: 1028740\n",
      "/rid: 1251 of: 1028740\n",
      "/rid: 1252 of: 1028740\n",
      "/rid: 1253 of: 1028740\n",
      "/rid: 1254 of: 1028740\n",
      "/rid: 1255 of: 1028740\n",
      "/rid: 1256 of: 1028740\n",
      "/rid: 1257 of: 1028740\n",
      "/rid: 1258 of: 1028740\n",
      "/rid: 1259 of: 1028740\n",
      "/rid: 1260 of: 1028740\n",
      "/rid: 1261 of: 1028740\n",
      "/rid: 1262 of: 1028740\n",
      "/rid: 1263 of: 1028740\n",
      "/rid: 1264 of: 1028740\n",
      "/rid: 1265 of: 1028740\n",
      "/rid: 1266 of: 1028740\n",
      "/rid: 1267 of: 1028740\n",
      "/rid: 1268 of: 1028740\n",
      "/rid: 1269 of: 1028740\n",
      "/rid: 1270 of: 1028740\n",
      "/rid: 1271 of: 1028740\n",
      "/rid: 1272 of: 1028740\n",
      "/rid: 1273 of: 1028740\n",
      "/rid: 1274 of: 1028740\n",
      "/rid: 1275 of: 1028740\n",
      "/rid: 1276 of: 1028740\n",
      "/rid: 1277 of: 1028740\n",
      "/rid: 1278 of: 1028740\n",
      "/rid: 1279 of: 1028740\n",
      "/rid: 1280 of: 1028740\n",
      "/rid: 1281 of: 1028740\n",
      "/rid: 1282 of: 1028740\n",
      "/rid: 1283 of: 1028740\n",
      "/rid: 1284 of: 1028740\n",
      "/rid: 1285 of: 1028740\n",
      "/rid: 1286 of: 1028740\n",
      "/rid: 1287 of: 1028740\n",
      "/rid: 1288 of: 1028740\n",
      "/rid: 1289 of: 1028740\n",
      "/rid: 1290 of: 1028740\n",
      "/rid: 1291 of: 1028740\n",
      "/rid: 1292 of: 1028740\n",
      "/rid: 1293 of: 1028740\n",
      "/rid: 1294 of: 1028740\n",
      "/rid: 1295 of: 1028740\n",
      "/rid: 1296 of: 1028740\n",
      "/rid: 1297 of: 1028740\n",
      "/rid: 1298 of: 1028740\n",
      "/rid: 1299 of: 1028740\n",
      "/rid: 1300 of: 1028740\n",
      "/rid: 1301 of: 1028740\n",
      "/rid: 1302 of: 1028740\n",
      "/rid: 1303 of: 1028740\n",
      "/rid: 1304 of: 1028740\n",
      "/rid: 1305 of: 1028740\n",
      "/rid: 1306 of: 1028740\n",
      "/rid: 1307 of: 1028740\n",
      "/rid: 1308 of: 1028740\n",
      "/rid: 1309 of: 1028740\n",
      "/rid: 1310 of: 1028740\n",
      "/rid: 1311 of: 1028740\n",
      "/rid: 1312 of: 1028740\n",
      "/rid: 1313 of: 1028740\n",
      "/rid: 1314 of: 1028740\n",
      "/rid: 1315 of: 1028740\n",
      "/rid: 1316 of: 1028740\n",
      "/rid: 1317 of: 1028740\n",
      "/rid: 1318 of: 1028740\n",
      "/rid: 1319 of: 1028740\n",
      "/rid: 1320 of: 1028740\n",
      "/rid: 1321 of: 1028740\n",
      "/rid: 1322 of: 1028740\n",
      "/rid: 1323 of: 1028740\n",
      "/rid: 1324 of: 1028740\n",
      "/rid: 1325 of: 1028740\n",
      "/rid: 1326 of: 1028740\n",
      "/rid: 1327 of: 1028740\n",
      "/rid: 1328 of: 1028740\n",
      "/rid: 1329 of: 1028740\n",
      "/rid: 1330 of: 1028740\n",
      "/rid: 1331 of: 1028740\n",
      "/rid: 1332 of: 1028740\n",
      "/rid: 1333 of: 1028740\n",
      "/rid: 1334 of: 1028740\n",
      "/rid: 1335 of: 1028740\n",
      "/rid: 1336 of: 1028740\n",
      "/rid: 1337 of: 1028740\n",
      "/rid: 1338 of: 1028740\n",
      "/rid: 1339 of: 1028740\n",
      "/rid: 1340 of: 1028740\n",
      "/rid: 1341 of: 1028740\n",
      "/rid: 1342 of: 1028740\n",
      "/rid: 1343 of: 1028740\n",
      "/rid: 1344 of: 1028740\n",
      "/rid: 1345 of: 1028740\n",
      "/rid: 1346 of: 1028740\n",
      "/rid: 1347 of: 1028740\n",
      "/rid: 1348 of: 1028740\n",
      "/rid: 1349 of: 1028740\n",
      "/rid: 1350 of: 1028740\n",
      "/rid: 1351 of: 1028740\n",
      "/rid: 1352 of: 1028740\n",
      "/rid: 1353 of: 1028740\n",
      "/rid: 1354 of: 1028740\n",
      "/rid: 1355 of: 1028740\n",
      "/rid: 1356 of: 1028740\n",
      "/rid: 1357 of: 1028740\n",
      "/rid: 1358 of: 1028740\n",
      "/rid: 1359 of: 1028740\n",
      "/rid: 1360 of: 1028740\n",
      "/rid: 1361 of: 1028740\n",
      "/rid: 1362 of: 1028740\n",
      "/rid: 1363 of: 1028740\n",
      "/rid: 1364 of: 1028740\n",
      "/rid: 1365 of: 1028740\n",
      "/rid: 1366 of: 1028740\n",
      "/rid: 1367 of: 1028740\n",
      "/rid: 1368 of: 1028740\n",
      "/rid: 1369 of: 1028740\n",
      "/rid: 1370 of: 1028740\n",
      "/rid: 1371 of: 1028740\n",
      "/rid: 1372 of: 1028740\n",
      "/rid: 1373 of: 1028740\n",
      "/rid: 1374 of: 1028740\n",
      "/rid: 1375 of: 1028740\n",
      "/rid: 1376 of: 1028740\n",
      "/rid: 1377 of: 1028740\n",
      "/rid: 1378 of: 1028740\n",
      "/rid: 1379 of: 1028740\n",
      "/rid: 1380 of: 1028740\n",
      "/rid: 1381 of: 1028740\n",
      "/rid: 1382 of: 1028740\n",
      "/rid: 1383 of: 1028740\n",
      "/rid: 1384 of: 1028740\n",
      "/rid: 1385 of: 1028740\n",
      "/rid: 1386 of: 1028740\n",
      "/rid: 1387 of: 1028740\n",
      "/rid: 1388 of: 1028740\n",
      "/rid: 1389 of: 1028740\n",
      "/rid: 1390 of: 1028740\n",
      "/rid: 1391 of: 1028740\n",
      "/rid: 1392 of: 1028740\n",
      "/rid: 1393 of: 1028740\n",
      "/rid: 1394 of: 1028740\n",
      "/rid: 1395 of: 1028740\n",
      "/rid: 1396 of: 1028740\n",
      "/rid: 1397 of: 1028740\n",
      "/rid: 1398 of: 1028740\n",
      "/rid: 1399 of: 1028740\n",
      "/rid: 1400 of: 1028740\n",
      "/rid: 1401 of: 1028740\n",
      "/rid: 1402 of: 1028740\n",
      "/rid: 1403 of: 1028740\n",
      "/rid: 1404 of: 1028740\n",
      "/rid: 1405 of: 1028740\n",
      "/rid: 1406 of: 1028740\n",
      "/rid: 1407 of: 1028740\n",
      "/rid: 1408 of: 1028740\n",
      "/rid: 1409 of: 1028740\n",
      "/rid: 1410 of: 1028740\n",
      "/rid: 1411 of: 1028740\n",
      "/rid: 1412 of: 1028740\n",
      "/rid: 1413 of: 1028740\n",
      "/rid: 1414 of: 1028740\n",
      "/rid: 1415 of: 1028740\n",
      "/rid: 1416 of: 1028740\n",
      "/rid: 1417 of: 1028740\n",
      "/rid: 1418 of: 1028740\n",
      "/rid: 1419 of: 1028740\n",
      "/rid: 1420 of: 1028740\n",
      "/rid: 1421 of: 1028740\n",
      "/rid: 1422 of: 1028740\n",
      "/rid: 1423 of: 1028740\n",
      "/rid: 1424 of: 1028740\n",
      "/rid: 1425 of: 1028740\n",
      "/rid: 1426 of: 1028740\n",
      "/rid: 1427 of: 1028740\n",
      "/rid: 1428 of: 1028740\n",
      "/rid: 1429 of: 1028740\n",
      "/rid: 1430 of: 1028740\n",
      "/rid: 1431 of: 1028740\n",
      "/rid: 1432 of: 1028740\n",
      "/rid: 1433 of: 1028740\n",
      "/rid: 1434 of: 1028740\n",
      "/rid: 1435 of: 1028740\n",
      "/rid: 1436 of: 1028740\n",
      "/rid: 1437 of: 1028740\n",
      "/rid: 1438 of: 1028740\n",
      "/rid: 1439 of: 1028740\n",
      "/rid: 1440 of: 1028740\n",
      "/rid: 1441 of: 1028740\n",
      "/rid: 1442 of: 1028740\n",
      "/rid: 1443 of: 1028740\n",
      "/rid: 1444 of: 1028740\n",
      "/rid: 1445 of: 1028740\n",
      "/rid: 1446 of: 1028740\n",
      "/rid: 1447 of: 1028740\n",
      "/rid: 1448 of: 1028740\n",
      "/rid: 1449 of: 1028740\n",
      "/rid: 1450 of: 1028740\n",
      "/rid: 1451 of: 1028740\n",
      "/rid: 1452 of: 1028740\n",
      "/rid: 1453 of: 1028740\n",
      "/rid: 1454 of: 1028740\n",
      "/rid: 1455 of: 1028740\n",
      "/rid: 1456 of: 1028740\n",
      "/rid: 1457 of: 1028740\n",
      "/rid: 1458 of: 1028740\n",
      "/rid: 1459 of: 1028740\n",
      "/rid: 1460 of: 1028740\n",
      "/rid: 1461 of: 1028740\n",
      "/rid: 1462 of: 1028740\n",
      "/rid: 1463 of: 1028740\n",
      "/rid: 1464 of: 1028740\n",
      "/rid: 1465 of: 1028740\n",
      "/rid: 1466 of: 1028740\n",
      "/rid: 1467 of: 1028740\n",
      "/rid: 1468 of: 1028740\n",
      "/rid: 1469 of: 1028740\n",
      "/rid: 1470 of: 1028740\n",
      "/rid: 1471 of: 1028740\n",
      "/rid: 1472 of: 1028740\n",
      "/rid: 1473 of: 1028740\n",
      "/rid: 1474 of: 1028740\n",
      "/rid: 1475 of: 1028740\n",
      "/rid: 1476 of: 1028740\n",
      "/rid: 1477 of: 1028740\n",
      "/rid: 1478 of: 1028740\n",
      "/rid: 1479 of: 1028740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 1480 of: 1028740\n",
      "/rid: 1481 of: 1028740\n",
      "/rid: 1482 of: 1028740\n",
      "/rid: 1483 of: 1028740\n",
      "/rid: 1484 of: 1028740\n",
      "/rid: 1485 of: 1028740\n",
      "/rid: 1486 of: 1028740\n",
      "/rid: 1487 of: 1028740\n",
      "/rid: 1488 of: 1028740\n",
      "/rid: 1489 of: 1028740\n",
      "/rid: 1490 of: 1028740\n",
      "/rid: 1491 of: 1028740\n",
      "/rid: 1492 of: 1028740\n",
      "/rid: 1493 of: 1028740\n",
      "/rid: 1494 of: 1028740\n",
      "/rid: 1495 of: 1028740\n",
      "/rid: 1496 of: 1028740\n",
      "/rid: 1497 of: 1028740\n",
      "/rid: 1498 of: 1028740\n",
      "/rid: 1499 of: 1028740\n",
      "/rid: 1500 of: 1028740\n",
      "/rid: 1501 of: 1028740\n",
      "/rid: 1502 of: 1028740\n",
      "/rid: 1503 of: 1028740\n",
      "/rid: 1504 of: 1028740\n",
      "/rid: 1505 of: 1028740\n",
      "/rid: 1506 of: 1028740\n",
      "/rid: 1507 of: 1028740\n",
      "/rid: 1508 of: 1028740\n",
      "/rid: 1509 of: 1028740\n",
      "/rid: 1510 of: 1028740\n",
      "/rid: 1511 of: 1028740\n",
      "/rid: 1512 of: 1028740\n",
      "/rid: 1513 of: 1028740\n",
      "/rid: 1514 of: 1028740\n",
      "/rid: 1515 of: 1028740\n",
      "/rid: 1516 of: 1028740\n",
      "/rid: 1517 of: 1028740\n",
      "/rid: 1518 of: 1028740\n",
      "/rid: 1519 of: 1028740\n",
      "/rid: 1520 of: 1028740\n",
      "/rid: 1521 of: 1028740\n",
      "/rid: 1522 of: 1028740\n",
      "/rid: 1523 of: 1028740\n",
      "/rid: 1524 of: 1028740\n",
      "/rid: 1525 of: 1028740\n",
      "/rid: 1526 of: 1028740\n",
      "/rid: 1527 of: 1028740\n",
      "/rid: 1528 of: 1028740\n",
      "/rid: 1529 of: 1028740\n",
      "/rid: 1530 of: 1028740\n",
      "/rid: 1531 of: 1028740\n",
      "/rid: 1532 of: 1028740\n",
      "/rid: 1533 of: 1028740\n",
      "/rid: 1534 of: 1028740\n",
      "/rid: 1535 of: 1028740\n",
      "/rid: 1536 of: 1028740\n",
      "/rid: 1537 of: 1028740\n",
      "/rid: 1538 of: 1028740\n",
      "/rid: 1539 of: 1028740\n",
      "/rid: 1540 of: 1028740\n",
      "/rid: 1541 of: 1028740\n",
      "/rid: 1542 of: 1028740\n",
      "/rid: 1543 of: 1028740\n",
      "/rid: 1544 of: 1028740\n",
      "/rid: 1545 of: 1028740\n",
      "/rid: 1546 of: 1028740\n",
      "/rid: 1547 of: 1028740\n",
      "/rid: 1548 of: 1028740\n",
      "/rid: 1549 of: 1028740\n",
      "/rid: 1550 of: 1028740\n",
      "/rid: 1551 of: 1028740\n",
      "/rid: 1552 of: 1028740\n",
      "/rid: 1553 of: 1028740\n",
      "/rid: 1554 of: 1028740\n",
      "/rid: 1555 of: 1028740\n",
      "/rid: 1556 of: 1028740\n",
      "/rid: 1557 of: 1028740\n",
      "/rid: 1558 of: 1028740\n",
      "/rid: 1559 of: 1028740\n",
      "/rid: 1560 of: 1028740\n",
      "/rid: 1561 of: 1028740\n",
      "/rid: 1562 of: 1028740\n",
      "/rid: 1563 of: 1028740\n",
      "/rid: 1564 of: 1028740\n",
      "/rid: 1565 of: 1028740\n",
      "/rid: 1566 of: 1028740\n",
      "/rid: 1567 of: 1028740\n",
      "/rid: 1568 of: 1028740\n",
      "/rid: 1569 of: 1028740\n",
      "/rid: 1570 of: 1028740\n",
      "/rid: 1571 of: 1028740\n",
      "/rid: 1572 of: 1028740\n",
      "/rid: 1573 of: 1028740\n",
      "/rid: 1574 of: 1028740\n",
      "/rid: 1575 of: 1028740\n",
      "/rid: 1576 of: 1028740\n",
      "/rid: 1577 of: 1028740\n",
      "/rid: 1578 of: 1028740\n",
      "/rid: 1579 of: 1028740\n",
      "/rid: 1580 of: 1028740\n",
      "/rid: 1581 of: 1028740\n",
      "/rid: 1582 of: 1028740\n",
      "/rid: 1583 of: 1028740\n",
      "/rid: 1584 of: 1028740\n",
      "/rid: 1585 of: 1028740\n",
      "/rid: 1586 of: 1028740\n",
      "/rid: 1587 of: 1028740\n",
      "/rid: 1588 of: 1028740\n",
      "/rid: 1589 of: 1028740\n",
      "/rid: 1590 of: 1028740\n",
      "/rid: 1591 of: 1028740\n",
      "/rid: 1592 of: 1028740\n",
      "/rid: 1593 of: 1028740\n",
      "/rid: 1594 of: 1028740\n",
      "/rid: 1595 of: 1028740\n",
      "/rid: 1596 of: 1028740\n",
      "/rid: 1597 of: 1028740\n",
      "/rid: 1598 of: 1028740\n",
      "/rid: 1599 of: 1028740\n",
      "/rid: 1600 of: 1028740\n",
      "/rid: 1601 of: 1028740\n",
      "/rid: 1602 of: 1028740\n",
      "/rid: 1603 of: 1028740\n",
      "/rid: 1604 of: 1028740\n",
      "/rid: 1605 of: 1028740\n",
      "/rid: 1606 of: 1028740\n",
      "/rid: 1607 of: 1028740\n",
      "/rid: 1608 of: 1028740\n",
      "/rid: 1609 of: 1028740\n",
      "/rid: 1610 of: 1028740\n",
      "/rid: 1611 of: 1028740\n",
      "/rid: 1612 of: 1028740\n",
      "/rid: 1613 of: 1028740\n",
      "/rid: 1614 of: 1028740\n",
      "/rid: 1615 of: 1028740\n",
      "/rid: 1616 of: 1028740\n",
      "/rid: 1617 of: 1028740\n",
      "/rid: 1618 of: 1028740\n",
      "/rid: 1619 of: 1028740\n",
      "/rid: 1620 of: 1028740\n",
      "/rid: 1621 of: 1028740\n",
      "/rid: 1622 of: 1028740\n",
      "/rid: 1623 of: 1028740\n",
      "/rid: 1624 of: 1028740\n",
      "/rid: 1625 of: 1028740\n",
      "/rid: 1626 of: 1028740\n",
      "/rid: 1627 of: 1028740\n",
      "/rid: 1628 of: 1028740\n",
      "/rid: 1629 of: 1028740\n",
      "/rid: 1630 of: 1028740\n",
      "/rid: 1631 of: 1028740\n",
      "/rid: 1632 of: 1028740\n",
      "/rid: 1633 of: 1028740\n",
      "/rid: 1634 of: 1028740\n",
      "/rid: 1635 of: 1028740\n",
      "/rid: 1636 of: 1028740\n",
      "/rid: 1637 of: 1028740\n",
      "/rid: 1638 of: 1028740\n",
      "/rid: 1639 of: 1028740\n",
      "/rid: 1640 of: 1028740\n",
      "/rid: 1641 of: 1028740\n",
      "/rid: 1642 of: 1028740\n",
      "/rid: 1643 of: 1028740\n",
      "/rid: 1644 of: 1028740\n",
      "/rid: 1645 of: 1028740\n",
      "/rid: 1646 of: 1028740\n",
      "/rid: 1647 of: 1028740\n",
      "/rid: 1648 of: 1028740\n",
      "/rid: 1649 of: 1028740\n",
      "/rid: 1650 of: 1028740\n",
      "/rid: 1651 of: 1028740\n",
      "/rid: 1652 of: 1028740\n",
      "/rid: 1653 of: 1028740\n",
      "/rid: 1654 of: 1028740\n",
      "/rid: 1655 of: 1028740\n",
      "/rid: 1656 of: 1028740\n",
      "/rid: 1657 of: 1028740\n",
      "/rid: 1658 of: 1028740\n",
      "/rid: 1659 of: 1028740\n",
      "/rid: 1660 of: 1028740\n",
      "/rid: 1661 of: 1028740\n",
      "/rid: 1662 of: 1028740\n",
      "/rid: 1663 of: 1028740\n",
      "/rid: 1664 of: 1028740\n",
      "/rid: 1665 of: 1028740\n",
      "/rid: 1666 of: 1028740\n",
      "/rid: 1667 of: 1028740\n",
      "/rid: 1668 of: 1028740\n",
      "/rid: 1669 of: 1028740\n",
      "/rid: 1670 of: 1028740\n",
      "/rid: 1671 of: 1028740\n",
      "/rid: 1672 of: 1028740\n",
      "/rid: 1673 of: 1028740\n",
      "/rid: 1674 of: 1028740\n",
      "/rid: 1675 of: 1028740\n",
      "/rid: 1676 of: 1028740\n",
      "/rid: 1677 of: 1028740\n",
      "/rid: 1678 of: 1028740\n",
      "/rid: 1679 of: 1028740\n",
      "/rid: 1680 of: 1028740\n",
      "/rid: 1681 of: 1028740\n",
      "/rid: 1682 of: 1028740\n",
      "/rid: 1683 of: 1028740\n",
      "/rid: 1684 of: 1028740\n",
      "/rid: 1685 of: 1028740\n",
      "/rid: 1686 of: 1028740\n",
      "/rid: 1687 of: 1028740\n",
      "/rid: 1688 of: 1028740\n",
      "/rid: 1689 of: 1028740\n",
      "/rid: 1690 of: 1028740\n",
      "/rid: 1691 of: 1028740\n",
      "/rid: 1692 of: 1028740\n",
      "/rid: 1693 of: 1028740\n",
      "/rid: 1694 of: 1028740\n",
      "/rid: 1695 of: 1028740\n",
      "/rid: 1696 of: 1028740\n",
      "/rid: 1697 of: 1028740\n",
      "/rid: 1698 of: 1028740\n",
      "/rid: 1699 of: 1028740\n",
      "/rid: 1700 of: 1028740\n",
      "/rid: 1701 of: 1028740\n",
      "/rid: 1702 of: 1028740\n",
      "/rid: 1703 of: 1028740\n",
      "/rid: 1704 of: 1028740\n",
      "/rid: 1705 of: 1028740\n",
      "/rid: 1706 of: 1028740\n",
      "/rid: 1707 of: 1028740\n",
      "/rid: 1708 of: 1028740\n",
      "/rid: 1709 of: 1028740\n",
      "/rid: 1710 of: 1028740\n",
      "/rid: 1711 of: 1028740\n",
      "/rid: 1712 of: 1028740\n",
      "/rid: 1713 of: 1028740\n",
      "/rid: 1714 of: 1028740\n",
      "/rid: 1715 of: 1028740\n",
      "/rid: 1716 of: 1028740\n",
      "/rid: 1717 of: 1028740\n",
      "/rid: 1718 of: 1028740\n",
      "/rid: 1719 of: 1028740\n",
      "/rid: 1720 of: 1028740\n",
      "/rid: 1721 of: 1028740\n",
      "/rid: 1722 of: 1028740\n",
      "/rid: 1723 of: 1028740\n",
      "/rid: 1724 of: 1028740\n",
      "/rid: 1725 of: 1028740\n",
      "/rid: 1726 of: 1028740\n",
      "/rid: 1727 of: 1028740\n",
      "/rid: 1728 of: 1028740\n",
      "/rid: 1729 of: 1028740\n",
      "/rid: 1730 of: 1028740\n",
      "/rid: 1731 of: 1028740\n",
      "/rid: 1732 of: 1028740\n",
      "/rid: 1733 of: 1028740\n",
      "/rid: 1734 of: 1028740\n",
      "/rid: 1735 of: 1028740\n",
      "/rid: 1736 of: 1028740\n",
      "/rid: 1737 of: 1028740\n",
      "/rid: 1738 of: 1028740\n",
      "/rid: 1739 of: 1028740\n",
      "/rid: 1740 of: 1028740\n",
      "/rid: 1741 of: 1028740\n",
      "/rid: 1742 of: 1028740\n",
      "/rid: 1743 of: 1028740\n",
      "/rid: 1744 of: 1028740\n",
      "/rid: 1745 of: 1028740\n",
      "/rid: 1746 of: 1028740\n",
      "/rid: 1747 of: 1028740\n",
      "/rid: 1748 of: 1028740\n",
      "/rid: 1749 of: 1028740\n",
      "/rid: 1750 of: 1028740\n",
      "/rid: 1751 of: 1028740\n",
      "/rid: 1752 of: 1028740\n",
      "/rid: 1753 of: 1028740\n",
      "/rid: 1754 of: 1028740\n",
      "/rid: 1755 of: 1028740\n",
      "/rid: 1756 of: 1028740\n",
      "/rid: 1757 of: 1028740\n",
      "/rid: 1758 of: 1028740\n",
      "/rid: 1759 of: 1028740\n",
      "/rid: 1760 of: 1028740\n",
      "/rid: 1761 of: 1028740\n",
      "/rid: 1762 of: 1028740\n",
      "/rid: 1763 of: 1028740\n",
      "/rid: 1764 of: 1028740\n",
      "/rid: 1765 of: 1028740\n",
      "/rid: 1766 of: 1028740\n",
      "/rid: 1767 of: 1028740\n",
      "/rid: 1768 of: 1028740\n",
      "/rid: 1769 of: 1028740\n",
      "/rid: 1770 of: 1028740\n",
      "/rid: 1771 of: 1028740\n",
      "/rid: 1772 of: 1028740\n",
      "/rid: 1773 of: 1028740\n",
      "/rid: 1774 of: 1028740\n",
      "/rid: 1775 of: 1028740\n",
      "/rid: 1776 of: 1028740\n",
      "/rid: 1777 of: 1028740\n",
      "/rid: 1778 of: 1028740\n",
      "/rid: 1779 of: 1028740\n",
      "/rid: 1780 of: 1028740\n",
      "/rid: 1781 of: 1028740\n",
      "/rid: 1782 of: 1028740\n",
      "/rid: 1783 of: 1028740\n",
      "/rid: 1784 of: 1028740\n",
      "/rid: 1785 of: 1028740\n",
      "/rid: 1786 of: 1028740\n",
      "/rid: 1787 of: 1028740\n",
      "/rid: 1788 of: 1028740\n",
      "/rid: 1789 of: 1028740\n",
      "/rid: 1790 of: 1028740\n",
      "/rid: 1791 of: 1028740\n",
      "/rid: 1792 of: 1028740\n",
      "/rid: 1793 of: 1028740\n",
      "/rid: 1794 of: 1028740\n",
      "/rid: 1795 of: 1028740\n",
      "/rid: 1796 of: 1028740\n",
      "/rid: 1797 of: 1028740\n",
      "/rid: 1798 of: 1028740\n",
      "/rid: 1799 of: 1028740\n",
      "/rid: 1800 of: 1028740\n",
      "/rid: 1801 of: 1028740\n",
      "/rid: 1802 of: 1028740\n",
      "/rid: 1803 of: 1028740\n",
      "/rid: 1804 of: 1028740\n",
      "/rid: 1805 of: 1028740\n",
      "/rid: 1806 of: 1028740\n",
      "/rid: 1807 of: 1028740\n",
      "/rid: 1808 of: 1028740\n",
      "/rid: 1809 of: 1028740\n",
      "/rid: 1810 of: 1028740\n",
      "/rid: 1811 of: 1028740\n",
      "/rid: 1812 of: 1028740\n",
      "/rid: 1813 of: 1028740\n",
      "/rid: 1814 of: 1028740\n",
      "/rid: 1815 of: 1028740\n",
      "/rid: 1816 of: 1028740\n",
      "/rid: 1817 of: 1028740\n",
      "/rid: 1818 of: 1028740\n",
      "/rid: 1819 of: 1028740\n",
      "/rid: 1820 of: 1028740\n",
      "/rid: 1821 of: 1028740\n",
      "/rid: 1822 of: 1028740\n",
      "/rid: 1823 of: 1028740\n",
      "/rid: 1824 of: 1028740\n",
      "/rid: 1825 of: 1028740\n",
      "/rid: 1826 of: 1028740\n",
      "/rid: 1827 of: 1028740\n",
      "/rid: 1828 of: 1028740\n",
      "/rid: 1829 of: 1028740\n",
      "/rid: 1830 of: 1028740\n",
      "/rid: 1831 of: 1028740\n",
      "/rid: 1832 of: 1028740\n",
      "/rid: 1833 of: 1028740\n",
      "/rid: 1834 of: 1028740\n",
      "/rid: 1835 of: 1028740\n",
      "/rid: 1836 of: 1028740\n",
      "/rid: 1837 of: 1028740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 1838 of: 1028740\n",
      "/rid: 1839 of: 1028740\n",
      "/rid: 1840 of: 1028740\n",
      "/rid: 1841 of: 1028740\n",
      "/rid: 1842 of: 1028740\n",
      "/rid: 1843 of: 1028740\n",
      "/rid: 1844 of: 1028740\n",
      "/rid: 1845 of: 1028740\n",
      "/rid: 1846 of: 1028740\n",
      "/rid: 1847 of: 1028740\n",
      "/rid: 1848 of: 1028740\n",
      "/rid: 1849 of: 1028740\n",
      "/rid: 1850 of: 1028740\n",
      "/rid: 1851 of: 1028740\n",
      "/rid: 1852 of: 1028740\n",
      "/rid: 1853 of: 1028740\n",
      "/rid: 1854 of: 1028740\n",
      "/rid: 1855 of: 1028740\n",
      "/rid: 1856 of: 1028740\n",
      "/rid: 1857 of: 1028740\n",
      "/rid: 1858 of: 1028740\n",
      "/rid: 1859 of: 1028740\n",
      "/rid: 1860 of: 1028740\n",
      "/rid: 1861 of: 1028740\n",
      "/rid: 1862 of: 1028740\n",
      "/rid: 1863 of: 1028740\n",
      "/rid: 1864 of: 1028740\n",
      "/rid: 1865 of: 1028740\n",
      "/rid: 1866 of: 1028740\n",
      "/rid: 1867 of: 1028740\n",
      "/rid: 1868 of: 1028740\n",
      "/rid: 1869 of: 1028740\n",
      "/rid: 1870 of: 1028740\n",
      "/rid: 1871 of: 1028740\n",
      "/rid: 1872 of: 1028740\n",
      "/rid: 1873 of: 1028740\n",
      "/rid: 1874 of: 1028740\n",
      "/rid: 1875 of: 1028740\n",
      "/rid: 1876 of: 1028740\n",
      "/rid: 1877 of: 1028740\n",
      "/rid: 1878 of: 1028740\n",
      "/rid: 1879 of: 1028740\n",
      "/rid: 1880 of: 1028740\n",
      "/rid: 1881 of: 1028740\n",
      "/rid: 1882 of: 1028740\n",
      "/rid: 1883 of: 1028740\n",
      "/rid: 1884 of: 1028740\n",
      "/rid: 1885 of: 1028740\n",
      "/rid: 1886 of: 1028740\n",
      "/rid: 1887 of: 1028740\n",
      "/rid: 1888 of: 1028740\n",
      "/rid: 1889 of: 1028740\n",
      "/rid: 1890 of: 1028740\n",
      "/rid: 1891 of: 1028740\n",
      "/rid: 1892 of: 1028740\n",
      "/rid: 1893 of: 1028740\n",
      "/rid: 1894 of: 1028740\n",
      "/rid: 1895 of: 1028740\n",
      "/rid: 1896 of: 1028740\n",
      "/rid: 1897 of: 1028740\n",
      "/rid: 1898 of: 1028740\n",
      "/rid: 1899 of: 1028740\n",
      "/rid: 1900 of: 1028740\n",
      "/rid: 1901 of: 1028740\n",
      "/rid: 1902 of: 1028740\n",
      "/rid: 1903 of: 1028740\n",
      "/rid: 1904 of: 1028740\n",
      "/rid: 1905 of: 1028740\n",
      "/rid: 1906 of: 1028740\n",
      "/rid: 1907 of: 1028740\n",
      "/rid: 1908 of: 1028740\n",
      "/rid: 1909 of: 1028740\n",
      "/rid: 1910 of: 1028740\n",
      "/rid: 1911 of: 1028740\n",
      "/rid: 1912 of: 1028740\n",
      "/rid: 1913 of: 1028740\n",
      "/rid: 1914 of: 1028740\n",
      "/rid: 1915 of: 1028740\n",
      "/rid: 1916 of: 1028740\n",
      "/rid: 1917 of: 1028740\n",
      "/rid: 1918 of: 1028740\n",
      "/rid: 1919 of: 1028740\n",
      "/rid: 1920 of: 1028740\n",
      "/rid: 1921 of: 1028740\n",
      "/rid: 1922 of: 1028740\n",
      "/rid: 1923 of: 1028740\n",
      "/rid: 1924 of: 1028740\n",
      "/rid: 1925 of: 1028740\n",
      "/rid: 1926 of: 1028740\n",
      "/rid: 1927 of: 1028740\n",
      "/rid: 1928 of: 1028740\n",
      "/rid: 1929 of: 1028740\n",
      "/rid: 1930 of: 1028740\n",
      "/rid: 1931 of: 1028740\n",
      "/rid: 1932 of: 1028740\n",
      "/rid: 1933 of: 1028740\n",
      "/rid: 1934 of: 1028740\n",
      "/rid: 1935 of: 1028740\n",
      "/rid: 1936 of: 1028740\n",
      "/rid: 1937 of: 1028740\n",
      "/rid: 1938 of: 1028740\n",
      "/rid: 1939 of: 1028740\n",
      "/rid: 1940 of: 1028740\n",
      "/rid: 1941 of: 1028740\n",
      "/rid: 1942 of: 1028740\n",
      "/rid: 1943 of: 1028740\n",
      "/rid: 1944 of: 1028740\n",
      "/rid: 1945 of: 1028740\n",
      "/rid: 1946 of: 1028740\n",
      "/rid: 1947 of: 1028740\n",
      "/rid: 1948 of: 1028740\n",
      "/rid: 1949 of: 1028740\n",
      "/rid: 1950 of: 1028740\n",
      "/rid: 1951 of: 1028740\n",
      "/rid: 1952 of: 1028740\n",
      "/rid: 1953 of: 1028740\n",
      "/rid: 1954 of: 1028740\n",
      "/rid: 1955 of: 1028740\n",
      "/rid: 1956 of: 1028740\n",
      "/rid: 1957 of: 1028740\n",
      "/rid: 1958 of: 1028740\n",
      "/rid: 1959 of: 1028740\n",
      "/rid: 1960 of: 1028740\n",
      "/rid: 1961 of: 1028740\n",
      "/rid: 1962 of: 1028740\n",
      "/rid: 1963 of: 1028740\n",
      "/rid: 1964 of: 1028740\n",
      "/rid: 1965 of: 1028740\n",
      "/rid: 1966 of: 1028740\n",
      "/rid: 1967 of: 1028740\n",
      "/rid: 1968 of: 1028740\n",
      "/rid: 1969 of: 1028740\n",
      "/rid: 1970 of: 1028740\n",
      "/rid: 1971 of: 1028740\n",
      "/rid: 1972 of: 1028740\n",
      "/rid: 1973 of: 1028740\n",
      "/rid: 1974 of: 1028740\n",
      "/rid: 1975 of: 1028740\n",
      "/rid: 1976 of: 1028740\n",
      "/rid: 1977 of: 1028740\n",
      "/rid: 1978 of: 1028740\n",
      "/rid: 1979 of: 1028740\n",
      "/rid: 1980 of: 1028740\n",
      "/rid: 1981 of: 1028740\n",
      "/rid: 1982 of: 1028740\n",
      "/rid: 1983 of: 1028740\n",
      "/rid: 1984 of: 1028740\n",
      "/rid: 1985 of: 1028740\n",
      "/rid: 1986 of: 1028740\n",
      "/rid: 1987 of: 1028740\n",
      "/rid: 1988 of: 1028740\n",
      "/rid: 1989 of: 1028740\n",
      "/rid: 1990 of: 1028740\n",
      "/rid: 1991 of: 1028740\n",
      "/rid: 1992 of: 1028740\n",
      "/rid: 1993 of: 1028740\n",
      "/rid: 1994 of: 1028740\n",
      "/rid: 1995 of: 1028740\n",
      "/rid: 1996 of: 1028740\n",
      "/rid: 1997 of: 1028740\n",
      "/rid: 1998 of: 1028740\n",
      "/rid: 1999 of: 1028740\n",
      "/rid: 2000 of: 1028740\n",
      "/rid: 2001 of: 1028740\n",
      "/rid: 2002 of: 1028740\n",
      "/rid: 2003 of: 1028740\n",
      "/rid: 2004 of: 1028740\n",
      "/rid: 2005 of: 1028740\n",
      "/rid: 2006 of: 1028740\n",
      "/rid: 2007 of: 1028740\n",
      "/rid: 2008 of: 1028740\n",
      "/rid: 2009 of: 1028740\n",
      "/rid: 2010 of: 1028740\n",
      "/rid: 2011 of: 1028740\n",
      "/rid: 2012 of: 1028740\n",
      "/rid: 2013 of: 1028740\n",
      "/rid: 2014 of: 1028740\n",
      "/rid: 2015 of: 1028740\n",
      "/rid: 2016 of: 1028740\n",
      "/rid: 2017 of: 1028740\n",
      "/rid: 2018 of: 1028740\n",
      "/rid: 2019 of: 1028740\n",
      "/rid: 2020 of: 1028740\n",
      "/rid: 2021 of: 1028740\n",
      "/rid: 2022 of: 1028740\n",
      "/rid: 2023 of: 1028740\n",
      "/rid: 2024 of: 1028740\n",
      "/rid: 2025 of: 1028740\n",
      "/rid: 2026 of: 1028740\n",
      "/rid: 2027 of: 1028740\n",
      "/rid: 2028 of: 1028740\n",
      "/rid: 2029 of: 1028740\n",
      "/rid: 2030 of: 1028740\n",
      "/rid: 2031 of: 1028740\n",
      "/rid: 2032 of: 1028740\n",
      "/rid: 2033 of: 1028740\n",
      "/rid: 2034 of: 1028740\n",
      "/rid: 2035 of: 1028740\n",
      "/rid: 2036 of: 1028740\n",
      "/rid: 2037 of: 1028740\n",
      "/rid: 2038 of: 1028740\n",
      "/rid: 2039 of: 1028740\n",
      "/rid: 2040 of: 1028740\n",
      "/rid: 2041 of: 1028740\n",
      "/rid: 2042 of: 1028740\n",
      "/rid: 2043 of: 1028740\n",
      "/rid: 2044 of: 1028740\n",
      "/rid: 2045 of: 1028740\n",
      "/rid: 2046 of: 1028740\n",
      "/rid: 2047 of: 1028740\n",
      "/rid: 2048 of: 1028740\n",
      "/rid: 2049 of: 1028740\n",
      "/rid: 2050 of: 1028740\n",
      "/rid: 2051 of: 1028740\n",
      "/rid: 2052 of: 1028740\n",
      "/rid: 2053 of: 1028740\n",
      "/rid: 2054 of: 1028740\n",
      "/rid: 2055 of: 1028740\n",
      "/rid: 2056 of: 1028740\n",
      "/rid: 2057 of: 1028740\n",
      "/rid: 2058 of: 1028740\n",
      "/rid: 2059 of: 1028740\n",
      "/rid: 2060 of: 1028740\n",
      "/rid: 2061 of: 1028740\n",
      "/rid: 2062 of: 1028740\n",
      "/rid: 2063 of: 1028740\n",
      "/rid: 2064 of: 1028740\n",
      "/rid: 2065 of: 1028740\n",
      "/rid: 2066 of: 1028740\n",
      "/rid: 2067 of: 1028740\n",
      "/rid: 2068 of: 1028740\n",
      "/rid: 2069 of: 1028740\n",
      "/rid: 2070 of: 1028740\n",
      "/rid: 2071 of: 1028740\n",
      "/rid: 2072 of: 1028740\n",
      "/rid: 2073 of: 1028740\n",
      "/rid: 2074 of: 1028740\n",
      "/rid: 2075 of: 1028740\n",
      "/rid: 2076 of: 1028740\n",
      "/rid: 2077 of: 1028740\n",
      "/rid: 2078 of: 1028740\n",
      "/rid: 2079 of: 1028740\n",
      "/rid: 2080 of: 1028740\n",
      "/rid: 2081 of: 1028740\n",
      "/rid: 2082 of: 1028740\n",
      "/rid: 2083 of: 1028740\n",
      "/rid: 2084 of: 1028740\n",
      "/rid: 2085 of: 1028740\n",
      "/rid: 2086 of: 1028740\n",
      "/rid: 2087 of: 1028740\n",
      "/rid: 2088 of: 1028740\n",
      "/rid: 2089 of: 1028740\n",
      "/rid: 2090 of: 1028740\n",
      "/rid: 2091 of: 1028740\n",
      "/rid: 2092 of: 1028740\n",
      "/rid: 2093 of: 1028740\n",
      "/rid: 2094 of: 1028740\n",
      "/rid: 2095 of: 1028740\n",
      "/rid: 2096 of: 1028740\n",
      "/rid: 2097 of: 1028740\n",
      "/rid: 2098 of: 1028740\n",
      "/rid: 2099 of: 1028740\n",
      "/rid: 2100 of: 1028740\n",
      "/rid: 2101 of: 1028740\n",
      "/rid: 2102 of: 1028740\n",
      "/rid: 2103 of: 1028740\n",
      "/rid: 2104 of: 1028740\n",
      "/rid: 2105 of: 1028740\n",
      "/rid: 2106 of: 1028740\n",
      "/rid: 2107 of: 1028740\n",
      "/rid: 2108 of: 1028740\n",
      "/rid: 2109 of: 1028740\n",
      "/rid: 2110 of: 1028740\n",
      "/rid: 2111 of: 1028740\n",
      "/rid: 2112 of: 1028740\n",
      "/rid: 2113 of: 1028740\n",
      "/rid: 2114 of: 1028740\n",
      "/rid: 2115 of: 1028740\n",
      "/rid: 2116 of: 1028740\n",
      "/rid: 2117 of: 1028740\n",
      "/rid: 2118 of: 1028740\n",
      "/rid: 2119 of: 1028740\n",
      "/rid: 2120 of: 1028740\n",
      "/rid: 2121 of: 1028740\n",
      "/rid: 2122 of: 1028740\n",
      "/rid: 2123 of: 1028740\n",
      "/rid: 2124 of: 1028740\n",
      "/rid: 2125 of: 1028740\n",
      "/rid: 2126 of: 1028740\n",
      "/rid: 2127 of: 1028740\n",
      "/rid: 2128 of: 1028740\n",
      "/rid: 2129 of: 1028740\n",
      "/rid: 2130 of: 1028740\n",
      "/rid: 2131 of: 1028740\n",
      "/rid: 2132 of: 1028740\n",
      "/rid: 2133 of: 1028740\n",
      "/rid: 2134 of: 1028740\n",
      "/rid: 2135 of: 1028740\n",
      "/rid: 2136 of: 1028740\n",
      "/rid: 2137 of: 1028740\n",
      "/rid: 2138 of: 1028740\n",
      "/rid: 2139 of: 1028740\n",
      "/rid: 2140 of: 1028740\n",
      "/rid: 2141 of: 1028740\n",
      "/rid: 2142 of: 1028740\n",
      "/rid: 2143 of: 1028740\n",
      "/rid: 2144 of: 1028740\n",
      "/rid: 2145 of: 1028740\n",
      "/rid: 2146 of: 1028740\n",
      "/rid: 2147 of: 1028740\n",
      "/rid: 2148 of: 1028740\n",
      "/rid: 2149 of: 1028740\n",
      "/rid: 2150 of: 1028740\n",
      "/rid: 2151 of: 1028740\n",
      "/rid: 2152 of: 1028740\n",
      "/rid: 2153 of: 1028740\n",
      "/rid: 2154 of: 1028740\n",
      "/rid: 2155 of: 1028740\n",
      "/rid: 2156 of: 1028740\n",
      "/rid: 2157 of: 1028740\n",
      "/rid: 2158 of: 1028740\n",
      "/rid: 2159 of: 1028740\n",
      "/rid: 2160 of: 1028740\n",
      "/rid: 2161 of: 1028740\n",
      "/rid: 2162 of: 1028740\n",
      "/rid: 2163 of: 1028740\n",
      "/rid: 2164 of: 1028740\n",
      "/rid: 2165 of: 1028740\n",
      "/rid: 2166 of: 1028740\n",
      "/rid: 2167 of: 1028740\n",
      "/rid: 2168 of: 1028740\n",
      "/rid: 2169 of: 1028740\n",
      "/rid: 2170 of: 1028740\n",
      "/rid: 2171 of: 1028740\n",
      "/rid: 2172 of: 1028740\n",
      "/rid: 2173 of: 1028740\n",
      "/rid: 2174 of: 1028740\n",
      "/rid: 2175 of: 1028740\n",
      "/rid: 2176 of: 1028740\n",
      "/rid: 2177 of: 1028740\n",
      "/rid: 2178 of: 1028740\n",
      "/rid: 2179 of: 1028740\n",
      "/rid: 2180 of: 1028740\n",
      "/rid: 2181 of: 1028740\n",
      "/rid: 2182 of: 1028740\n",
      "/rid: 2183 of: 1028740\n",
      "/rid: 2184 of: 1028740\n",
      "/rid: 2185 of: 1028740\n",
      "/rid: 2186 of: 1028740\n",
      "/rid: 2187 of: 1028740\n",
      "/rid: 2188 of: 1028740\n",
      "/rid: 2189 of: 1028740\n",
      "/rid: 2190 of: 1028740\n",
      "/rid: 2191 of: 1028740\n",
      "/rid: 2192 of: 1028740\n",
      "/rid: 2193 of: 1028740\n",
      "/rid: 2194 of: 1028740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 2195 of: 1028740\n",
      "/rid: 2196 of: 1028740\n",
      "/rid: 2197 of: 1028740\n",
      "/rid: 2198 of: 1028740\n",
      "/rid: 2199 of: 1028740\n",
      "/rid: 2200 of: 1028740\n",
      "/rid: 2201 of: 1028740\n",
      "/rid: 2202 of: 1028740\n",
      "/rid: 2203 of: 1028740\n",
      "/rid: 2204 of: 1028740\n",
      "/rid: 2205 of: 1028740\n",
      "/rid: 2206 of: 1028740\n",
      "/rid: 2207 of: 1028740\n",
      "/rid: 2208 of: 1028740\n",
      "/rid: 2209 of: 1028740\n",
      "/rid: 2210 of: 1028740\n",
      "/rid: 2211 of: 1028740\n",
      "/rid: 2212 of: 1028740\n",
      "/rid: 2213 of: 1028740\n",
      "/rid: 2214 of: 1028740\n",
      "/rid: 2215 of: 1028740\n",
      "/rid: 2216 of: 1028740\n",
      "/rid: 2217 of: 1028740\n",
      "/rid: 2218 of: 1028740\n",
      "/rid: 2219 of: 1028740\n",
      "/rid: 2220 of: 1028740\n",
      "/rid: 2221 of: 1028740\n",
      "/rid: 2222 of: 1028740\n",
      "/rid: 2223 of: 1028740\n",
      "/rid: 2224 of: 1028740\n",
      "/rid: 2225 of: 1028740\n",
      "/rid: 2226 of: 1028740\n",
      "/rid: 2227 of: 1028740\n",
      "/rid: 2228 of: 1028740\n",
      "/rid: 2229 of: 1028740\n",
      "/rid: 2230 of: 1028740\n",
      "/rid: 2231 of: 1028740\n",
      "/rid: 2232 of: 1028740\n",
      "/rid: 2233 of: 1028740\n",
      "/rid: 2234 of: 1028740\n",
      "/rid: 2235 of: 1028740\n",
      "/rid: 2236 of: 1028740\n",
      "/rid: 2237 of: 1028740\n",
      "/rid: 2238 of: 1028740\n",
      "/rid: 2239 of: 1028740\n",
      "/rid: 2240 of: 1028740\n",
      "/rid: 2241 of: 1028740\n",
      "/rid: 2242 of: 1028740\n",
      "/rid: 2243 of: 1028740\n",
      "/rid: 2244 of: 1028740\n",
      "/rid: 2245 of: 1028740\n",
      "/rid: 2246 of: 1028740\n",
      "/rid: 2247 of: 1028740\n",
      "/rid: 2248 of: 1028740\n",
      "/rid: 2249 of: 1028740\n",
      "/rid: 2250 of: 1028740\n",
      "/rid: 2251 of: 1028740\n",
      "/rid: 2252 of: 1028740\n",
      "/rid: 2253 of: 1028740\n",
      "/rid: 2254 of: 1028740\n",
      "/rid: 2255 of: 1028740\n",
      "/rid: 2256 of: 1028740\n",
      "/rid: 2257 of: 1028740\n",
      "/rid: 2258 of: 1028740\n",
      "/rid: 2259 of: 1028740\n",
      "/rid: 2260 of: 1028740\n",
      "/rid: 2261 of: 1028740\n",
      "/rid: 2262 of: 1028740\n",
      "/rid: 2263 of: 1028740\n",
      "/rid: 2264 of: 1028740\n",
      "/rid: 2265 of: 1028740\n",
      "/rid: 2266 of: 1028740\n",
      "/rid: 2267 of: 1028740\n",
      "/rid: 2268 of: 1028740\n",
      "/rid: 2269 of: 1028740\n",
      "/rid: 2270 of: 1028740\n",
      "/rid: 2271 of: 1028740\n",
      "/rid: 2272 of: 1028740\n",
      "/rid: 2273 of: 1028740\n",
      "/rid: 2274 of: 1028740\n",
      "/rid: 2275 of: 1028740\n",
      "/rid: 2276 of: 1028740\n",
      "/rid: 2277 of: 1028740\n",
      "/rid: 2278 of: 1028740\n",
      "/rid: 2279 of: 1028740\n",
      "/rid: 2280 of: 1028740\n",
      "/rid: 2281 of: 1028740\n",
      "/rid: 2282 of: 1028740\n",
      "/rid: 2283 of: 1028740\n",
      "/rid: 2284 of: 1028740\n",
      "/rid: 2285 of: 1028740\n",
      "/rid: 2286 of: 1028740\n",
      "/rid: 2287 of: 1028740\n",
      "/rid: 2288 of: 1028740\n",
      "/rid: 2289 of: 1028740\n",
      "/rid: 2290 of: 1028740\n",
      "/rid: 2291 of: 1028740\n",
      "/rid: 2292 of: 1028740\n",
      "/rid: 2293 of: 1028740\n",
      "/rid: 2294 of: 1028740\n",
      "/rid: 2295 of: 1028740\n",
      "/rid: 2296 of: 1028740\n",
      "/rid: 2297 of: 1028740\n",
      "/rid: 2298 of: 1028740\n",
      "/rid: 2299 of: 1028740\n",
      "/rid: 2300 of: 1028740\n",
      "/rid: 2301 of: 1028740\n",
      "/rid: 2302 of: 1028740\n",
      "/rid: 2303 of: 1028740\n",
      "/rid: 2304 of: 1028740\n",
      "/rid: 2305 of: 1028740\n",
      "/rid: 2306 of: 1028740\n",
      "/rid: 2307 of: 1028740\n",
      "/rid: 2308 of: 1028740\n",
      "/rid: 2309 of: 1028740\n",
      "/rid: 2310 of: 1028740\n",
      "/rid: 2311 of: 1028740\n",
      "/rid: 2312 of: 1028740\n",
      "/rid: 2313 of: 1028740\n",
      "/rid: 2314 of: 1028740\n",
      "/rid: 2315 of: 1028740\n",
      "/rid: 2316 of: 1028740\n",
      "/rid: 2317 of: 1028740\n",
      "/rid: 2318 of: 1028740\n",
      "/rid: 2319 of: 1028740\n",
      "/rid: 2320 of: 1028740\n",
      "/rid: 2321 of: 1028740\n",
      "/rid: 2322 of: 1028740\n",
      "/rid: 2323 of: 1028740\n",
      "/rid: 2324 of: 1028740\n",
      "/rid: 2325 of: 1028740\n",
      "/rid: 2326 of: 1028740\n",
      "/rid: 2327 of: 1028740\n",
      "/rid: 2328 of: 1028740\n",
      "/rid: 2329 of: 1028740\n",
      "/rid: 2330 of: 1028740\n",
      "/rid: 2331 of: 1028740\n",
      "/rid: 2332 of: 1028740\n",
      "/rid: 2333 of: 1028740\n",
      "/rid: 2334 of: 1028740\n",
      "/rid: 2335 of: 1028740\n",
      "/rid: 2336 of: 1028740\n",
      "/rid: 2337 of: 1028740\n",
      "/rid: 2338 of: 1028740\n",
      "/rid: 2339 of: 1028740\n",
      "/rid: 2340 of: 1028740\n",
      "/rid: 2341 of: 1028740\n",
      "/rid: 2342 of: 1028740\n",
      "/rid: 2343 of: 1028740\n",
      "/rid: 2344 of: 1028740\n",
      "/rid: 2345 of: 1028740\n",
      "/rid: 2346 of: 1028740\n",
      "/rid: 2347 of: 1028740\n",
      "/rid: 2348 of: 1028740\n",
      "/rid: 2349 of: 1028740\n",
      "/rid: 2350 of: 1028740\n",
      "/rid: 2351 of: 1028740\n",
      "/rid: 2352 of: 1028740\n",
      "/rid: 2353 of: 1028740\n",
      "/rid: 2354 of: 1028740\n",
      "/rid: 2355 of: 1028740\n",
      "/rid: 2356 of: 1028740\n",
      "/rid: 2357 of: 1028740\n",
      "/rid: 2358 of: 1028740\n",
      "/rid: 2359 of: 1028740\n",
      "/rid: 2360 of: 1028740\n",
      "/rid: 2361 of: 1028740\n",
      "/rid: 2362 of: 1028740\n",
      "/rid: 2363 of: 1028740\n",
      "/rid: 2364 of: 1028740\n",
      "/rid: 2365 of: 1028740\n",
      "/rid: 2366 of: 1028740\n",
      "/rid: 2367 of: 1028740\n",
      "/rid: 2368 of: 1028740\n",
      "/rid: 2369 of: 1028740\n",
      "/rid: 2370 of: 1028740\n",
      "/rid: 2371 of: 1028740\n",
      "/rid: 2372 of: 1028740\n",
      "/rid: 2373 of: 1028740\n",
      "/rid: 2374 of: 1028740\n",
      "/rid: 2375 of: 1028740\n",
      "/rid: 2376 of: 1028740\n",
      "/rid: 2377 of: 1028740\n",
      "/rid: 2378 of: 1028740\n",
      "/rid: 2379 of: 1028740\n",
      "/rid: 2380 of: 1028740\n",
      "/rid: 2381 of: 1028740\n",
      "/rid: 2382 of: 1028740\n",
      "/rid: 2383 of: 1028740\n",
      "/rid: 2384 of: 1028740\n",
      "/rid: 2385 of: 1028740\n",
      "/rid: 2386 of: 1028740\n",
      "/rid: 2387 of: 1028740\n",
      "/rid: 2388 of: 1028740\n",
      "/rid: 2389 of: 1028740\n",
      "/rid: 2390 of: 1028740\n",
      "/rid: 2391 of: 1028740\n",
      "/rid: 2392 of: 1028740\n",
      "/rid: 2393 of: 1028740\n",
      "/rid: 2394 of: 1028740\n",
      "/rid: 2395 of: 1028740\n",
      "/rid: 2396 of: 1028740\n",
      "/rid: 2397 of: 1028740\n",
      "/rid: 2398 of: 1028740\n",
      "/rid: 2399 of: 1028740\n",
      "/rid: 2400 of: 1028740\n",
      "/rid: 2401 of: 1028740\n",
      "/rid: 2402 of: 1028740\n",
      "/rid: 2403 of: 1028740\n",
      "/rid: 2404 of: 1028740\n",
      "/rid: 2405 of: 1028740\n",
      "/rid: 2406 of: 1028740\n",
      "/rid: 2407 of: 1028740\n",
      "/rid: 2408 of: 1028740\n",
      "/rid: 2409 of: 1028740\n",
      "/rid: 2410 of: 1028740\n",
      "/rid: 2411 of: 1028740\n",
      "/rid: 2412 of: 1028740\n",
      "/rid: 2413 of: 1028740\n",
      "/rid: 2414 of: 1028740\n",
      "/rid: 2415 of: 1028740\n",
      "/rid: 2416 of: 1028740\n",
      "/rid: 2417 of: 1028740\n",
      "/rid: 2418 of: 1028740\n",
      "/rid: 2419 of: 1028740\n",
      "/rid: 2420 of: 1028740\n",
      "/rid: 2421 of: 1028740\n",
      "/rid: 2422 of: 1028740\n",
      "/rid: 2423 of: 1028740\n",
      "/rid: 2424 of: 1028740\n",
      "/rid: 2425 of: 1028740\n",
      "/rid: 2426 of: 1028740\n",
      "/rid: 2427 of: 1028740\n",
      "/rid: 2428 of: 1028740\n",
      "/rid: 2429 of: 1028740\n",
      "/rid: 2430 of: 1028740\n",
      "/rid: 2431 of: 1028740\n",
      "/rid: 2432 of: 1028740\n",
      "/rid: 2433 of: 1028740\n",
      "/rid: 2434 of: 1028740\n",
      "/rid: 2435 of: 1028740\n",
      "/rid: 2436 of: 1028740\n",
      "/rid: 2437 of: 1028740\n",
      "/rid: 2438 of: 1028740\n",
      "/rid: 2439 of: 1028740\n",
      "/rid: 2440 of: 1028740\n",
      "/rid: 2441 of: 1028740\n",
      "/rid: 2442 of: 1028740\n",
      "/rid: 2443 of: 1028740\n",
      "/rid: 2444 of: 1028740\n",
      "/rid: 2445 of: 1028740\n",
      "/rid: 2446 of: 1028740\n",
      "/rid: 2447 of: 1028740\n",
      "/rid: 2448 of: 1028740\n",
      "/rid: 2449 of: 1028740\n",
      "/rid: 2450 of: 1028740\n",
      "/rid: 2451 of: 1028740\n",
      "/rid: 2452 of: 1028740\n",
      "/rid: 2453 of: 1028740\n",
      "/rid: 2454 of: 1028740\n",
      "/rid: 2455 of: 1028740\n",
      "/rid: 2456 of: 1028740\n",
      "/rid: 2457 of: 1028740\n",
      "/rid: 2458 of: 1028740\n",
      "/rid: 2459 of: 1028740\n",
      "/rid: 2460 of: 1028740\n",
      "/rid: 2461 of: 1028740\n",
      "/rid: 2462 of: 1028740\n",
      "/rid: 2463 of: 1028740\n",
      "/rid: 2464 of: 1028740\n",
      "/rid: 2465 of: 1028740\n",
      "/rid: 2466 of: 1028740\n",
      "/rid: 2467 of: 1028740\n",
      "/rid: 2468 of: 1028740\n",
      "/rid: 2469 of: 1028740\n",
      "/rid: 2470 of: 1028740\n",
      "/rid: 2471 of: 1028740\n",
      "/rid: 2472 of: 1028740\n",
      "/rid: 2473 of: 1028740\n",
      "/rid: 2474 of: 1028740\n",
      "/rid: 2475 of: 1028740\n",
      "/rid: 2476 of: 1028740\n",
      "/rid: 2477 of: 1028740\n",
      "/rid: 2478 of: 1028740\n",
      "/rid: 2479 of: 1028740\n",
      "/rid: 2480 of: 1028740\n",
      "/rid: 2481 of: 1028740\n",
      "/rid: 2482 of: 1028740\n",
      "/rid: 2483 of: 1028740\n",
      "/rid: 2484 of: 1028740\n",
      "/rid: 2485 of: 1028740\n",
      "/rid: 2486 of: 1028740\n",
      "/rid: 2487 of: 1028740\n",
      "/rid: 2488 of: 1028740\n",
      "/rid: 2489 of: 1028740\n",
      "/rid: 2490 of: 1028740\n",
      "/rid: 2491 of: 1028740\n",
      "/rid: 2492 of: 1028740\n",
      "/rid: 2493 of: 1028740\n",
      "/rid: 2494 of: 1028740\n",
      "/rid: 2495 of: 1028740\n",
      "/rid: 2496 of: 1028740\n",
      "/rid: 2497 of: 1028740\n",
      "/rid: 2498 of: 1028740\n",
      "/rid: 2499 of: 1028740\n",
      "/rid: 2500 of: 1028740\n",
      "/rid: 2501 of: 1028740\n",
      "/rid: 2502 of: 1028740\n",
      "/rid: 2503 of: 1028740\n",
      "/rid: 2504 of: 1028740\n",
      "/rid: 2505 of: 1028740\n",
      "/rid: 2506 of: 1028740\n",
      "/rid: 2507 of: 1028740\n",
      "/rid: 2508 of: 1028740\n",
      "/rid: 2509 of: 1028740\n",
      "/rid: 2510 of: 1028740\n",
      "/rid: 2511 of: 1028740\n",
      "/rid: 2512 of: 1028740\n",
      "/rid: 2513 of: 1028740\n",
      "/rid: 2514 of: 1028740\n",
      "/rid: 2515 of: 1028740\n",
      "/rid: 2516 of: 1028740\n",
      "/rid: 2517 of: 1028740\n",
      "/rid: 2518 of: 1028740\n",
      "/rid: 2519 of: 1028740\n",
      "/rid: 2520 of: 1028740\n",
      "/rid: 2521 of: 1028740\n",
      "/rid: 2522 of: 1028740\n",
      "/rid: 2523 of: 1028740\n",
      "/rid: 2524 of: 1028740\n",
      "/rid: 2525 of: 1028740\n",
      "/rid: 2526 of: 1028740\n",
      "/rid: 2527 of: 1028740\n",
      "/rid: 2528 of: 1028740\n",
      "/rid: 2529 of: 1028740\n",
      "/rid: 2530 of: 1028740\n",
      "/rid: 2531 of: 1028740\n",
      "/rid: 2532 of: 1028740\n",
      "/rid: 2533 of: 1028740\n",
      "/rid: 2534 of: 1028740\n",
      "/rid: 2535 of: 1028740\n",
      "/rid: 2536 of: 1028740\n",
      "/rid: 2537 of: 1028740\n",
      "/rid: 2538 of: 1028740\n",
      "/rid: 2539 of: 1028740\n",
      "/rid: 2540 of: 1028740\n",
      "/rid: 2541 of: 1028740\n",
      "/rid: 2542 of: 1028740\n",
      "/rid: 2543 of: 1028740\n",
      "/rid: 2544 of: 1028740\n",
      "/rid: 2545 of: 1028740\n",
      "/rid: 2546 of: 1028740\n",
      "/rid: 2547 of: 1028740\n",
      "/rid: 2548 of: 1028740\n",
      "/rid: 2549 of: 1028740\n",
      "/rid: 2550 of: 1028740\n",
      "/rid: 2551 of: 1028740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 2552 of: 1028740\n",
      "/rid: 2553 of: 1028740\n",
      "/rid: 2554 of: 1028740\n",
      "/rid: 2555 of: 1028740\n",
      "/rid: 2556 of: 1028740\n",
      "/rid: 2557 of: 1028740\n",
      "/rid: 2558 of: 1028740\n",
      "/rid: 2559 of: 1028740\n",
      "/rid: 2560 of: 1028740\n",
      "/rid: 2561 of: 1028740\n",
      "/rid: 2562 of: 1028740\n",
      "/rid: 2563 of: 1028740\n",
      "/rid: 2564 of: 1028740\n",
      "/rid: 2565 of: 1028740\n",
      "/rid: 2566 of: 1028740\n",
      "/rid: 2567 of: 1028740\n",
      "/rid: 2568 of: 1028740\n",
      "/rid: 2569 of: 1028740\n",
      "/rid: 2570 of: 1028740\n",
      "/rid: 2571 of: 1028740\n",
      "/rid: 2572 of: 1028740\n",
      "/rid: 2573 of: 1028740\n",
      "/rid: 2574 of: 1028740\n",
      "/rid: 2575 of: 1028740\n",
      "/rid: 2576 of: 1028740\n",
      "/rid: 2577 of: 1028740\n",
      "/rid: 2578 of: 1028740\n",
      "/rid: 2579 of: 1028740\n",
      "/rid: 2580 of: 1028740\n",
      "/rid: 2581 of: 1028740\n",
      "/rid: 2582 of: 1028740\n",
      "/rid: 2583 of: 1028740\n",
      "/rid: 2584 of: 1028740\n",
      "/rid: 2585 of: 1028740\n",
      "/rid: 2586 of: 1028740\n",
      "/rid: 2587 of: 1028740\n",
      "/rid: 2588 of: 1028740\n",
      "/rid: 2589 of: 1028740\n",
      "/rid: 2590 of: 1028740\n",
      "/rid: 2591 of: 1028740\n",
      "/rid: 2592 of: 1028740\n",
      "/rid: 2593 of: 1028740\n",
      "/rid: 2594 of: 1028740\n",
      "/rid: 2595 of: 1028740\n",
      "/rid: 2596 of: 1028740\n",
      "/rid: 2597 of: 1028740\n",
      "/rid: 2598 of: 1028740\n",
      "/rid: 2599 of: 1028740\n",
      "/rid: 2600 of: 1028740\n",
      "/rid: 2601 of: 1028740\n",
      "/rid: 2602 of: 1028740\n",
      "/rid: 2603 of: 1028740\n",
      "/rid: 2604 of: 1028740\n",
      "/rid: 2605 of: 1028740\n",
      "/rid: 2606 of: 1028740\n",
      "/rid: 2607 of: 1028740\n",
      "/rid: 2608 of: 1028740\n",
      "/rid: 2609 of: 1028740\n",
      "/rid: 2610 of: 1028740\n",
      "/rid: 2611 of: 1028740\n",
      "/rid: 2612 of: 1028740\n",
      "/rid: 2613 of: 1028740\n",
      "/rid: 2614 of: 1028740\n",
      "/rid: 2615 of: 1028740\n",
      "/rid: 2616 of: 1028740\n",
      "/rid: 2617 of: 1028740\n",
      "/rid: 2618 of: 1028740\n",
      "/rid: 2619 of: 1028740\n",
      "/rid: 2620 of: 1028740\n",
      "/rid: 2621 of: 1028740\n",
      "/rid: 2622 of: 1028740\n",
      "/rid: 2623 of: 1028740\n",
      "/rid: 2624 of: 1028740\n",
      "/rid: 2625 of: 1028740\n",
      "/rid: 2626 of: 1028740\n",
      "/rid: 2627 of: 1028740\n",
      "/rid: 2628 of: 1028740\n",
      "/rid: 2629 of: 1028740\n",
      "/rid: 2630 of: 1028740\n",
      "/rid: 2631 of: 1028740\n",
      "/rid: 2632 of: 1028740\n",
      "/rid: 2633 of: 1028740\n",
      "/rid: 2634 of: 1028740\n",
      "/rid: 2635 of: 1028740\n",
      "/rid: 2636 of: 1028740\n",
      "/rid: 2637 of: 1028740\n",
      "/rid: 2638 of: 1028740\n",
      "/rid: 2639 of: 1028740\n",
      "/rid: 2640 of: 1028740\n",
      "/rid: 2641 of: 1028740\n",
      "/rid: 2642 of: 1028740\n",
      "/rid: 2643 of: 1028740\n",
      "/rid: 2644 of: 1028740\n",
      "/rid: 2645 of: 1028740\n",
      "/rid: 2646 of: 1028740\n",
      "/rid: 2647 of: 1028740\n",
      "/rid: 2648 of: 1028740\n",
      "/rid: 2649 of: 1028740\n",
      "/rid: 2650 of: 1028740\n",
      "/rid: 2651 of: 1028740\n",
      "/rid: 2652 of: 1028740\n",
      "/rid: 2653 of: 1028740\n",
      "/rid: 2654 of: 1028740\n",
      "/rid: 2655 of: 1028740\n",
      "/rid: 2656 of: 1028740\n",
      "/rid: 2657 of: 1028740\n",
      "/rid: 2658 of: 1028740\n",
      "/rid: 2659 of: 1028740\n",
      "/rid: 2660 of: 1028740\n",
      "/rid: 2661 of: 1028740\n",
      "/rid: 2662 of: 1028740\n",
      "/rid: 2663 of: 1028740\n",
      "/rid: 2664 of: 1028740\n",
      "/rid: 2665 of: 1028740\n",
      "/rid: 2666 of: 1028740\n",
      "/rid: 2667 of: 1028740\n",
      "/rid: 2668 of: 1028740\n",
      "/rid: 2669 of: 1028740\n",
      "/rid: 2670 of: 1028740\n",
      "/rid: 2671 of: 1028740\n",
      "/rid: 2672 of: 1028740\n",
      "/rid: 2673 of: 1028740\n",
      "/rid: 2674 of: 1028740\n",
      "/rid: 2675 of: 1028740\n",
      "/rid: 2676 of: 1028740\n",
      "/rid: 2677 of: 1028740\n",
      "/rid: 2678 of: 1028740\n",
      "/rid: 2679 of: 1028740\n",
      "/rid: 2680 of: 1028740\n",
      "/rid: 2681 of: 1028740\n",
      "/rid: 2682 of: 1028740\n",
      "/rid: 2683 of: 1028740\n",
      "/rid: 2684 of: 1028740\n",
      "/rid: 2685 of: 1028740\n",
      "/rid: 2686 of: 1028740\n",
      "/rid: 2687 of: 1028740\n",
      "/rid: 2688 of: 1028740\n",
      "/rid: 2689 of: 1028740\n",
      "/rid: 2690 of: 1028740\n",
      "/rid: 2691 of: 1028740\n",
      "/rid: 2692 of: 1028740\n",
      "/rid: 2693 of: 1028740\n",
      "/rid: 2694 of: 1028740\n",
      "/rid: 2695 of: 1028740\n",
      "/rid: 2696 of: 1028740\n",
      "/rid: 2697 of: 1028740\n",
      "/rid: 2698 of: 1028740\n",
      "/rid: 2699 of: 1028740\n",
      "/rid: 2700 of: 1028740\n",
      "/rid: 2701 of: 1028740\n",
      "/rid: 2702 of: 1028740\n",
      "/rid: 2703 of: 1028740\n",
      "/rid: 2704 of: 1028740\n",
      "/rid: 2705 of: 1028740\n",
      "/rid: 2706 of: 1028740\n",
      "/rid: 2707 of: 1028740\n",
      "/rid: 2708 of: 1028740\n",
      "/rid: 2709 of: 1028740\n",
      "/rid: 2710 of: 1028740\n",
      "/rid: 2711 of: 1028740\n",
      "/rid: 2712 of: 1028740\n",
      "/rid: 2713 of: 1028740\n",
      "/rid: 2714 of: 1028740\n",
      "/rid: 2715 of: 1028740\n",
      "/rid: 2716 of: 1028740\n",
      "/rid: 2717 of: 1028740\n",
      "/rid: 2718 of: 1028740\n",
      "/rid: 2719 of: 1028740\n",
      "/rid: 2720 of: 1028740\n",
      "/rid: 2721 of: 1028740\n",
      "/rid: 2722 of: 1028740\n",
      "/rid: 2723 of: 1028740\n",
      "/rid: 2724 of: 1028740\n",
      "/rid: 2725 of: 1028740\n",
      "/rid: 2726 of: 1028740\n",
      "/rid: 2727 of: 1028740\n",
      "/rid: 2728 of: 1028740\n",
      "/rid: 2729 of: 1028740\n",
      "/rid: 2730 of: 1028740\n",
      "/rid: 2731 of: 1028740\n",
      "/rid: 2732 of: 1028740\n",
      "/rid: 2733 of: 1028740\n",
      "/rid: 2734 of: 1028740\n",
      "/rid: 2735 of: 1028740\n",
      "/rid: 2736 of: 1028740\n",
      "/rid: 2737 of: 1028740\n",
      "/rid: 2738 of: 1028740\n",
      "/rid: 2739 of: 1028740\n",
      "/rid: 2740 of: 1028740\n",
      "/rid: 2741 of: 1028740\n",
      "/rid: 2742 of: 1028740\n",
      "/rid: 2743 of: 1028740\n",
      "/rid: 2744 of: 1028740\n",
      "/rid: 2745 of: 1028740\n",
      "/rid: 2746 of: 1028740\n",
      "/rid: 2747 of: 1028740\n",
      "/rid: 2748 of: 1028740\n",
      "/rid: 2749 of: 1028740\n",
      "/rid: 2750 of: 1028740\n",
      "/rid: 2751 of: 1028740\n",
      "/rid: 2752 of: 1028740\n",
      "/rid: 2753 of: 1028740\n",
      "/rid: 2754 of: 1028740\n",
      "/rid: 2755 of: 1028740\n",
      "/rid: 2756 of: 1028740\n",
      "/rid: 2757 of: 1028740\n",
      "/rid: 2758 of: 1028740\n",
      "/rid: 2759 of: 1028740\n",
      "/rid: 2760 of: 1028740\n",
      "/rid: 2761 of: 1028740\n",
      "/rid: 2762 of: 1028740\n",
      "/rid: 2763 of: 1028740\n",
      "/rid: 2764 of: 1028740\n",
      "/rid: 2765 of: 1028740\n",
      "/rid: 2766 of: 1028740\n",
      "/rid: 2767 of: 1028740\n",
      "/rid: 2768 of: 1028740\n",
      "/rid: 2769 of: 1028740\n",
      "/rid: 2770 of: 1028740\n",
      "/rid: 2771 of: 1028740\n",
      "/rid: 2772 of: 1028740\n",
      "/rid: 2773 of: 1028740\n",
      "/rid: 2774 of: 1028740\n",
      "/rid: 2775 of: 1028740\n",
      "/rid: 2776 of: 1028740\n",
      "/rid: 2777 of: 1028740\n",
      "/rid: 2778 of: 1028740\n",
      "/rid: 2779 of: 1028740\n",
      "/rid: 2780 of: 1028740\n",
      "/rid: 2781 of: 1028740\n",
      "/rid: 2782 of: 1028740\n",
      "/rid: 2783 of: 1028740\n",
      "/rid: 2784 of: 1028740\n",
      "/rid: 2785 of: 1028740\n",
      "/rid: 2786 of: 1028740\n",
      "/rid: 2787 of: 1028740\n",
      "/rid: 2788 of: 1028740\n",
      "/rid: 2789 of: 1028740\n",
      "/rid: 2790 of: 1028740\n",
      "/rid: 2791 of: 1028740\n",
      "/rid: 2792 of: 1028740\n",
      "/rid: 2793 of: 1028740\n",
      "/rid: 2794 of: 1028740\n",
      "/rid: 2795 of: 1028740\n",
      "/rid: 2796 of: 1028740\n",
      "/rid: 2797 of: 1028740\n",
      "/rid: 2798 of: 1028740\n",
      "/rid: 2799 of: 1028740\n",
      "/rid: 2800 of: 1028740\n",
      "/rid: 2801 of: 1028740\n",
      "/rid: 2802 of: 1028740\n",
      "/rid: 2803 of: 1028740\n",
      "/rid: 2804 of: 1028740\n",
      "/rid: 2805 of: 1028740\n",
      "/rid: 2806 of: 1028740\n",
      "/rid: 2807 of: 1028740\n",
      "/rid: 2808 of: 1028740\n",
      "/rid: 2809 of: 1028740\n",
      "/rid: 2810 of: 1028740\n",
      "/rid: 2811 of: 1028740\n",
      "/rid: 2812 of: 1028740\n",
      "/rid: 2813 of: 1028740\n",
      "/rid: 2814 of: 1028740\n",
      "/rid: 2815 of: 1028740\n",
      "/rid: 2816 of: 1028740\n",
      "/rid: 2817 of: 1028740\n",
      "/rid: 2818 of: 1028740\n",
      "/rid: 2819 of: 1028740\n",
      "/rid: 2820 of: 1028740\n",
      "/rid: 2821 of: 1028740\n",
      "/rid: 2822 of: 1028740\n",
      "/rid: 2823 of: 1028740\n",
      "/rid: 2824 of: 1028740\n",
      "/rid: 2825 of: 1028740\n",
      "/rid: 2826 of: 1028740\n",
      "/rid: 2827 of: 1028740\n",
      "/rid: 2828 of: 1028740\n",
      "/rid: 2829 of: 1028740\n",
      "/rid: 2830 of: 1028740\n",
      "/rid: 2831 of: 1028740\n",
      "/rid: 2832 of: 1028740\n",
      "/rid: 2833 of: 1028740\n",
      "/rid: 2834 of: 1028740\n",
      "/rid: 2835 of: 1028740\n",
      "/rid: 2836 of: 1028740\n",
      "/rid: 2837 of: 1028740\n",
      "/rid: 2838 of: 1028740\n",
      "/rid: 2839 of: 1028740\n",
      "/rid: 2840 of: 1028740\n",
      "/rid: 2841 of: 1028740\n",
      "/rid: 2842 of: 1028740\n",
      "/rid: 2843 of: 1028740\n",
      "/rid: 2844 of: 1028740\n",
      "/rid: 2845 of: 1028740\n",
      "/rid: 2846 of: 1028740\n",
      "/rid: 2847 of: 1028740\n",
      "/rid: 2848 of: 1028740\n",
      "/rid: 2849 of: 1028740\n",
      "/rid: 2850 of: 1028740\n",
      "/rid: 2851 of: 1028740\n",
      "/rid: 2852 of: 1028740\n",
      "/rid: 2853 of: 1028740\n",
      "/rid: 2854 of: 1028740\n",
      "/rid: 2855 of: 1028740\n",
      "/rid: 2856 of: 1028740\n",
      "/rid: 2857 of: 1028740\n",
      "/rid: 2858 of: 1028740\n",
      "/rid: 2859 of: 1028740\n",
      "/rid: 2860 of: 1028740\n",
      "/rid: 2861 of: 1028740\n",
      "/rid: 2862 of: 1028740\n",
      "/rid: 2863 of: 1028740\n",
      "/rid: 2864 of: 1028740\n",
      "/rid: 2865 of: 1028740\n",
      "/rid: 2866 of: 1028740\n",
      "/rid: 2867 of: 1028740\n",
      "/rid: 2868 of: 1028740\n",
      "/rid: 2869 of: 1028740\n",
      "/rid: 2870 of: 1028740\n",
      "/rid: 2871 of: 1028740\n",
      "/rid: 2872 of: 1028740\n",
      "/rid: 2873 of: 1028740\n",
      "/rid: 2874 of: 1028740\n",
      "/rid: 2875 of: 1028740\n",
      "/rid: 2876 of: 1028740\n",
      "/rid: 2877 of: 1028740\n",
      "/rid: 2878 of: 1028740\n",
      "/rid: 2879 of: 1028740\n",
      "/rid: 2880 of: 1028740\n",
      "/rid: 2881 of: 1028740\n",
      "/rid: 2882 of: 1028740\n",
      "/rid: 2883 of: 1028740\n",
      "/rid: 2884 of: 1028740\n",
      "/rid: 2885 of: 1028740\n",
      "/rid: 2886 of: 1028740\n",
      "/rid: 2887 of: 1028740\n",
      "/rid: 2888 of: 1028740\n",
      "/rid: 2889 of: 1028740\n",
      "/rid: 2890 of: 1028740\n",
      "/rid: 2891 of: 1028740\n",
      "/rid: 2892 of: 1028740\n",
      "/rid: 2893 of: 1028740\n",
      "/rid: 2894 of: 1028740\n",
      "/rid: 2895 of: 1028740\n",
      "/rid: 2896 of: 1028740\n",
      "/rid: 2897 of: 1028740\n",
      "/rid: 2898 of: 1028740\n",
      "/rid: 2899 of: 1028740\n",
      "/rid: 2900 of: 1028740\n",
      "/rid: 2901 of: 1028740\n",
      "/rid: 2902 of: 1028740\n",
      "/rid: 2903 of: 1028740\n",
      "/rid: 2904 of: 1028740\n",
      "/rid: 2905 of: 1028740\n",
      "/rid: 2906 of: 1028740\n",
      "/rid: 2907 of: 1028740\n",
      "/rid: 2908 of: 1028740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rid: 2909 of: 1028740\n",
      "/rid: 2910 of: 1028740\n",
      "/rid: 2911 of: 1028740\n",
      "/rid: 2912 of: 1028740\n",
      "/rid: 2913 of: 1028740\n",
      "/rid: 2914 of: 1028740\n",
      "/rid: 2915 of: 1028740\n",
      "/rid: 2916 of: 1028740\n",
      "/rid: 2917 of: 1028740\n",
      "/rid: 2918 of: 1028740\n",
      "/rid: 2919 of: 1028740\n",
      "/rid: 2920 of: 1028740\n",
      "/rid: 2921 of: 1028740\n",
      "/rid: 2922 of: 1028740\n",
      "/rid: 2923 of: 1028740\n",
      "/rid: 2924 of: 1028740\n",
      "/rid: 2925 of: 1028740\n",
      "/rid: 2926 of: 1028740\n",
      "/rid: 2927 of: 1028740\n",
      "/rid: 2928 of: 1028740\n",
      "/rid: 2929 of: 1028740\n",
      "/rid: 2930 of: 1028740\n",
      "/rid: 2931 of: 1028740\n",
      "/rid: 2932 of: 1028740\n",
      "/rid: 2933 of: 1028740\n",
      "/rid: 2934 of: 1028740\n",
      "/rid: 2935 of: 1028740\n",
      "/rid: 2936 of: 1028740\n",
      "/rid: 2937 of: 1028740\n",
      "/rid: 2938 of: 1028740\n",
      "/rid: 2939 of: 1028740\n",
      "/rid: 2940 of: 1028740\n",
      "/rid: 2941 of: 1028740\n",
      "/rid: 2942 of: 1028740\n",
      "/rid: 2943 of: 1028740\n",
      "/rid: 2944 of: 1028740\n",
      "/rid: 2945 of: 1028740\n",
      "/rid: 2946 of: 1028740\n",
      "/rid: 2947 of: 1028740\n",
      "/rid: 2948 of: 1028740\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27545/3683542466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpatches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#patches = [new_image[idx] for idx in slices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "t_start=time.time()\n",
    "for idx in slices:\n",
    "    print('/rid:',i, 'of:',len(slices))\n",
    "    i+=1\n",
    "    if patches is None:\n",
    "        patches=np.array(new_image[idx])\n",
    "    else:\n",
    "        patches = np.concatenate((patches, new_image[idx]),axis=0)\n",
    "#patches = [new_image[idx] for idx in slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b6425ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(X[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a85d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c071305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca0aa795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1028740, 32, 32, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.array(X[0])\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a52671",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.array(X[1])\n",
    "a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01174877",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = np.concatenate((a1 , a2),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4bda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf08739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Environments/deep_transfer_learning_env/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  after removing the cwd from sys.path.\n",
      "/home/mostafa/Marwa/Environments/deep_transfer_learning_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> DEBUG  training01_01 Voxels to classify: 1724143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:492: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    }
   ],
   "source": [
    "# compute lesion segmentation in batches of size options['batch_size']\n",
    "batch, centers = load_test_patches(test_x_data,\n",
    "                                   options['patch_size'],\n",
    "                                   options['batch_size'],\n",
    "                                   candidate_mask)\n",
    "if options['debug'] is True:\n",
    "    print (\"> DEBUG: testing current_batch:\", batch.shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ce041",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = len(batch)//options['batch_size'] if len(batch) % options['batch_size'] ==0 else len(batch)//options['batch_size'] +1\n",
    "iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e764d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    iter_num = len(batch)//options['batch_size'] if len(batch) % options['batch_size'] ==0 else len(batch)//options['batch_size'] +1\n",
    "\n",
    "    for i in range(iter_num):\n",
    "        start=i*options['batch_size']\n",
    "        end=start+options['batch_size']\n",
    "        data_source_valid = batch[start:end, :]\n",
    "        current_centers = centers[start:end]\n",
    "        # last batch not completed\n",
    "        # last iter from batches less than batch_size\n",
    "        if i ==iter_num-1 and len(batch) % options['batch_size'] != 0:\n",
    "            #data_source_valid = batch[start:, :]\n",
    "            #current_centers = centers[start:]\n",
    "            end = options['batch_size']-len(data_source_valid)\n",
    "            data_source_valid = np.concatenate((data_source_valid,  batch[:end, :]), axis=0)\n",
    "            current_centers = np.concatenate((current_centers, centers[:end]), axis=0)\n",
    "\n",
    "\n",
    "        data_source_valid = torch.from_numpy(data_source_valid)\n",
    "        if cuda:\n",
    "            data_source_valid = data_source_valid.cuda()\n",
    "        data_source_valid= Variable(data_source_valid)\n",
    "        s_output, _ = model(data_source_valid)\n",
    "\n",
    "        #F.log_softmax(s_output, dim = 1) # sum up batch loss\n",
    "        y_pred = s_output.data.max(1)[1] # get the index of the max log-probability\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        y_pred.reshape(-1, 1)\n",
    "        #y_pred = y_pred.numpy()\n",
    "\n",
    "        [x, y, z] = np.stack(current_centers, axis=1)\n",
    "\n",
    "        seg_image[x, y, z] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd1416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb3e11fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> DEBUG  training01_01 Voxels to classify: 1724143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23968/280638114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     seg_mask = test_scan(model,\n\u001b[1;32m      6\u001b[0m                          \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                          options, save_nifti=False)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mseg_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_mask\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py\u001b[0m in \u001b[0;36mtest_scan\u001b[0;34m(model, test_x_data, options, save_nifti, candidate_mask, cuda)\u001b[0m\n\u001b[1;32m    533\u001b[0m                                        \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'patch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                                        \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m                                        candidate_mask)\n\u001b[0m\u001b[1;32m    536\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'debug'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"> DEBUG: testing current_batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py\u001b[0m in \u001b[0;36mload_test_patches\u001b[0;34m(test_x_data, patch_size, batch_size, voxel_candidates, datatype)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_modality\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_modality\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_voxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py\u001b[0m in \u001b[0;36mget_patches\u001b[0;34m(image, centers, patch_size)\u001b[0m\n\u001b[1;32m    489\u001b[0m                                                     \u001b[0mpatch_half\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                                                     patch_size)]\n\u001b[0;32m--> 491\u001b[0;31m                   for center in new_centers]\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    489\u001b[0m                                                     \u001b[0mpatch_half\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                                                     patch_size)]\n\u001b[0;32m--> 491\u001b[0;31m                   for center in new_centers]\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    485\u001b[0m                         for idx, size in zip(patch_half, patch_size))\n\u001b[1;32m    486\u001b[0m         \u001b[0mnew_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         slices = [[slice(c_idx-p_idx, c_idx+(s_idx-p_idx))\n\u001b[0m\u001b[1;32m    488\u001b[0m                    for (c_idx, p_idx, s_idx) in zip(center,\n\u001b[1;32m    489\u001b[0m                                                     \u001b[0mpatch_half\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seg_masks = []\n",
    "for scan, s in zip(train_x_data.keys(), range(len(scans))):\n",
    "    #print(train_x_data.items())\n",
    "    #print(dict(list(train_x_data.items())[s:s+1]))\n",
    "    seg_mask = test_scan(model,\n",
    "                         dict(list(train_x_data.items())[s:s+1]),\n",
    "                         options, save_nifti=False)\n",
    "    seg_masks.append(seg_mask > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ebe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, _ = load_training_data(train_x_data, train_y_data, options, model=model)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb2f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> DEBUG  training01_01 Voxels to classify: 1724143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/Code/deep-transfer-learning/UDA/pytorch0.3/DAN/utils/data_load.py:492: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  patches = [new_image[idx] for idx in slices]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for idx in x_dict:\n",
    "train_x_data = {idx: x_dict[idx]}\n",
    "if y_dict is not None:\n",
    "    train_y_data = {idx: y_dict[idx]}\n",
    "    X, Y, _ = load_training_data(train_x_data, train_y_data, options, model=model)\n",
    "    print(X.shape, Y.shape)\n",
    "else:\n",
    "    X = load_target_voxels(train_x_data, options)\n",
    "    Y = None\n",
    "    train_y_data = None\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b00de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Path(h5_path).mkdir(parents=True, exist_ok=True)\n",
    "f5_path = os.path.join(h5_path, 'file_'+idx+'.hdf5')\n",
    "if dataset_name == 'ISBI':\n",
    "    index = train_data.loc[train_data.patient_id+train_data.study == idx].index[0]\n",
    "else:\n",
    "    index = train_data.loc[train_data.center_id+'_'+train_data.patient == idx].index[0]\n",
    "\n",
    "train_data.loc[index, f5_path_column_name] = f5_path\n",
    "\n",
    "#for i in raw_data:\n",
    "with h5py.File(f5_path, 'w') as f:\n",
    "    print(X.shape, 'patches', X.shape[0], 'modalities', X.shape[-1])\n",
    "    f.create_dataset(\"id\", data=idx)\n",
    "    f.create_dataset(\"patches\", data=X.shape[0])\n",
    "    f.create_dataset(\"modalities\", data=X.shape[-1])\n",
    "    f.create_dataset(str('X'), data=X)\n",
    "    if Y is not None:\n",
    "        f.create_dataset(str('Y'), data=Y)\n",
    "train_data.to_csv(train_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79967a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Marwa/DataSets/ISBI1/h5df_files1/file_training01_02.hdf5\n",
      "DANNet(\n",
      "  (sharedNet): ResNet(\n",
      "    (conv1): Conv3d(2, 64, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (dropout): Dropout3d(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool3d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (cls_fc1): Linear(in_features=16384, out_features=2048, bias=True)\n",
      "  (cls_fc): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "valid_files, valid_files_ref, valid_patches = load_data_patches(options['h5_path'], options['train_csv_path'], phase='valid', fold=fold, options=options)\n",
    "valid_generator = DatasetGenerator(data=valid_files, options=options, patches=valid_patches)\n",
    "#source_train_loader = dl.load_training(options, train_x_data, train_y_data, model=pretrained_model)\n",
    "#source_valid_loader = dl.load_training(options, valid_x_data, valid_y_data, model=pretrained_model)\n",
    "\n",
    "#source_test_loader = data_loader.load_testing('', source_path, batch_size, kwargs)\n",
    "\n",
    "len_source_train_dataset = train_generator.__len__() * options['batch_size']\n",
    "len_source_valid_dataset = valid_generator.__len__() * options['batch_size']\n",
    "len_source_train_loader = train_generator.__len__()\n",
    "len_source_valid_loader = valid_generator.__len__()\n",
    "\n",
    "model = models.DANNet(num_classes=2)\n",
    "if options['load_initial_weights']:\n",
    "    model = torch.load(options['initial_weights_file'])\n",
    "elif options['save_initial_weights']:\n",
    "    Path(options['initial_weights_path']).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model, options['initial_weights_file'])\n",
    "#writer.add_graph(model, torch.rand(size=(128,2,16,16,16)))\n",
    "#writer.flush()\n",
    "#writer.close()\n",
    "#sys.exit()\n",
    "correct = 0\n",
    "print(model)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': model.sharedNet.parameters()},\n",
    "    {'params': model.cls_fc.parameters(), 'lr': lr[1]},\n",
    "    ], lr=lr[0], momentum=momentum, weight_decay=l2_decay)\n",
    "path= os.path.join(options['weight_paths'], options['experiment'], train_count)\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "history_df = pd.DataFrame(columns=['lr', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "patience = options['patience']\n",
    "patience_value = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "138655b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/32384 (0%)]\tLoss: 0.838205\n",
      "Train Epoch: 1 [1280/32384 (4%)]\tLoss: 2.139352\n",
      "Train Epoch: 1 [2560/32384 (8%)]\tLoss: 4.989221\n",
      "Train Epoch: 1 [3840/32384 (12%)]\tLoss: 3.468369\n",
      "Train Epoch: 1 [5120/32384 (16%)]\tLoss: 2.202323\n",
      "Train Epoch: 1 [6400/32384 (20%)]\tLoss: 0.928869\n",
      "Train Epoch: 1 [7680/32384 (24%)]\tLoss: 1.040794\n",
      "Train Epoch: 1 [8960/32384 (28%)]\tLoss: 0.721055\n",
      "Train Epoch: 1 [10240/32384 (32%)]\tLoss: 0.736845\n",
      "Train Epoch: 1 [11520/32384 (36%)]\tLoss: 0.963362\n",
      "Train Epoch: 1 [12800/32384 (40%)]\tLoss: 0.619272\n",
      "Train Epoch: 1 [14080/32384 (43%)]\tLoss: 0.685717\n",
      "Train Epoch: 1 [15360/32384 (47%)]\tLoss: 0.675628\n",
      "Train Epoch: 1 [16640/32384 (51%)]\tLoss: 0.770761\n",
      "Train Epoch: 1 [17920/32384 (55%)]\tLoss: 0.630115\n",
      "Train Epoch: 1 [19200/32384 (59%)]\tLoss: 0.915417\n",
      "Train Epoch: 1 [20480/32384 (63%)]\tLoss: 0.908883\n",
      "Train Epoch: 1 [21760/32384 (67%)]\tLoss: 0.647430\n",
      "Train Epoch: 1 [23040/32384 (71%)]\tLoss: 0.731678\n",
      "Train Epoch: 1 [24320/32384 (75%)]\tLoss: 0.587523\n",
      "Train Epoch: 1 [25600/32384 (79%)]\tLoss: 0.613241\n",
      "Train Epoch: 1 [26880/32384 (83%)]\tLoss: 0.591506\n",
      "Train Epoch: 1 [28160/32384 (87%)]\tLoss: 0.838563\n",
      "Train Epoch: 1 [29440/32384 (91%)]\tLoss: 0.591714\n",
      "Train Epoch: 1 [30720/32384 (95%)]\tLoss: 0.630899\n",
      "Train Epoch: 1 [32000/32384 (99%)]\tLoss: 0.681051\n",
      "\n",
      "ISBI set: Average loss: 0.7941, Accuracy: 15031/30080 (49.97%)\n",
      "\n",
      "source: ISBI to target:  max correct: 15031 max accuracy 49.97%\n",
      "\n",
      "patience:  0\n",
      "Train Epoch: 2 [0/32384 (0%)]\tLoss: 0.585719\n",
      "Train Epoch: 2 [1280/32384 (4%)]\tLoss: 0.630411\n",
      "Train Epoch: 2 [2560/32384 (8%)]\tLoss: 0.563721\n",
      "Train Epoch: 2 [3840/32384 (12%)]\tLoss: 0.620662\n",
      "Train Epoch: 2 [5120/32384 (16%)]\tLoss: 0.816251\n",
      "Train Epoch: 2 [6400/32384 (20%)]\tLoss: 0.622344\n",
      "Train Epoch: 2 [7680/32384 (24%)]\tLoss: 0.790023\n",
      "Train Epoch: 2 [8960/32384 (28%)]\tLoss: 0.595027\n",
      "Train Epoch: 2 [10240/32384 (32%)]\tLoss: 0.612451\n",
      "Train Epoch: 2 [11520/32384 (36%)]\tLoss: 0.652432\n",
      "Train Epoch: 2 [12800/32384 (40%)]\tLoss: 0.662469\n",
      "Train Epoch: 2 [14080/32384 (43%)]\tLoss: 0.640948\n",
      "Train Epoch: 2 [15360/32384 (47%)]\tLoss: 0.667235\n",
      "Train Epoch: 2 [16640/32384 (51%)]\tLoss: 0.727537\n",
      "Train Epoch: 2 [17920/32384 (55%)]\tLoss: 0.624882\n",
      "Train Epoch: 2 [19200/32384 (59%)]\tLoss: 0.672961\n",
      "Train Epoch: 2 [20480/32384 (63%)]\tLoss: 0.642010\n",
      "Train Epoch: 2 [21760/32384 (67%)]\tLoss: 0.604838\n",
      "Train Epoch: 2 [23040/32384 (71%)]\tLoss: 0.772773\n",
      "Train Epoch: 2 [24320/32384 (75%)]\tLoss: 0.658456\n",
      "Train Epoch: 2 [25600/32384 (79%)]\tLoss: 0.576986\n",
      "Train Epoch: 2 [26880/32384 (83%)]\tLoss: 0.615744\n",
      "Train Epoch: 2 [28160/32384 (87%)]\tLoss: 0.589925\n",
      "Train Epoch: 2 [29440/32384 (91%)]\tLoss: 0.605273\n",
      "Train Epoch: 2 [30720/32384 (95%)]\tLoss: 0.618453\n",
      "Train Epoch: 2 [32000/32384 (99%)]\tLoss: 0.804118\n",
      "\n",
      "ISBI set: Average loss: 0.7350, Accuracy: 15031/30080 (49.97%)\n",
      "\n",
      "source: ISBI to target:  max correct: 15031 max accuracy 49.97%\n",
      "\n",
      "patience:  1\n"
     ]
    }
   ],
   "source": [
    "epochs=2\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_correct, train_loss = train(epoch, model, optimizer)\n",
    "    #torch.cuda.synchronize()\n",
    "    t_correct, test_loss = validate(model)\n",
    "\n",
    "    FILE = os.path.join(path, str(epoch)+'_model.pth')\n",
    "    torch.save(model, FILE)\n",
    "    if t_correct > correct:\n",
    "        correct = t_correct\n",
    "        patience_value = 0\n",
    "    else:\n",
    "        patience_value += 1\n",
    "    print('patience: ', patience_value)\n",
    "    df = pd.DataFrame([[lr[0], train_loss, train_correct, test_loss,  t_correct]], columns=['lr', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "    history_df = history_df.append(df)\n",
    "\n",
    "    history_df.reset_index(inplace=True)\n",
    "    history_df.drop(columns=['index'], inplace=True)\n",
    "    history_df.to_csv(options['history_csv_path'], index=False)\n",
    "    if patience_value >= patience:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99401409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
