{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21c2813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d2259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import math\n",
    "import data_loader\n",
    "import ResNet as models\n",
    "from config.settings_test import *\n",
    "from utils.data_preprocess import *\n",
    "import dataset_loader as dl\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from utils.data_load import load_data_patches, generate_data_patches\n",
    "from DatasetsGeneratorFromFiles import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4733a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = lr[0] / math.pow((1 + 10 * (epoch - 1) / epochs), 0.75)\n",
    "    optimizer.param_groups[1]['lr'] = lr[1] / math.pow((1 + 10 * (epoch - 1) / epochs), 0.75)\n",
    "\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    correct = 0\n",
    "\n",
    "    #iter_source_train = iter(source_train_loader)\n",
    "    #num_iter_train = len_source_train_loader\n",
    "    for i, (data_source_train, label_source_train) in enumerate(train_generator.__getitem__()):\n",
    "        data_source_train, label_source_train = torch.from_numpy(data_source_train), torch.from_numpy(label_source_train)\n",
    "    #for i in range(1, num_iter_train):\n",
    "        #data_source_train, label_source_train = iter_source_train.next()\n",
    "        if cuda:\n",
    "            data_source_train, label_source_train = data_source_train.cuda(), label_source_train.cuda()\n",
    "        data_source_train, label_source_train = Variable(data_source_train), Variable(label_source_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        label_source_train_pred, _ = model(data_source_train)\n",
    "        # loss = F.cross_entropy(F.log_softmax(label_source_train_pred, dim=1), label_source_train.type(torch.long))\n",
    "        loss = F.cross_entropy(label_source_train_pred, label_source_train.type(torch.long), reduction='mean')\n",
    "        with torch.no_grad():\n",
    "            train_loss += loss\n",
    "            pred = label_source_train_pred.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(label_source_train.view_as(pred)).cpu().sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data_source_train), len_source_train_dataset,\n",
    "                100. * i / len_source_train_loader, loss.item()))\n",
    "\n",
    "    correct = correct.item()\n",
    "    correct_rate = correct / len_source_train_dataset\n",
    "    train_loss = train_loss.item() / len_source_train_loader\n",
    "    return correct_rate, train_loss\n",
    "\n",
    "\n",
    "def validate(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for i, (data_source_valid, label_source_valid) in enumerate(valid_generator.__getitem__()):\n",
    "            data_source_valid, label_source_valid = torch.from_numpy(data_source_valid), torch.from_numpy(label_source_valid)\n",
    "        #iter_source_valid = iter(source_valid_loader)\n",
    "        #num_iter_valid = len_source_valid_loader\n",
    "        #for i in range(1, num_iter_valid):\n",
    "            #data_source_valid, label_source_valid = iter_source_valid.next()\n",
    "            if cuda:\n",
    "                data_source_valid, label_source_valid = data_source_valid.cuda(), label_source_valid.cuda()\n",
    "            data_source_valid, label_source_valid = Variable(data_source_valid), Variable(label_source_valid)\n",
    "            s_output, _ = model(data_source_valid)\n",
    "            test_loss += F.cross_entropy(s_output, label_source_valid.type(torch.long), reduction='mean') # sum up batch loss\n",
    "            pred = s_output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(label_source_valid.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss = test_loss.item() / len_source_valid_loader\n",
    "        correct = correct.item()\n",
    "        correct_rate = correct / len_source_valid_dataset\n",
    "        print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            source_name, test_loss, correct, len_source_valid_dataset, 100. * correct_rate))\n",
    "\n",
    "        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\\n'.format(\n",
    "              source_name, '', correct, 100. * correct_rate))\n",
    "        return correct_rate, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2bdc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# torch.cuda.set_device(1)\n",
    "writer = SummaryWriter('runs')\n",
    "\n",
    "# Training settings\n",
    "st = Settings()\n",
    "options = st.get_options()\n",
    "\n",
    "second_train = options['second_train']\n",
    "pretrained_model = None\n",
    "train_count = options['train_count']\n",
    "if second_train:\n",
    "    pretrained_model = models.DANNet(num_classes=2)\n",
    "    pretrained_model_path = os.path.join(options['weight_paths'], options['experiment'], '1', options['pre_trained_model'])\n",
    "    pretrained_model = torch.load(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9dde516",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = options['batch_size']\n",
    "epochs = options['max_epochs']\n",
    "lr = [0.001, 0.01]\n",
    "momentum = 0.9\n",
    "no_cuda =False\n",
    "seed = options['seed']\n",
    "log_interval = 10\n",
    "l2_decay = 5e-4\n",
    "source_path = options['train_folder']\n",
    "source_name = 'ISBI'\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# resize images in path\n",
    "#resize_images(options)\n",
    "\n",
    "# generate csv file\n",
    "#df = generate_csv(options)\n",
    "\n",
    "# split data to train, validate folds\n",
    "#split_folds(options['train_csv_path'], options['seed'], options['k_fold'])\n",
    "\n",
    "# list scan\n",
    "fold = 0\n",
    "# fold train data\n",
    "df = pd.read_csv(options['train_csv_path'])\n",
    "# select training scans\n",
    "train_files = df.loc[df['fold'] != fold, ['patient_id', 'study']].values\n",
    "valid_files = df.loc[df['fold'] == fold, ['patient_id', 'study']].values\n",
    "train_scan_list = [f[0]+f[1] for f in train_files]\n",
    "valid_scan_list = [f[0]+f[1] for f in valid_files]\n",
    "\n",
    "train_scan_list.sort()\n",
    "valid_scan_list.sort()\n",
    "\n",
    "train_x_data = {f: {m: os.path.join(options['train_folder'], f, options['tmp_folder'], n)\n",
    "                    for m, n in zip(options['modalities'], options['preprocess_x_names'])}\n",
    "                for f in train_scan_list}\n",
    "train_y_data = {f: os.path.join(options['train_folder'], f, options['tmp_folder'],\n",
    "                                options['preprocess_y_names'][0])\n",
    "                for f in train_scan_list}\n",
    "\n",
    "valid_x_data = {f: {m: os.path.join(options['train_folder'], f, options['tmp_folder'], n)\n",
    "                    for m, n in zip(options['modalities'], options['preprocess_x_names'])}\n",
    "                for f in valid_scan_list}\n",
    "valid_y_data = {f: os.path.join(options['train_folder'], f, options['tmp_folder'],\n",
    "                                options['preprocess_y_names'][0])\n",
    "                for f in valid_scan_list}\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6baa7891",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_data_patches() missing 1 required positional argument: 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11860/3745480909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgenerate_data_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h5_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_csv_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISBI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_data_patches() missing 1 required positional argument: 'options'"
     ]
    }
   ],
   "source": [
    "    if second_train:\n",
    "        generate_data_patches(train_x_data, train_y_data, options, dataset_name='ISBI', model=pretrained_model)\n",
    "        #pass\n",
    "    else:\n",
    "        generate_data_patches(train_x_data, train_y_data, options, dataset_name='ISBI')\n",
    "        #pass\n",
    "    train_files, train_files_ref, train_patches = load_data_patches(options['h5_path'], options['train_csv_path'], phase='train', fold=fold, options=options)\n",
    "    train_generator = DatasetGenerator(data=train_files, options=options, patches=train_patches)\n",
    "\n",
    "    if second_train:\n",
    "        generate_data_patches(valid_x_data, valid_y_data, options, dataset_name='ISBI', model=pretrained_model)\n",
    "        #pass\n",
    "    else:\n",
    "        generate_data_patches(valid_x_data, valid_y_data, options, dataset_name='ISBI')\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63306818",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_data_patches() missing 1 required positional argument: 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11860/2513668840.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mgenerate_data_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h5_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_csv_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISBI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;31m#pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_files_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h5_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_csv_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_data_patches() missing 1 required positional argument: 'options'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_files, train_files_ref, train_patches = load_data_patches(options['h5_path'], options['train_csv_path'], phase='train', fold=fold, options=options)\n",
    "train_generator = DatasetGenerator(data=train_files, options=options, patches=train_patches)\n",
    "\n",
    "if second_train:\n",
    "    generate_data_patches(valid_x_data, valid_y_data, options['h5_path'], options['train_csv_path'], options, dataset_name='ISBI', model=pretrained_model)\n",
    "    #pass\n",
    "else:\n",
    "    generate_data_patches(valid_x_data, valid_y_data, options['h5_path'], options['train_csv_path'], options, dataset_name='ISBI')\n",
    "    #pass \n",
    "valid_files, valid_files_ref, valid_patches = load_data_patches(options['h5_path'], options['train_csv_path'], phase='valid', fold=fold, options=options)\n",
    "valid_generator = DatasetGenerator(data=valid_files, options=options, patches=valid_patches)\n",
    "#source_train_loader = dl.load_training(options, train_x_data, train_y_data, model=pretrained_model)\n",
    "#source_valid_loader = dl.load_training(options, valid_x_data, valid_y_data, model=pretrained_model)\n",
    "\n",
    "#source_test_loader = data_loader.load_testing('', source_path, batch_size, kwargs)\n",
    "\n",
    "len_source_train_dataset = train_generator.__len__() * options['batch_size']\n",
    "len_source_valid_dataset = valid_generator.__len__() * options['batch_size']\n",
    "len_source_train_loader = train_generator.__len__()\n",
    "len_source_valid_loader = valid_generator.__len__()\n",
    "\n",
    "model = models.DANNet(num_classes=2)\n",
    "if options['load_initial_weights']:\n",
    "    model = torch.load(options['initial_weights_file'])\n",
    "elif options['save_initial_weights']:\n",
    "    Path(options['initial_weights_path']).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model, options['initial_weights_file'])\n",
    "#writer.add_graph(model, torch.rand(size=(128,2,16,16,16)))\n",
    "#writer.flush()\n",
    "#writer.close()\n",
    "#sys.exit()\n",
    "correct = 0\n",
    "print(model)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': model.sharedNet.parameters()},\n",
    "    {'params': model.cls_fc.parameters(), 'lr': lr[1]},\n",
    "    ], lr=lr[0], momentum=momentum, weight_decay=l2_decay)\n",
    "path= os.path.join(options['weight_paths'], options['experiment'], train_count)\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "history_df = pd.DataFrame(columns=['lr', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "patience = options['patience']\n",
    "patience_value = 0\n",
    "\n",
    "epochs=2\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_correct, train_loss = train(epoch, model, optimizer)\n",
    "    #torch.cuda.synchronize()\n",
    "    t_correct, test_loss = validate(model)\n",
    "\n",
    "    FILE = os.path.join(path, str(epoch)+'_model.pth')\n",
    "    torch.save(model, FILE)\n",
    "    if t_correct > correct:\n",
    "        correct = t_correct\n",
    "        patience_value = 0\n",
    "    else:\n",
    "        patience_value += 1\n",
    "    print('patience: ', patience_value)\n",
    "    df = pd.DataFrame([[lr[0], train_loss, train_correct, test_loss,  t_correct]], columns=['lr', 'loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "    history_df = history_df.append(df)\n",
    "\n",
    "    history_df.reset_index(inplace=True)\n",
    "    history_df.drop(columns=['index'], inplace=True)\n",
    "    history_df.to_csv(options['history_csv_path'], index=False)\n",
    "    if patience_value >= patience:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ace4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
