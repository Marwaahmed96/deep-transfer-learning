{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#torch.cuda.set_device(1)\n",
    "writer = SummaryWriter('runs')\n",
    "\n",
    "# Training settings\n",
    "st = Settings()\n",
    "options = st.get_options()\n",
    "\n",
    "batch_size = options['batch_size']\n",
    "epochs = options['max_epochs']\n",
    "lr = [0.001, 0.01]\n",
    "momentum = 0.9\n",
    "no_cuda =False\n",
    "seed = options['seed']\n",
    "log_interval = 10\n",
    "l2_decay = 5e-4\n",
    "source_path = options['train_folder']\n",
    "source_name = ''\n",
    "cuda = not no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9efc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # resize images in path\n",
    "    #resize_images(options)\n",
    "\n",
    "    # generate csv file\n",
    "    df = generate_csv(options)\n",
    "\n",
    "    # split data to train, validate folds\n",
    "    split_folds(options['train_csv_path'], options['seed'], options['k_fold'])\n",
    "\n",
    "    # list scan\n",
    "    fold = 0\n",
    "    # fold train data\n",
    "    df = pd.read_csv(options['train_csv_path'])\n",
    "    # select training scans\n",
    "    train_files = df.loc[df['fold'] != fold, ['patient_id','study']].values\n",
    "    valid_files = df.loc[df['fold'] == fold, ['patient_id', 'study']].values\n",
    "    train_scan_list = [f[0]+f[1] for f in train_files]\n",
    "    valid_scan_list = [f[0]+f[1] for f in valid_files]\n",
    "\n",
    "    train_scan_list.sort()\n",
    "    valid_scan_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_x_data = {f: {m: os.path.join(options['train_folder'], f, options['tmp_folder'], n)\n",
    "                        for m, n in zip(options['modalities'], options['preprocess_x_names'])}\n",
    "                    for f in train_scan_list}\n",
    "    train_y_data = {f: os.path.join(options['train_folder'], f, options['tmp_folder'],\n",
    "                                    options['preprocess_y_names'][0])\n",
    "                    for f in train_scan_list}\n",
    "\n",
    "    valid_x_data = {f: {m: os.path.join(options['train_folder'], f, options['tmp_folder'], n)\n",
    "                        for m, n in zip(options['modalities'], options['preprocess_x_names'])}\n",
    "                    for f in valid_scan_list}\n",
    "    valid_y_data = {f: os.path.join(options['train_folder'], f, options['tmp_folder'],\n",
    "                                    options['preprocess_y_names'][0])\n",
    "                    for f in valid_scan_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "    source_train_loader = dl.load_training(options, train_x_data, train_y_data)\n",
    "    source_valid_loader = dl.load_training(options, valid_x_data, valid_y_data)\n",
    "\n",
    "    #source_test_loader = data_loader.load_testing('', source_path, batch_size, kwargs)\n",
    "\n",
    "    len_source_train_dataset = len(source_train_loader.dataset)\n",
    "    len_source_valid_dataset = len(source_valid_loader.dataset)\n",
    "    len_source_train_loader = len(source_train_loader)\n",
    "    len_source_valid_loader = len(source_valid_loader)\n",
    "\n",
    "    model = models.DANNet_source(num_classes=2)\n",
    "    writer.add_graph(model, torch.rand(size=(128,2,16,16,16)))\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    #sys.exit()\n",
    "    correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(model)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD([\n",
    "        {'params': model.sharedNet.parameters()},\n",
    "        {'params': model.cls_fc.parameters(), 'lr': lr[1]},\n",
    "        ], lr=lr[0], momentum=momentum, weight_decay=l2_decay)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch, model, optimizer)\n",
    "        #torch.cuda.synchronize()\n",
    "        #t_correct = validate(model)\n",
    "        t_correct=0\n",
    "        if t_correct > correct:\n",
    "            correct = t_correct\n",
    "        #correct = correct.item()\n",
    "        print(correct)\n",
    "        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\\n'.format(\n",
    "              source_name, '', correct, 100. * correct / len_source_valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f95c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e3a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803cdb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = lr[0] / math.pow((1 + 10 * (epoch - 1) / epochs), 0.75)\n",
    "    optimizer.param_groups[1]['lr'] = lr[1] / math.pow((1 + 10 * (epoch - 1) / epochs), 0.75)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    iter_source_train = iter(source_train_loader)\n",
    "    num_iter_train = len_source_train_loader\n",
    "    for i in range(1, num_iter_train):\n",
    "        data_source_train, label_source_train = iter_source_train.next()\n",
    "        if cuda:\n",
    "            data_source_train, label_source_train = data_source_train.cuda(), label_source_train.cuda()\n",
    "        data_source_train, label_source_train = Variable(data_source_train), Variable(label_source_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        label_source_train_pred = model(data_source_train)\n",
    "        loss = F.cross_entropy(F.log_softmax( label_source_train_pred, dim=1), label_source_train.type(torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tsoft_Loss: {:.6f}'.format(\n",
    "                epoch, i * len(data_source_train), len_source_train_dataset,\n",
    "                100. * i / len_source_train_loader, loss.data, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2296ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    iter_source_valid = iter(source_valid_loader)\n",
    "    num_iter_valid = len_source_valid_loader\n",
    "    for i in range(1, num_iter_valid):\n",
    "        data_source_valid, label_source_valid = iter_source_valid.next()\n",
    "        if cuda:\n",
    "            data_source_valid, label_source_valid = data_source_valid.cuda(), label_source_valid.cuda()\n",
    "        data_source_valid, label_source_valid = Variable(data_source_valid, volatile=True), Variable(label_source_valid, volatile=True)\n",
    "        s_output = model(data_source_valid)\n",
    "        test_loss += F.cross_entropy(F.log_softmax(s_output, dim = 1), label_source_valid.type(torch.long)) # sum up batch loss\n",
    "        pred = s_output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(label_source_valid.view_as(pred)).cpu().sum()\n",
    "        print(i)\n",
    "\n",
    "    test_loss /= len_source_valid_dataset\n",
    "    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        source_name, test_loss, correct, len_source_valid_dataset,\n",
    "        100. * correct / len_source_valid_dataset))\n",
    "    return correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
